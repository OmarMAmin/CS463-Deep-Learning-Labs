{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_kaggle_csv(matrix, header,filename):\n",
    "    frame = pd.DataFrame(data = matrix,columns=header)\n",
    "    frame.to_csv(path_or_buf  = filename,index = False,sep =',')\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt(delimiter=',',fname='train.csv',skip_header=True)\n",
    "test_data = np.genfromtxt(delimiter=',',fname='test.csv',skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0\n",
      "255.0 255.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "train_y = train_data[:,0].copy()\n",
    "train_x = train_data[:,1:].copy()\n",
    "print(train_x.max())\n",
    "print(test_data.max(),train_data.max())\n",
    "img_width, img_height = 28,28\n",
    "#train_x = train_x.reshape((train_x.shape[0],28,28,1))\n",
    "#train_x = np.repeat(train_x,3,axis=3 )\n",
    "#test_x = test_data.reshape((test_data.shape[0],28,28,1))\n",
    "#test_x = np.repeat(test_x,3,axis=3)\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(train_y.reshape((train_y.shape[0],1)))\n",
    "train_y = enc.transform(train_y.reshape((train_y.shape[0],1))).toarray()\n",
    "train_x /= 255\n",
    "test_x  = test_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4300a4b54691>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_y' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_y.shape)\n",
    "print(train_x.shape)\n",
    "print(train_x.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training one layer network for mnist dataset\n",
    "One layer network doesn't have the capacity to overfit to the training data, so we can expect a similar test accuracy to our training accuracy when submitting to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.layers.core import Dense, Activation # defining the layers\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(optimizer,epochs,train_x,train_y,regularization = 0,valid_x = None,valid_y = None):\n",
    "    ## model type is either sequential or model, sequential will be almost enough for our course\n",
    "    linear_model = Sequential()\n",
    "    ## now we have an empty model that will be created through sequentially adding layers to it.\n",
    "    ## add a layer that takes the input image and pass it through a fully connected layer with 512 hidden nodes\n",
    "    ## dense layer ==> fully connected layer\n",
    "    linear_model.add(Dense(10,input_shape=(784,),kernel_regularizer=regularizers.l2(regularization)))\n",
    "    linear_model.add(Activation('softmax'))\n",
    "    \n",
    "    ## no need for validation set as this is a simple linear model\n",
    "    linear_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,#RMSprop()\n",
    "                  metrics=['accuracy'])\n",
    "    if valid_x is None or valid_y is None:\n",
    "        history = linear_model.fit(train_x,train_y,\n",
    "                        batch_size = 64,epochs=epochs,\n",
    "                        verbose=1)#,validation_data=(test, Y_test))\n",
    "    else:\n",
    "        history = linear_model.fit(train_x,train_y,\n",
    "                        batch_size = 64,epochs=epochs,\n",
    "                        verbose=1,validation_data=(valid_x, valid_y))#,validation_data=(test, Y_test))\n",
    "    return linear_model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.6225 - acc: 0.8462\n",
      "Epoch 2/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.3431 - acc: 0.9058\n",
      "Epoch 3/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.3054 - acc: 0.9147\n",
      "Epoch 4/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.2872 - acc: 0.9189\n",
      "Epoch 5/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.2760 - acc: 0.9224\n",
      "Epoch 6/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.2684 - acc: 0.9253\n",
      "Epoch 7/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2625 - acc: 0.9253\n",
      "Epoch 8/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.2574 - acc: 0.9278\n",
      "Epoch 9/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.2536 - acc: 0.9286\n",
      "Epoch 10/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.2501 - acc: 0.9297\n",
      "Epoch 11/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.2475 - acc: 0.9308\n",
      "Epoch 12/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.2449 - acc: 0.9312\n",
      "Epoch 13/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2433 - acc: 0.9319\n",
      "Epoch 14/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2409 - acc: 0.9322\n",
      "Epoch 15/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.2388 - acc: 0.9330\n",
      "Epoch 16/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.2373 - acc: 0.9338\n",
      "Epoch 17/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2359 - acc: 0.9334\n",
      "Epoch 18/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.2346 - acc: 0.9340\n",
      "Epoch 19/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2330 - acc: 0.9340\n",
      "Epoch 20/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.2320 - acc: 0.9345\n",
      "Epoch 21/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.2308 - acc: 0.9350\n",
      "Epoch 22/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.2294 - acc: 0.9354\n",
      "Epoch 23/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2286 - acc: 0.9361\n",
      "Epoch 24/300\n",
      "42000/42000 [==============================] - 3s 71us/step - loss: 0.2279 - acc: 0.9355\n",
      "Epoch 25/300\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.2267 - acc: 0.9363\n",
      "Epoch 26/300\n",
      "42000/42000 [==============================] - 2s 55us/step - loss: 0.2262 - acc: 0.9366\n",
      "Epoch 27/300\n",
      "42000/42000 [==============================] - 2s 55us/step - loss: 0.2259 - acc: 0.9355\n",
      "Epoch 28/300\n",
      "42000/42000 [==============================] - 5s 107us/step - loss: 0.2242 - acc: 0.9368\n",
      "Epoch 29/300\n",
      "42000/42000 [==============================] - 5s 116us/step - loss: 0.2240 - acc: 0.9367\n",
      "Epoch 30/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2226 - acc: 0.9376\n",
      "Epoch 31/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.2220 - acc: 0.9374\n",
      "Epoch 32/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.2221 - acc: 0.9380\n",
      "Epoch 33/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.2207 - acc: 0.9374\n",
      "Epoch 34/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2203 - acc: 0.9379\n",
      "Epoch 35/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.2198 - acc: 0.9382\n",
      "Epoch 36/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2196 - acc: 0.9376\n",
      "Epoch 37/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2189 - acc: 0.9387\n",
      "Epoch 38/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.2183 - acc: 0.9380\n",
      "Epoch 39/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.2183 - acc: 0.9378\n",
      "Epoch 40/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.2172 - acc: 0.9390\n",
      "Epoch 41/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.2169 - acc: 0.9388\n",
      "Epoch 42/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.2163 - acc: 0.9383\n",
      "Epoch 43/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.2158 - acc: 0.9392\n",
      "Epoch 44/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2155 - acc: 0.9398\n",
      "Epoch 45/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.2149 - acc: 0.9393\n",
      "Epoch 46/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2145 - acc: 0.9397\n",
      "Epoch 47/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.2139 - acc: 0.9397\n",
      "Epoch 48/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.2135 - acc: 0.9396\n",
      "Epoch 49/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.2132 - acc: 0.9403\n",
      "Epoch 50/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.2132 - acc: 0.9399\n",
      "Epoch 51/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2126 - acc: 0.9401\n",
      "Epoch 52/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2123 - acc: 0.9400\n",
      "Epoch 53/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2116 - acc: 0.9399\n",
      "Epoch 54/300\n",
      "42000/42000 [==============================] - 5s 108us/step - loss: 0.2117 - acc: 0.9399\n",
      "Epoch 55/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2114 - acc: 0.9397\n",
      "Epoch 56/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.2110 - acc: 0.9400\n",
      "Epoch 57/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.2106 - acc: 0.9406\n",
      "Epoch 58/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.2099 - acc: 0.9405\n",
      "Epoch 59/300\n",
      "42000/42000 [==============================] - 5s 108us/step - loss: 0.2099 - acc: 0.9410\n",
      "Epoch 60/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.2097 - acc: 0.9404\n",
      "Epoch 61/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.2092 - acc: 0.9405\n",
      "Epoch 62/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2091 - acc: 0.9400\n",
      "Epoch 63/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.2085 - acc: 0.9410\n",
      "Epoch 64/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.2086 - acc: 0.9410\n",
      "Epoch 65/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2080 - acc: 0.9406\n",
      "Epoch 66/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2078 - acc: 0.9415\n",
      "Epoch 67/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.2074 - acc: 0.9413\n",
      "Epoch 68/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.2073 - acc: 0.9418\n",
      "Epoch 69/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2072 - acc: 0.9414\n",
      "Epoch 70/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.2068 - acc: 0.9415\n",
      "Epoch 71/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.2067 - acc: 0.9413\n",
      "Epoch 72/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.2065 - acc: 0.9414\n",
      "Epoch 73/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2063 - acc: 0.9421\n",
      "Epoch 74/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2061 - acc: 0.9411\n",
      "Epoch 75/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.2056 - acc: 0.9417\n",
      "Epoch 76/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.2056 - acc: 0.9420\n",
      "Epoch 77/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.2050 - acc: 0.9420\n",
      "Epoch 78/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.2052 - acc: 0.9423\n",
      "Epoch 79/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2049 - acc: 0.9422\n",
      "Epoch 80/300\n",
      "42000/42000 [==============================] - 5s 116us/step - loss: 0.2045 - acc: 0.9424\n",
      "Epoch 81/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.2046 - acc: 0.9419\n",
      "Epoch 82/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.2043 - acc: 0.9426\n",
      "Epoch 83/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.2040 - acc: 0.9430\n",
      "Epoch 84/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.2036 - acc: 0.9425\n",
      "Epoch 85/300\n",
      "42000/42000 [==============================] - 4s 88us/step - loss: 0.2037 - acc: 0.9425\n",
      "Epoch 86/300\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.2031 - acc: 0.9425\n",
      "Epoch 87/300\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.2031 - acc: 0.9427\n",
      "Epoch 88/300\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.2032 - acc: 0.9426\n",
      "Epoch 89/300\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.2026 - acc: 0.9422\n",
      "Epoch 90/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2032 - acc: 0.9424\n",
      "Epoch 91/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.2025 - acc: 0.9423\n",
      "Epoch 92/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2019 - acc: 0.9430\n",
      "Epoch 93/300\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.2021 - acc: 0.9431\n",
      "Epoch 94/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.2018 - acc: 0.9426\n",
      "Epoch 95/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2017 - acc: 0.9433\n",
      "Epoch 96/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2012 - acc: 0.9431\n",
      "Epoch 97/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.2016 - acc: 0.9433\n",
      "Epoch 98/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2013 - acc: 0.9430\n",
      "Epoch 99/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2009 - acc: 0.9435\n",
      "Epoch 100/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.2009 - acc: 0.9431\n",
      "Epoch 101/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.2011 - acc: 0.9429\n",
      "Epoch 102/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.2007 - acc: 0.9422\n",
      "Epoch 103/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.2001 - acc: 0.9435\n",
      "Epoch 104/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.2000 - acc: 0.9435\n",
      "Epoch 105/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.2002 - acc: 0.9430\n",
      "Epoch 106/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.2000 - acc: 0.9440\n",
      "Epoch 107/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.2001 - acc: 0.9434\n",
      "Epoch 108/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1997 - acc: 0.9439\n",
      "Epoch 109/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1993 - acc: 0.9438\n",
      "Epoch 110/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1993 - acc: 0.9441\n",
      "Epoch 111/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1993 - acc: 0.9437\n",
      "Epoch 112/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1991 - acc: 0.9434\n",
      "Epoch 113/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1990 - acc: 0.9439\n",
      "Epoch 114/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1984 - acc: 0.9439\n",
      "Epoch 115/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1986 - acc: 0.9434\n",
      "Epoch 116/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1984 - acc: 0.9435\n",
      "Epoch 117/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1987 - acc: 0.9441\n",
      "Epoch 118/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1977 - acc: 0.9440\n",
      "Epoch 119/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1979 - acc: 0.9435\n",
      "Epoch 120/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1979 - acc: 0.9441\n",
      "Epoch 121/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1976 - acc: 0.9442\n",
      "Epoch 122/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1974 - acc: 0.9443\n",
      "Epoch 123/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1976 - acc: 0.9440\n",
      "Epoch 124/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1974 - acc: 0.9431\n",
      "Epoch 125/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1972 - acc: 0.9440\n",
      "Epoch 126/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1972 - acc: 0.9443\n",
      "Epoch 127/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1971 - acc: 0.9444\n",
      "Epoch 128/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1965 - acc: 0.9446\n",
      "Epoch 129/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.1970 - acc: 0.9443\n",
      "Epoch 130/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1963 - acc: 0.9445\n",
      "Epoch 131/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1963 - acc: 0.9440\n",
      "Epoch 132/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1964 - acc: 0.9446\n",
      "Epoch 133/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.1965 - acc: 0.9445\n",
      "Epoch 134/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1963 - acc: 0.9440\n",
      "Epoch 135/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1961 - acc: 0.9441\n",
      "Epoch 136/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1961 - acc: 0.9443\n",
      "Epoch 137/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1959 - acc: 0.9447\n",
      "Epoch 138/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1950 - acc: 0.9450\n",
      "Epoch 139/300\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 0.1957 - acc: 0.943 - 4s 99us/step - loss: 0.1956 - acc: 0.9439\n",
      "Epoch 140/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1955 - acc: 0.9445\n",
      "Epoch 141/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1954 - acc: 0.9440\n",
      "Epoch 142/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1953 - acc: 0.9442\n",
      "Epoch 143/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1954 - acc: 0.9451\n",
      "Epoch 144/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1953 - acc: 0.9440\n",
      "Epoch 145/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1948 - acc: 0.9443\n",
      "Epoch 146/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1949 - acc: 0.9449\n",
      "Epoch 147/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1945 - acc: 0.9451\n",
      "Epoch 148/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1945 - acc: 0.9446\n",
      "Epoch 149/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1948 - acc: 0.9440\n",
      "Epoch 150/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1946 - acc: 0.9454\n",
      "Epoch 151/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1941 - acc: 0.9446\n",
      "Epoch 152/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1939 - acc: 0.9449\n",
      "Epoch 153/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1940 - acc: 0.9446\n",
      "Epoch 154/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1939 - acc: 0.9451\n",
      "Epoch 155/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1936 - acc: 0.9454\n",
      "Epoch 156/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1943 - acc: 0.9441\n",
      "Epoch 157/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1932 - acc: 0.9451\n",
      "Epoch 158/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1932 - acc: 0.9457\n",
      "Epoch 159/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1940 - acc: 0.9449\n",
      "Epoch 160/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1936 - acc: 0.9451\n",
      "Epoch 161/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1933 - acc: 0.9456\n",
      "Epoch 162/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1932 - acc: 0.9452\n",
      "Epoch 163/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1933 - acc: 0.9453\n",
      "Epoch 164/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1932 - acc: 0.9452\n",
      "Epoch 165/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1931 - acc: 0.9455\n",
      "Epoch 166/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1929 - acc: 0.9449\n",
      "Epoch 167/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1931 - acc: 0.9447\n",
      "Epoch 168/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1925 - acc: 0.9452\n",
      "Epoch 169/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1926 - acc: 0.9452\n",
      "Epoch 170/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1924 - acc: 0.9450\n",
      "Epoch 171/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1925 - acc: 0.9455\n",
      "Epoch 172/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1928 - acc: 0.9449\n",
      "Epoch 173/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1922 - acc: 0.9458\n",
      "Epoch 174/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1918 - acc: 0.9460\n",
      "Epoch 175/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1923 - acc: 0.9448\n",
      "Epoch 176/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1920 - acc: 0.9451\n",
      "Epoch 177/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1917 - acc: 0.9457\n",
      "Epoch 178/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1920 - acc: 0.9451\n",
      "Epoch 179/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1919 - acc: 0.9453\n",
      "Epoch 180/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1915 - acc: 0.9458\n",
      "Epoch 181/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1914 - acc: 0.9456\n",
      "Epoch 182/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1916 - acc: 0.9454\n",
      "Epoch 183/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1917 - acc: 0.9460\n",
      "Epoch 184/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1917 - acc: 0.9460\n",
      "Epoch 185/300\n",
      "42000/42000 [==============================] - 3s 62us/step - loss: 0.1911 - acc: 0.9452\n",
      "Epoch 186/300\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.1913 - acc: 0.9456\n",
      "Epoch 187/300\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.1910 - acc: 0.9460\n",
      "Epoch 188/300\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.1904 - acc: 0.9465\n",
      "Epoch 189/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1909 - acc: 0.9457\n",
      "Epoch 190/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1915 - acc: 0.9453\n",
      "Epoch 191/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1910 - acc: 0.9456\n",
      "Epoch 192/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1907 - acc: 0.9457\n",
      "Epoch 193/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1905 - acc: 0.9467\n",
      "Epoch 194/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1906 - acc: 0.9458\n",
      "Epoch 195/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.1908 - acc: 0.9460\n",
      "Epoch 196/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.1903 - acc: 0.9461\n",
      "Epoch 197/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1902 - acc: 0.9460\n",
      "Epoch 198/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.1904 - acc: 0.9465\n",
      "Epoch 199/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1901 - acc: 0.9454\n",
      "Epoch 200/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1900 - acc: 0.9462\n",
      "Epoch 201/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.1902 - acc: 0.9460\n",
      "Epoch 202/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.1900 - acc: 0.9459\n",
      "Epoch 203/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1900 - acc: 0.9460\n",
      "Epoch 204/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.1903 - acc: 0.9467\n",
      "Epoch 205/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.1900 - acc: 0.9466\n",
      "Epoch 206/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.1901 - acc: 0.9461\n",
      "Epoch 207/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1898 - acc: 0.9465\n",
      "Epoch 208/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.1893 - acc: 0.9467\n",
      "Epoch 209/300\n",
      "42000/42000 [==============================] - 5s 108us/step - loss: 0.1895 - acc: 0.9461\n",
      "Epoch 210/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.1895 - acc: 0.9464\n",
      "Epoch 211/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1895 - acc: 0.9462\n",
      "Epoch 212/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1896 - acc: 0.9464\n",
      "Epoch 213/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.1892 - acc: 0.9468\n",
      "Epoch 214/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1892 - acc: 0.9471\n",
      "Epoch 215/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1893 - acc: 0.9460\n",
      "Epoch 216/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.1891 - acc: 0.9461\n",
      "Epoch 217/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.1890 - acc: 0.9465\n",
      "Epoch 218/300\n",
      "42000/42000 [==============================] - 4s 87us/step - loss: 0.1889 - acc: 0.9463\n",
      "Epoch 219/300\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.1890 - acc: 0.9461\n",
      "Epoch 220/300\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.1889 - acc: 0.9465\n",
      "Epoch 221/300\n",
      "42000/42000 [==============================] - 2s 55us/step - loss: 0.1887 - acc: 0.9460\n",
      "Epoch 222/300\n",
      "42000/42000 [==============================] - 4s 88us/step - loss: 0.1890 - acc: 0.9467\n",
      "Epoch 223/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1888 - acc: 0.9464\n",
      "Epoch 224/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1883 - acc: 0.9467\n",
      "Epoch 225/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.1889 - acc: 0.9465\n",
      "Epoch 226/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.1887 - acc: 0.9462\n",
      "Epoch 227/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1884 - acc: 0.9465\n",
      "Epoch 228/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1886 - acc: 0.9466\n",
      "Epoch 229/300\n",
      "42000/42000 [==============================] - 5s 115us/step - loss: 0.1884 - acc: 0.9470\n",
      "Epoch 230/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1883 - acc: 0.9459\n",
      "Epoch 231/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.1886 - acc: 0.9463\n",
      "Epoch 232/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1885 - acc: 0.9463\n",
      "Epoch 233/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1885 - acc: 0.9464\n",
      "Epoch 234/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.1878 - acc: 0.9469\n",
      "Epoch 235/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.1880 - acc: 0.9464\n",
      "Epoch 236/300\n",
      "42000/42000 [==============================] - 5s 114us/step - loss: 0.1878 - acc: 0.9472\n",
      "Epoch 237/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1874 - acc: 0.9469\n",
      "Epoch 238/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.1879 - acc: 0.9471\n",
      "Epoch 239/300\n",
      "42000/42000 [==============================] - 5s 113us/step - loss: 0.1880 - acc: 0.9467\n",
      "Epoch 240/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.1876 - acc: 0.9466\n",
      "Epoch 241/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1876 - acc: 0.9468\n",
      "Epoch 242/300\n",
      "42000/42000 [==============================] - 5s 110us/step - loss: 0.1878 - acc: 0.9469\n",
      "Epoch 243/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1878 - acc: 0.9466\n",
      "Epoch 244/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1874 - acc: 0.9465\n",
      "Epoch 245/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1872 - acc: 0.9464\n",
      "Epoch 246/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1876 - acc: 0.9465\n",
      "Epoch 247/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1872 - acc: 0.9472\n",
      "Epoch 248/300\n",
      "42000/42000 [==============================] - 5s 111us/step - loss: 0.1876 - acc: 0.9467\n",
      "Epoch 249/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1875 - acc: 0.9470\n",
      "Epoch 250/300\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.1871 - acc: 0.9464\n",
      "Epoch 251/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.1875 - acc: 0.9467\n",
      "Epoch 252/300\n",
      "42000/42000 [==============================] - 2s 54us/step - loss: 0.1873 - acc: 0.9475\n",
      "Epoch 253/300\n",
      "42000/42000 [==============================] - 2s 55us/step - loss: 0.1872 - acc: 0.9466\n",
      "Epoch 254/300\n",
      "42000/42000 [==============================] - 2s 55us/step - loss: 0.1873 - acc: 0.9462\n",
      "Epoch 255/300\n",
      "42000/42000 [==============================] - 3s 69us/step - loss: 0.1871 - acc: 0.9463\n",
      "Epoch 256/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1868 - acc: 0.9471\n",
      "Epoch 257/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1872 - acc: 0.9468\n",
      "Epoch 258/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1866 - acc: 0.9474\n",
      "Epoch 259/300\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.1865 - acc: 0.9477\n",
      "Epoch 260/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1868 - acc: 0.9470\n",
      "Epoch 261/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1869 - acc: 0.9466\n",
      "Epoch 262/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1866 - acc: 0.9473\n",
      "Epoch 263/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1864 - acc: 0.9466\n",
      "Epoch 264/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1863 - acc: 0.9466\n",
      "Epoch 265/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1867 - acc: 0.9475\n",
      "Epoch 266/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1867 - acc: 0.9470\n",
      "Epoch 267/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1864 - acc: 0.9470\n",
      "Epoch 268/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1863 - acc: 0.9469\n",
      "Epoch 269/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1862 - acc: 0.9474\n",
      "Epoch 270/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1862 - acc: 0.9474\n",
      "Epoch 271/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1866 - acc: 0.9470\n",
      "Epoch 272/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1857 - acc: 0.9478\n",
      "Epoch 273/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1864 - acc: 0.9467\n",
      "Epoch 274/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.1861 - acc: 0.9477\n",
      "Epoch 275/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1862 - acc: 0.9479\n",
      "Epoch 276/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1861 - acc: 0.9472\n",
      "Epoch 277/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1862 - acc: 0.9469\n",
      "Epoch 278/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1860 - acc: 0.9475\n",
      "Epoch 279/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1855 - acc: 0.9476\n",
      "Epoch 280/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1858 - acc: 0.9470\n",
      "Epoch 281/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1860 - acc: 0.9473\n",
      "Epoch 282/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1859 - acc: 0.9471\n",
      "Epoch 283/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1857 - acc: 0.9470\n",
      "Epoch 284/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1855 - acc: 0.9473\n",
      "Epoch 285/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1857 - acc: 0.9473\n",
      "Epoch 286/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1856 - acc: 0.9472\n",
      "Epoch 287/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1855 - acc: 0.9474\n",
      "Epoch 288/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.1856 - acc: 0.9474\n",
      "Epoch 289/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1855 - acc: 0.9475\n",
      "Epoch 290/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1853 - acc: 0.9471\n",
      "Epoch 291/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1855 - acc: 0.9479\n",
      "Epoch 292/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.1856 - acc: 0.9474\n",
      "Epoch 293/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1851 - acc: 0.9474\n",
      "Epoch 294/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1857 - acc: 0.9477\n",
      "Epoch 295/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.1854 - acc: 0.9475\n",
      "Epoch 296/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.1854 - acc: 0.9473\n",
      "Epoch 297/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1849 - acc: 0.9476\n",
      "Epoch 298/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1852 - acc: 0.9477\n",
      "Epoch 299/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.1849 - acc: 0.9477\n",
      "Epoch 300/300\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.1851 - acc: 0.9468\n",
      "Epoch 1/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 1.1356 - acc: 0.7379\n",
      "Epoch 2/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.6200 - acc: 0.8569\n",
      "Epoch 3/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.5179 - acc: 0.8709\n",
      "Epoch 4/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.4692 - acc: 0.8795\n",
      "Epoch 5/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.4391 - acc: 0.8850\n",
      "Epoch 6/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.4183 - acc: 0.8891\n",
      "Epoch 7/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.4027 - acc: 0.8917\n",
      "Epoch 8/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.3906 - acc: 0.8945\n",
      "Epoch 9/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.3806 - acc: 0.8965\n",
      "Epoch 10/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.3724 - acc: 0.8983\n",
      "Epoch 11/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.3653 - acc: 0.8995\n",
      "Epoch 12/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.3592 - acc: 0.9012\n",
      "Epoch 13/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.3538 - acc: 0.9020\n",
      "Epoch 14/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.3491 - acc: 0.9036\n",
      "Epoch 15/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.3448 - acc: 0.9043\n",
      "Epoch 16/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.3410 - acc: 0.9054\n",
      "Epoch 17/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.3376 - acc: 0.9064\n",
      "Epoch 18/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.3344 - acc: 0.9071\n",
      "Epoch 19/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.3314 - acc: 0.9080\n",
      "Epoch 20/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.3287 - acc: 0.9082\n",
      "Epoch 21/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.3262 - acc: 0.9096\n",
      "Epoch 22/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.3238 - acc: 0.9094\n",
      "Epoch 23/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.3216 - acc: 0.9102\n",
      "Epoch 24/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.3196 - acc: 0.9109\n",
      "Epoch 25/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.3177 - acc: 0.9112\n",
      "Epoch 26/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.3158 - acc: 0.9118\n",
      "Epoch 27/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.3140 - acc: 0.9118\n",
      "Epoch 28/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.3125 - acc: 0.9127\n",
      "Epoch 29/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.3108 - acc: 0.9131\n",
      "Epoch 30/300\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.3094 - acc: 0.9136\n",
      "Epoch 31/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.3080 - acc: 0.9138\n",
      "Epoch 32/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.3067 - acc: 0.9144\n",
      "Epoch 33/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.3054 - acc: 0.9147\n",
      "Epoch 34/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.3041 - acc: 0.9149\n",
      "Epoch 35/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.3030 - acc: 0.9157\n",
      "Epoch 36/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.3019 - acc: 0.9157\n",
      "Epoch 37/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.3008 - acc: 0.9164\n",
      "Epoch 38/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2998 - acc: 0.9167\n",
      "Epoch 39/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2988 - acc: 0.9169\n",
      "Epoch 40/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2977 - acc: 0.9172\n",
      "Epoch 41/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2969 - acc: 0.9176\n",
      "Epoch 42/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2960 - acc: 0.9176\n",
      "Epoch 43/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2951 - acc: 0.9175\n",
      "Epoch 44/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2942 - acc: 0.9181\n",
      "Epoch 45/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2935 - acc: 0.9182\n",
      "Epoch 46/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2927 - acc: 0.9182\n",
      "Epoch 47/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2918 - acc: 0.9187\n",
      "Epoch 48/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2911 - acc: 0.9190\n",
      "Epoch 49/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2904 - acc: 0.9190\n",
      "Epoch 50/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2897 - acc: 0.9193\n",
      "Epoch 51/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2890 - acc: 0.9193\n",
      "Epoch 52/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2883 - acc: 0.9192\n",
      "Epoch 53/300\n",
      "42000/42000 [==============================] - 3s 67us/step - loss: 0.2877 - acc: 0.9196\n",
      "Epoch 54/300\n",
      "42000/42000 [==============================] - 2s 47us/step - loss: 0.2871 - acc: 0.9198\n",
      "Epoch 55/300\n",
      "42000/42000 [==============================] - 2s 48us/step - loss: 0.2865 - acc: 0.9204\n",
      "Epoch 56/300\n",
      "42000/42000 [==============================] - 2s 49us/step - loss: 0.2859 - acc: 0.9200\n",
      "Epoch 57/300\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.2853 - acc: 0.9203\n",
      "Epoch 58/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2848 - acc: 0.9203\n",
      "Epoch 59/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2842 - acc: 0.9205\n",
      "Epoch 60/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2837 - acc: 0.9209\n",
      "Epoch 61/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2831 - acc: 0.9213\n",
      "Epoch 62/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2826 - acc: 0.9216\n",
      "Epoch 63/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2821 - acc: 0.9215\n",
      "Epoch 64/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2816 - acc: 0.9220\n",
      "Epoch 65/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2811 - acc: 0.9217\n",
      "Epoch 66/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2807 - acc: 0.9216\n",
      "Epoch 67/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2802 - acc: 0.9221\n",
      "Epoch 68/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2797 - acc: 0.9221\n",
      "Epoch 69/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2793 - acc: 0.9224\n",
      "Epoch 70/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2789 - acc: 0.9226\n",
      "Epoch 71/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2784 - acc: 0.9223\n",
      "Epoch 72/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2780 - acc: 0.9225\n",
      "Epoch 73/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2776 - acc: 0.9230\n",
      "Epoch 74/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2772 - acc: 0.9234\n",
      "Epoch 75/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2768 - acc: 0.9230\n",
      "Epoch 76/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2764 - acc: 0.9228\n",
      "Epoch 77/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2761 - acc: 0.9230\n",
      "Epoch 78/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2756 - acc: 0.9234\n",
      "Epoch 79/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.2753 - acc: 0.9232\n",
      "Epoch 80/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2749 - acc: 0.9239\n",
      "Epoch 81/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2745 - acc: 0.9239\n",
      "Epoch 82/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2742 - acc: 0.9237\n",
      "Epoch 83/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2738 - acc: 0.9239\n",
      "Epoch 84/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2735 - acc: 0.9235\n",
      "Epoch 85/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2732 - acc: 0.9239\n",
      "Epoch 86/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2729 - acc: 0.9238\n",
      "Epoch 87/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2725 - acc: 0.9242\n",
      "Epoch 88/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2722 - acc: 0.9242\n",
      "Epoch 89/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2719 - acc: 0.9241\n",
      "Epoch 90/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2716 - acc: 0.9242\n",
      "Epoch 91/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2713 - acc: 0.9241\n",
      "Epoch 92/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2710 - acc: 0.9241\n",
      "Epoch 93/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2707 - acc: 0.9244\n",
      "Epoch 94/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2704 - acc: 0.9243\n",
      "Epoch 95/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2701 - acc: 0.9247\n",
      "Epoch 96/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2698 - acc: 0.9244\n",
      "Epoch 97/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2695 - acc: 0.9247\n",
      "Epoch 98/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2692 - acc: 0.9247\n",
      "Epoch 99/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2690 - acc: 0.9248\n",
      "Epoch 100/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2686 - acc: 0.9250\n",
      "Epoch 101/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2685 - acc: 0.9250\n",
      "Epoch 102/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2682 - acc: 0.9251\n",
      "Epoch 103/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.2679 - acc: 0.9251\n",
      "Epoch 104/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2676 - acc: 0.9250\n",
      "Epoch 105/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2674 - acc: 0.9251\n",
      "Epoch 106/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2672 - acc: 0.9255\n",
      "Epoch 107/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2669 - acc: 0.9253\n",
      "Epoch 108/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2666 - acc: 0.9253\n",
      "Epoch 109/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2664 - acc: 0.9253\n",
      "Epoch 110/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2662 - acc: 0.9252\n",
      "Epoch 111/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2659 - acc: 0.9256\n",
      "Epoch 112/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2657 - acc: 0.9259\n",
      "Epoch 113/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2655 - acc: 0.9256\n",
      "Epoch 114/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2653 - acc: 0.9255\n",
      "Epoch 115/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2651 - acc: 0.9257\n",
      "Epoch 116/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2649 - acc: 0.9260\n",
      "Epoch 117/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2646 - acc: 0.9260\n",
      "Epoch 118/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2644 - acc: 0.9260\n",
      "Epoch 119/300\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.2642 - acc: 0.9259\n",
      "Epoch 120/300\n",
      "42000/42000 [==============================] - 2s 49us/step - loss: 0.2639 - acc: 0.9263\n",
      "Epoch 121/300\n",
      "42000/42000 [==============================] - 2s 48us/step - loss: 0.2638 - acc: 0.9258\n",
      "Epoch 122/300\n",
      "42000/42000 [==============================] - 2s 48us/step - loss: 0.2635 - acc: 0.9264\n",
      "Epoch 123/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2634 - acc: 0.9261\n",
      "Epoch 124/300\n",
      "42000/42000 [==============================] - 3s 80us/step - loss: 0.2632 - acc: 0.9262\n",
      "Epoch 125/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2630 - acc: 0.9261\n",
      "Epoch 126/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2628 - acc: 0.9258\n",
      "Epoch 127/300\n",
      "42000/42000 [==============================] - 4s 86us/step - loss: 0.2626 - acc: 0.9260\n",
      "Epoch 128/300\n",
      "42000/42000 [==============================] - 4s 88us/step - loss: 0.2623 - acc: 0.9265\n",
      "Epoch 129/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2622 - acc: 0.9266\n",
      "Epoch 130/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2620 - acc: 0.9266\n",
      "Epoch 131/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2618 - acc: 0.9265\n",
      "Epoch 132/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2617 - acc: 0.9264\n",
      "Epoch 133/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2615 - acc: 0.9267\n",
      "Epoch 134/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2613 - acc: 0.9266\n",
      "Epoch 135/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2611 - acc: 0.9267\n",
      "Epoch 136/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2609 - acc: 0.9270\n",
      "Epoch 137/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2607 - acc: 0.9269\n",
      "Epoch 138/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2605 - acc: 0.9268\n",
      "Epoch 139/300\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.2603 - acc: 0.9270\n",
      "Epoch 140/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2601 - acc: 0.9269\n",
      "Epoch 141/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2600 - acc: 0.9267\n",
      "Epoch 142/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2599 - acc: 0.9270\n",
      "Epoch 143/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2597 - acc: 0.9270\n",
      "Epoch 144/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2594 - acc: 0.9271\n",
      "Epoch 145/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2594 - acc: 0.9270\n",
      "Epoch 146/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2591 - acc: 0.9273\n",
      "Epoch 147/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2590 - acc: 0.9274\n",
      "Epoch 148/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2589 - acc: 0.9277\n",
      "Epoch 149/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2587 - acc: 0.9273\n",
      "Epoch 150/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2586 - acc: 0.9276\n",
      "Epoch 151/300\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.2584 - acc: 0.9275\n",
      "Epoch 152/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2582 - acc: 0.9277\n",
      "Epoch 153/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2580 - acc: 0.9275\n",
      "Epoch 154/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2579 - acc: 0.9273\n",
      "Epoch 155/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2578 - acc: 0.9272\n",
      "Epoch 156/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2576 - acc: 0.9275\n",
      "Epoch 157/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2574 - acc: 0.9280\n",
      "Epoch 158/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2573 - acc: 0.9274\n",
      "Epoch 159/300\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.2572 - acc: 0.9277\n",
      "Epoch 160/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2570 - acc: 0.9277\n",
      "Epoch 161/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2569 - acc: 0.9280\n",
      "Epoch 162/300\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.2567 - acc: 0.9276\n",
      "Epoch 163/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2566 - acc: 0.9283\n",
      "Epoch 164/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2565 - acc: 0.9279\n",
      "Epoch 165/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2563 - acc: 0.9282\n",
      "Epoch 166/300\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.2562 - acc: 0.9283\n",
      "Epoch 167/300\n",
      "42000/42000 [==============================] - 4s 88us/step - loss: 0.2560 - acc: 0.9281\n",
      "Epoch 168/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2559 - acc: 0.9279\n",
      "Epoch 169/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2557 - acc: 0.9283\n",
      "Epoch 170/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2556 - acc: 0.9280\n",
      "Epoch 171/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2555 - acc: 0.9283\n",
      "Epoch 172/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2554 - acc: 0.9284\n",
      "Epoch 173/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2552 - acc: 0.9286\n",
      "Epoch 174/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2551 - acc: 0.9280\n",
      "Epoch 175/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2549 - acc: 0.9283\n",
      "Epoch 176/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2548 - acc: 0.9286\n",
      "Epoch 177/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2547 - acc: 0.9286\n",
      "Epoch 178/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2546 - acc: 0.9282\n",
      "Epoch 179/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2545 - acc: 0.9286\n",
      "Epoch 180/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2543 - acc: 0.9289\n",
      "Epoch 181/300\n",
      "42000/42000 [==============================] - 4s 87us/step - loss: 0.2542 - acc: 0.9286\n",
      "Epoch 182/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2540 - acc: 0.9284\n",
      "Epoch 183/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2540 - acc: 0.9287\n",
      "Epoch 184/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2539 - acc: 0.9290\n",
      "Epoch 185/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2537 - acc: 0.9290\n",
      "Epoch 186/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2535 - acc: 0.9290\n",
      "Epoch 187/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2535 - acc: 0.9288\n",
      "Epoch 188/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2534 - acc: 0.9289\n",
      "Epoch 189/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2532 - acc: 0.9292\n",
      "Epoch 190/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2531 - acc: 0.9290\n",
      "Epoch 191/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2530 - acc: 0.9291\n",
      "Epoch 192/300\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.2528 - acc: 0.9293\n",
      "Epoch 193/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2528 - acc: 0.9294\n",
      "Epoch 194/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2527 - acc: 0.9292\n",
      "Epoch 195/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2525 - acc: 0.9297\n",
      "Epoch 196/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2524 - acc: 0.9295\n",
      "Epoch 197/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2523 - acc: 0.9294\n",
      "Epoch 198/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2522 - acc: 0.9297\n",
      "Epoch 199/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2521 - acc: 0.9295\n",
      "Epoch 200/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2520 - acc: 0.9296\n",
      "Epoch 201/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2519 - acc: 0.9298\n",
      "Epoch 202/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2517 - acc: 0.9299\n",
      "Epoch 203/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2517 - acc: 0.9296\n",
      "Epoch 204/300\n",
      "42000/42000 [==============================] - 4s 89us/step - loss: 0.2515 - acc: 0.9298\n",
      "Epoch 205/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2515 - acc: 0.9298\n",
      "Epoch 206/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2513 - acc: 0.9299\n",
      "Epoch 207/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2513 - acc: 0.9302\n",
      "Epoch 208/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2510 - acc: 0.9300\n",
      "Epoch 209/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2511 - acc: 0.9299\n",
      "Epoch 210/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2509 - acc: 0.9301\n",
      "Epoch 211/300\n",
      "42000/42000 [==============================] - 4s 90us/step - loss: 0.2508 - acc: 0.9299\n",
      "Epoch 212/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2507 - acc: 0.9300\n",
      "Epoch 213/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2506 - acc: 0.9302\n",
      "Epoch 214/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2505 - acc: 0.9304\n",
      "Epoch 215/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2504 - acc: 0.9303\n",
      "Epoch 216/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2503 - acc: 0.9303\n",
      "Epoch 217/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2502 - acc: 0.9302\n",
      "Epoch 218/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2501 - acc: 0.9304\n",
      "Epoch 219/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2500 - acc: 0.9302\n",
      "Epoch 220/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2500 - acc: 0.9304\n",
      "Epoch 221/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2498 - acc: 0.9303\n",
      "Epoch 222/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2497 - acc: 0.9305\n",
      "Epoch 223/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2497 - acc: 0.9304\n",
      "Epoch 224/300\n",
      "42000/42000 [==============================] - 2s 48us/step - loss: 0.2495 - acc: 0.9307\n",
      "Epoch 225/300\n",
      "42000/42000 [==============================] - 2s 48us/step - loss: 0.2495 - acc: 0.9304\n",
      "Epoch 226/300\n",
      "42000/42000 [==============================] - 2s 49us/step - loss: 0.2494 - acc: 0.9310\n",
      "Epoch 227/300\n",
      "42000/42000 [==============================] - 3s 67us/step - loss: 0.2493 - acc: 0.9306\n",
      "Epoch 228/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2492 - acc: 0.9306\n",
      "Epoch 229/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2491 - acc: 0.9305\n",
      "Epoch 230/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2489 - acc: 0.9304\n",
      "Epoch 231/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2489 - acc: 0.9306\n",
      "Epoch 232/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2487 - acc: 0.9305\n",
      "Epoch 233/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2487 - acc: 0.9308\n",
      "Epoch 234/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2486 - acc: 0.9307\n",
      "Epoch 235/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2485 - acc: 0.9307\n",
      "Epoch 236/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2484 - acc: 0.9306\n",
      "Epoch 237/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2483 - acc: 0.9312\n",
      "Epoch 238/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2483 - acc: 0.9308\n",
      "Epoch 239/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2482 - acc: 0.9312\n",
      "Epoch 240/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2481 - acc: 0.9311\n",
      "Epoch 241/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2480 - acc: 0.9309\n",
      "Epoch 242/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2479 - acc: 0.9309\n",
      "Epoch 243/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2478 - acc: 0.9311\n",
      "Epoch 244/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2478 - acc: 0.9309\n",
      "Epoch 245/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2477 - acc: 0.9306\n",
      "Epoch 246/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2475 - acc: 0.9311\n",
      "Epoch 247/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2475 - acc: 0.9308\n",
      "Epoch 248/300\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.2474 - acc: 0.9311\n",
      "Epoch 249/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2473 - acc: 0.9310\n",
      "Epoch 250/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2473 - acc: 0.9311\n",
      "Epoch 251/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2472 - acc: 0.9312\n",
      "Epoch 252/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2470 - acc: 0.9312\n",
      "Epoch 253/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2470 - acc: 0.9314\n",
      "Epoch 254/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2469 - acc: 0.9311\n",
      "Epoch 255/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2468 - acc: 0.9308\n",
      "Epoch 256/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2467 - acc: 0.9311\n",
      "Epoch 257/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2467 - acc: 0.9310\n",
      "Epoch 258/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2465 - acc: 0.9313\n",
      "Epoch 259/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2465 - acc: 0.9314\n",
      "Epoch 260/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2464 - acc: 0.9311\n",
      "Epoch 261/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2464 - acc: 0.9311\n",
      "Epoch 262/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2463 - acc: 0.9313\n",
      "Epoch 263/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2462 - acc: 0.9312\n",
      "Epoch 264/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2461 - acc: 0.9315\n",
      "Epoch 265/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2461 - acc: 0.9316\n",
      "Epoch 266/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2459 - acc: 0.9313\n",
      "Epoch 267/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2459 - acc: 0.9317\n",
      "Epoch 268/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2458 - acc: 0.9320\n",
      "Epoch 269/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2458 - acc: 0.9315\n",
      "Epoch 270/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2456 - acc: 0.9316\n",
      "Epoch 271/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2456 - acc: 0.9319\n",
      "Epoch 272/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2455 - acc: 0.9316\n",
      "Epoch 273/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2455 - acc: 0.9315\n",
      "Epoch 274/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2453 - acc: 0.9319\n",
      "Epoch 275/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2452 - acc: 0.9318\n",
      "Epoch 276/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2452 - acc: 0.9315\n",
      "Epoch 277/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2451 - acc: 0.9317\n",
      "Epoch 278/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2450 - acc: 0.9313\n",
      "Epoch 279/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2450 - acc: 0.9318\n",
      "Epoch 280/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2449 - acc: 0.9321\n",
      "Epoch 281/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2448 - acc: 0.9320\n",
      "Epoch 282/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2447 - acc: 0.9318\n",
      "Epoch 283/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2447 - acc: 0.9317\n",
      "Epoch 284/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2446 - acc: 0.9319\n",
      "Epoch 285/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2445 - acc: 0.9320\n",
      "Epoch 286/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2445 - acc: 0.9322\n",
      "Epoch 287/300\n",
      "42000/42000 [==============================] - 2s 53us/step - loss: 0.2444 - acc: 0.9320\n",
      "Epoch 288/300\n",
      "42000/42000 [==============================] - 2s 47us/step - loss: 0.2444 - acc: 0.9321\n",
      "Epoch 289/300\n",
      "42000/42000 [==============================] - 2s 48us/step - loss: 0.2443 - acc: 0.9322\n",
      "Epoch 290/300\n",
      "42000/42000 [==============================] - 2s 48us/step - loss: 0.2442 - acc: 0.9318\n",
      "Epoch 291/300\n",
      "42000/42000 [==============================] - 3s 65us/step - loss: 0.2441 - acc: 0.9324\n",
      "Epoch 292/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2441 - acc: 0.9319\n",
      "Epoch 293/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2440 - acc: 0.9324\n",
      "Epoch 294/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2439 - acc: 0.9322\n",
      "Epoch 295/300\n",
      "42000/42000 [==============================] - 3s 81us/step - loss: 0.2439 - acc: 0.9321\n",
      "Epoch 296/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2438 - acc: 0.9320\n",
      "Epoch 297/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2437 - acc: 0.9322\n",
      "Epoch 298/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2437 - acc: 0.9321\n",
      "Epoch 299/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2436 - acc: 0.9320\n",
      "Epoch 300/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2435 - acc: 0.9322\n",
      "Epoch 1/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.5649 - acc: 0.8544\n",
      "Epoch 2/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.3228 - acc: 0.9098\n",
      "Epoch 3/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2951 - acc: 0.9172\n",
      "Epoch 4/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2829 - acc: 0.9209\n",
      "Epoch 5/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2746 - acc: 0.9229\n",
      "Epoch 6/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2693 - acc: 0.9246\n",
      "Epoch 7/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2653 - acc: 0.9262\n",
      "Epoch 8/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.2622 - acc: 0.9280\n",
      "Epoch 9/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2596 - acc: 0.9290\n",
      "Epoch 10/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2574 - acc: 0.9293\n",
      "Epoch 11/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2561 - acc: 0.9301\n",
      "Epoch 12/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2543 - acc: 0.9305\n",
      "Epoch 13/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2528 - acc: 0.9317\n",
      "Epoch 14/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2516 - acc: 0.9317\n",
      "Epoch 15/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2507 - acc: 0.9327\n",
      "Epoch 16/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2501 - acc: 0.9331\n",
      "Epoch 17/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2490 - acc: 0.9330\n",
      "Epoch 18/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.2479 - acc: 0.9326\n",
      "Epoch 19/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2477 - acc: 0.9340\n",
      "Epoch 20/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2477 - acc: 0.9336\n",
      "Epoch 21/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2466 - acc: 0.9340\n",
      "Epoch 22/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2462 - acc: 0.9341\n",
      "Epoch 23/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2460 - acc: 0.9349\n",
      "Epoch 24/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2451 - acc: 0.9343\n",
      "Epoch 25/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2453 - acc: 0.9350\n",
      "Epoch 26/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2450 - acc: 0.9349\n",
      "Epoch 27/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2445 - acc: 0.9355\n",
      "Epoch 28/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2444 - acc: 0.9353\n",
      "Epoch 29/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.2444 - acc: 0.9355\n",
      "Epoch 30/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2435 - acc: 0.9357\n",
      "Epoch 31/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2435 - acc: 0.9359\n",
      "Epoch 32/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.2430 - acc: 0.9361\n",
      "Epoch 33/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2432 - acc: 0.9362\n",
      "Epoch 34/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2424 - acc: 0.9362\n",
      "Epoch 35/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2424 - acc: 0.9364\n",
      "Epoch 36/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2423 - acc: 0.9368\n",
      "Epoch 37/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2421 - acc: 0.9370\n",
      "Epoch 38/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2421 - acc: 0.9366\n",
      "Epoch 39/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2420 - acc: 0.9368\n",
      "Epoch 40/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2413 - acc: 0.9369\n",
      "Epoch 41/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2413 - acc: 0.9367\n",
      "Epoch 42/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2415 - acc: 0.9370\n",
      "Epoch 43/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2413 - acc: 0.9375\n",
      "Epoch 44/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2408 - acc: 0.9377\n",
      "Epoch 45/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2413 - acc: 0.9373\n",
      "Epoch 46/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2406 - acc: 0.9380\n",
      "Epoch 47/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2407 - acc: 0.9372\n",
      "Epoch 48/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2404 - acc: 0.9379\n",
      "Epoch 49/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2402 - acc: 0.9377\n",
      "Epoch 50/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2401 - acc: 0.9375\n",
      "Epoch 51/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2403 - acc: 0.9375\n",
      "Epoch 52/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2403 - acc: 0.9382\n",
      "Epoch 53/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2403 - acc: 0.9379\n",
      "Epoch 54/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2400 - acc: 0.9381\n",
      "Epoch 55/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2398 - acc: 0.9385\n",
      "Epoch 56/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2400 - acc: 0.9383\n",
      "Epoch 57/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.2399 - acc: 0.9389\n",
      "Epoch 58/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2394 - acc: 0.9385\n",
      "Epoch 59/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2396 - acc: 0.9383\n",
      "Epoch 60/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2397 - acc: 0.9387\n",
      "Epoch 61/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2394 - acc: 0.9380\n",
      "Epoch 62/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2396 - acc: 0.9384\n",
      "Epoch 63/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2394 - acc: 0.9385\n",
      "Epoch 64/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2395 - acc: 0.9386\n",
      "Epoch 65/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2390 - acc: 0.9384\n",
      "Epoch 66/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2391 - acc: 0.9394\n",
      "Epoch 67/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2390 - acc: 0.9386\n",
      "Epoch 68/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2392 - acc: 0.9389\n",
      "Epoch 69/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2393 - acc: 0.9385\n",
      "Epoch 70/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2391 - acc: 0.9393\n",
      "Epoch 71/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2390 - acc: 0.9393\n",
      "Epoch 72/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.2388 - acc: 0.9390\n",
      "Epoch 73/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2386 - acc: 0.9397\n",
      "Epoch 74/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.2386 - acc: 0.9392\n",
      "Epoch 75/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2389 - acc: 0.9395\n",
      "Epoch 76/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2385 - acc: 0.9392\n",
      "Epoch 77/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2385 - acc: 0.9395\n",
      "Epoch 78/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2384 - acc: 0.9397\n",
      "Epoch 79/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2387 - acc: 0.9395\n",
      "Epoch 80/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2384 - acc: 0.9394\n",
      "Epoch 81/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2388 - acc: 0.9398\n",
      "Epoch 82/300\n",
      "42000/42000 [==============================] - 4s 97us/step - loss: 0.2385 - acc: 0.9393\n",
      "Epoch 83/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2383 - acc: 0.9399\n",
      "Epoch 84/300\n",
      "42000/42000 [==============================] - 3s 78us/step - loss: 0.2386 - acc: 0.9405\n",
      "Epoch 85/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2385 - acc: 0.9393\n",
      "Epoch 86/300\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.2384 - acc: 0.9398\n",
      "Epoch 87/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2385 - acc: 0.9399\n",
      "Epoch 88/300\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.2383 - acc: 0.9401\n",
      "Epoch 89/300\n",
      "42000/42000 [==============================] - 4s 99us/step - loss: 0.2381 - acc: 0.9406\n",
      "Epoch 90/300\n",
      "42000/42000 [==============================] - 5s 108us/step - loss: 0.2383 - acc: 0.9401\n",
      "Epoch 91/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2385 - acc: 0.9401\n",
      "Epoch 92/300\n",
      "42000/42000 [==============================] - 5s 108us/step - loss: 0.2380 - acc: 0.9400\n",
      "Epoch 93/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2383 - acc: 0.9397\n",
      "Epoch 94/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2384 - acc: 0.9405\n",
      "Epoch 95/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2377 - acc: 0.9403\n",
      "Epoch 96/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2383 - acc: 0.9402\n",
      "Epoch 97/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2379 - acc: 0.9405\n",
      "Epoch 98/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2382 - acc: 0.9404\n",
      "Epoch 99/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2379 - acc: 0.9412\n",
      "Epoch 100/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2380 - acc: 0.9401\n",
      "Epoch 101/300\n",
      "42000/42000 [==============================] - 5s 108us/step - loss: 0.2381 - acc: 0.9406\n",
      "Epoch 102/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2381 - acc: 0.9408\n",
      "Epoch 103/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2374 - acc: 0.9408\n",
      "Epoch 104/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2378 - acc: 0.9411\n",
      "Epoch 105/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2378 - acc: 0.9404\n",
      "Epoch 106/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2374 - acc: 0.9402\n",
      "Epoch 107/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2377 - acc: 0.9405\n",
      "Epoch 108/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.2379 - acc: 0.9406\n",
      "Epoch 109/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2377 - acc: 0.9413\n",
      "Epoch 110/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.2375 - acc: 0.9406\n",
      "Epoch 111/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2377 - acc: 0.9409\n",
      "Epoch 112/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2378 - acc: 0.9408\n",
      "Epoch 113/300\n",
      "42000/42000 [==============================] - 5s 108us/step - loss: 0.2378 - acc: 0.9404\n",
      "Epoch 114/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2376 - acc: 0.9403\n",
      "Epoch 115/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.2377 - acc: 0.9413\n",
      "Epoch 116/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2376 - acc: 0.9404\n",
      "Epoch 117/300\n",
      "42000/42000 [==============================] - 5s 108us/step - loss: 0.2376 - acc: 0.9413\n",
      "Epoch 118/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2377 - acc: 0.9411\n",
      "Epoch 119/300\n",
      "42000/42000 [==============================] - 5s 109us/step - loss: 0.2371 - acc: 0.9414\n",
      "Epoch 120/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2376 - acc: 0.9407\n",
      "Epoch 121/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2376 - acc: 0.9408\n",
      "Epoch 122/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2374 - acc: 0.9411\n",
      "Epoch 123/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2374 - acc: 0.9412\n",
      "Epoch 124/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2375 - acc: 0.9410\n",
      "Epoch 125/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2373 - acc: 0.9413\n",
      "Epoch 126/300\n",
      "42000/42000 [==============================] - 5s 107us/step - loss: 0.2374 - acc: 0.9412\n",
      "Epoch 127/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2372 - acc: 0.9415\n",
      "Epoch 128/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2374 - acc: 0.9418\n",
      "Epoch 129/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2379 - acc: 0.9405\n",
      "Epoch 130/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2374 - acc: 0.9420\n",
      "Epoch 131/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2374 - acc: 0.9412\n",
      "Epoch 132/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2374 - acc: 0.9418\n",
      "Epoch 133/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2373 - acc: 0.9413\n",
      "Epoch 134/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2372 - acc: 0.9418\n",
      "Epoch 135/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2371 - acc: 0.9411\n",
      "Epoch 136/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2373 - acc: 0.9406\n",
      "Epoch 137/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2369 - acc: 0.9417\n",
      "Epoch 138/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2374 - acc: 0.9417\n",
      "Epoch 139/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2371 - acc: 0.9418\n",
      "Epoch 140/300\n",
      "42000/42000 [==============================] - 5s 107us/step - loss: 0.2372 - acc: 0.9419\n",
      "Epoch 141/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2370 - acc: 0.9416\n",
      "Epoch 142/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2371 - acc: 0.9414\n",
      "Epoch 143/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2372 - acc: 0.9409\n",
      "Epoch 144/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2373 - acc: 0.9416\n",
      "Epoch 145/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2372 - acc: 0.9417\n",
      "Epoch 146/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2373 - acc: 0.9416\n",
      "Epoch 147/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2370 - acc: 0.9416\n",
      "Epoch 148/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2370 - acc: 0.9417\n",
      "Epoch 149/300\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.2372 - acc: 0.9422\n",
      "Epoch 150/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2370 - acc: 0.9413\n",
      "Epoch 151/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2371 - acc: 0.9417\n",
      "Epoch 152/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2370 - acc: 0.9420\n",
      "Epoch 153/300\n",
      "42000/42000 [==============================] - 2s 59us/step - loss: 0.2373 - acc: 0.9414\n",
      "Epoch 154/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2373 - acc: 0.9423\n",
      "Epoch 155/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2372 - acc: 0.9416\n",
      "Epoch 156/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2374 - acc: 0.9422\n",
      "Epoch 157/300\n",
      "42000/42000 [==============================] - 4s 84us/step - loss: 0.2371 - acc: 0.9420\n",
      "Epoch 158/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2371 - acc: 0.9420\n",
      "Epoch 159/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2368 - acc: 0.9428\n",
      "Epoch 160/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2369 - acc: 0.9423\n",
      "Epoch 161/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2370 - acc: 0.9426\n",
      "Epoch 162/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2368 - acc: 0.9419\n",
      "Epoch 163/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.2369 - acc: 0.9426\n",
      "Epoch 164/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2369 - acc: 0.9423\n",
      "Epoch 165/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2373 - acc: 0.9422\n",
      "Epoch 166/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2372 - acc: 0.9425\n",
      "Epoch 167/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2368 - acc: 0.9423\n",
      "Epoch 168/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2370 - acc: 0.9418\n",
      "Epoch 169/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2371 - acc: 0.9420\n",
      "Epoch 170/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2368 - acc: 0.9423\n",
      "Epoch 171/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2368 - acc: 0.9422\n",
      "Epoch 172/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2372 - acc: 0.9423\n",
      "Epoch 173/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2368 - acc: 0.9431\n",
      "Epoch 174/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2373 - acc: 0.9422\n",
      "Epoch 175/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2370 - acc: 0.9425\n",
      "Epoch 176/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2371 - acc: 0.9417\n",
      "Epoch 177/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2367 - acc: 0.9426\n",
      "Epoch 178/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2372 - acc: 0.9423\n",
      "Epoch 179/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2371 - acc: 0.9426\n",
      "Epoch 180/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2370 - acc: 0.9422\n",
      "Epoch 181/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2370 - acc: 0.9427\n",
      "Epoch 182/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2369 - acc: 0.9422\n",
      "Epoch 183/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2370 - acc: 0.9426\n",
      "Epoch 184/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2371 - acc: 0.9425\n",
      "Epoch 185/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2371 - acc: 0.9427\n",
      "Epoch 186/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2373 - acc: 0.9428\n",
      "Epoch 187/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2372 - acc: 0.9428\n",
      "Epoch 188/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2367 - acc: 0.9423\n",
      "Epoch 189/300\n",
      "42000/42000 [==============================] - 4s 98us/step - loss: 0.2371 - acc: 0.9421\n",
      "Epoch 190/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2373 - acc: 0.9431\n",
      "Epoch 191/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2368 - acc: 0.9428\n",
      "Epoch 192/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2369 - acc: 0.9423\n",
      "Epoch 193/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2365 - acc: 0.9427\n",
      "Epoch 194/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2367 - acc: 0.9426\n",
      "Epoch 195/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2369 - acc: 0.9428\n",
      "Epoch 196/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2370 - acc: 0.9429\n",
      "Epoch 197/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2372 - acc: 0.9434\n",
      "Epoch 198/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2373 - acc: 0.9425\n",
      "Epoch 199/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2374 - acc: 0.9431\n",
      "Epoch 200/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2369 - acc: 0.9426\n",
      "Epoch 201/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2368 - acc: 0.9423\n",
      "Epoch 202/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2372 - acc: 0.9432\n",
      "Epoch 203/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2372 - acc: 0.9429\n",
      "Epoch 204/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2368 - acc: 0.9430\n",
      "Epoch 205/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2371 - acc: 0.9430\n",
      "Epoch 206/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2370 - acc: 0.9427\n",
      "Epoch 207/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2371 - acc: 0.9430\n",
      "Epoch 208/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2371 - acc: 0.9428\n",
      "Epoch 209/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2370 - acc: 0.9425\n",
      "Epoch 210/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2370 - acc: 0.9435\n",
      "Epoch 211/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2371 - acc: 0.9435\n",
      "Epoch 212/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2370 - acc: 0.9425\n",
      "Epoch 213/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2373 - acc: 0.9431\n",
      "Epoch 214/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2369 - acc: 0.9432\n",
      "Epoch 215/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2369 - acc: 0.9422\n",
      "Epoch 216/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2372 - acc: 0.9429\n",
      "Epoch 217/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2370 - acc: 0.9430\n",
      "Epoch 218/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2368 - acc: 0.9431\n",
      "Epoch 219/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2371 - acc: 0.9429\n",
      "Epoch 220/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2372 - acc: 0.9432\n",
      "Epoch 221/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2371 - acc: 0.9430\n",
      "Epoch 222/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2370 - acc: 0.9437\n",
      "Epoch 223/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2372 - acc: 0.9437\n",
      "Epoch 224/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2371 - acc: 0.9425\n",
      "Epoch 225/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2369 - acc: 0.9429\n",
      "Epoch 226/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2373 - acc: 0.9430\n",
      "Epoch 227/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2374 - acc: 0.9430\n",
      "Epoch 228/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2370 - acc: 0.9431\n",
      "Epoch 229/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2373 - acc: 0.9442\n",
      "Epoch 230/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2365 - acc: 0.9433\n",
      "Epoch 231/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2374 - acc: 0.9434\n",
      "Epoch 232/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2370 - acc: 0.9434\n",
      "Epoch 233/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2374 - acc: 0.9433\n",
      "Epoch 234/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2372 - acc: 0.9434\n",
      "Epoch 235/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2374 - acc: 0.9441\n",
      "Epoch 236/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2373 - acc: 0.9428\n",
      "Epoch 237/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2371 - acc: 0.9434\n",
      "Epoch 238/300\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.2370 - acc: 0.9430\n",
      "Epoch 239/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2373 - acc: 0.9435\n",
      "Epoch 240/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2371 - acc: 0.9436\n",
      "Epoch 241/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2369 - acc: 0.9437\n",
      "Epoch 242/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2371 - acc: 0.9430\n",
      "Epoch 243/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2368 - acc: 0.9433\n",
      "Epoch 244/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2368 - acc: 0.9433\n",
      "Epoch 245/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2372 - acc: 0.9430\n",
      "Epoch 246/300\n",
      "42000/42000 [==============================] - 4s 92us/step - loss: 0.2371 - acc: 0.9434\n",
      "Epoch 247/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2370 - acc: 0.9438\n",
      "Epoch 248/300\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.2375 - acc: 0.9436\n",
      "Epoch 249/300\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.2374 - acc: 0.9441\n",
      "Epoch 250/300\n",
      "42000/42000 [==============================] - 4s 85us/step - loss: 0.2373 - acc: 0.9436\n",
      "Epoch 251/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2373 - acc: 0.9435\n",
      "Epoch 252/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2374 - acc: 0.9442\n",
      "Epoch 253/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2374 - acc: 0.9436\n",
      "Epoch 254/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2374 - acc: 0.9434\n",
      "Epoch 255/300\n",
      "42000/42000 [==============================] - 3s 74us/step - loss: 0.2373 - acc: 0.9436\n",
      "Epoch 256/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2373 - acc: 0.9433\n",
      "Epoch 257/300\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.2373 - acc: 0.9439\n",
      "Epoch 258/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2371 - acc: 0.9438\n",
      "Epoch 259/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2369 - acc: 0.9436\n",
      "Epoch 260/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2372 - acc: 0.9442\n",
      "Epoch 261/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2372 - acc: 0.9439\n",
      "Epoch 262/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2373 - acc: 0.9434\n",
      "Epoch 263/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2376 - acc: 0.9440\n",
      "Epoch 264/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2373 - acc: 0.9437\n",
      "Epoch 265/300\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.2373 - acc: 0.9441\n",
      "Epoch 266/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2370 - acc: 0.9441\n",
      "Epoch 267/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2376 - acc: 0.9434\n",
      "Epoch 268/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2372 - acc: 0.9433\n",
      "Epoch 269/300\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.2374 - acc: 0.9432\n",
      "Epoch 270/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2377 - acc: 0.9435\n",
      "Epoch 271/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2371 - acc: 0.9441\n",
      "Epoch 272/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2371 - acc: 0.9441\n",
      "Epoch 273/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2374 - acc: 0.9439\n",
      "Epoch 274/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2372 - acc: 0.9440\n",
      "Epoch 275/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2372 - acc: 0.9439\n",
      "Epoch 276/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2369 - acc: 0.9444\n",
      "Epoch 277/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2374 - acc: 0.9439\n",
      "Epoch 278/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2367 - acc: 0.9444\n",
      "Epoch 279/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2375 - acc: 0.9444\n",
      "Epoch 280/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2373 - acc: 0.9438\n",
      "Epoch 281/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2374 - acc: 0.9439\n",
      "Epoch 282/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2371 - acc: 0.9438\n",
      "Epoch 283/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2373 - acc: 0.9438\n",
      "Epoch 284/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2372 - acc: 0.9439\n",
      "Epoch 285/300\n",
      "42000/42000 [==============================] - 4s 107us/step - loss: 0.2374 - acc: 0.9440\n",
      "Epoch 286/300\n",
      "42000/42000 [==============================] - 2s 59us/step - loss: 0.2369 - acc: 0.9446\n",
      "Epoch 287/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2375 - acc: 0.9438\n",
      "Epoch 288/300\n",
      "42000/42000 [==============================] - 2s 50us/step - loss: 0.2375 - acc: 0.9440\n",
      "Epoch 289/300\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.2371 - acc: 0.9440\n",
      "Epoch 290/300\n",
      "42000/42000 [==============================] - 2s 51us/step - loss: 0.2368 - acc: 0.9445\n",
      "Epoch 291/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2370 - acc: 0.9439\n",
      "Epoch 292/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2372 - acc: 0.9445\n",
      "Epoch 293/300\n",
      "42000/42000 [==============================] - 4s 91us/step - loss: 0.2373 - acc: 0.9448\n",
      "Epoch 294/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2375 - acc: 0.9437\n",
      "Epoch 295/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2374 - acc: 0.9445\n",
      "Epoch 296/300\n",
      "42000/42000 [==============================] - 4s 106us/step - loss: 0.2370 - acc: 0.9441\n",
      "Epoch 297/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2372 - acc: 0.9435\n",
      "Epoch 298/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2374 - acc: 0.9437\n",
      "Epoch 299/300\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.2372 - acc: 0.9441\n",
      "Epoch 300/300\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.2372 - acc: 0.9439\n"
     ]
    }
   ],
   "source": [
    "adam_model,adam_history = linear_model('adam',300, train_x,train_y) \n",
    "sgd_model,sgd_history = linear_model('sgd', 300, train_x,train_y)\n",
    "rmsprop_model,rmsprop_history =  linear_model('rmsprop', 300, train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAGDCAYAAAAVouC4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1I9BSZIkSZL6+f/HOgVN5RRpBgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f538ee38048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.943904761905 0.946833333333\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.figure()\n",
    "hist1 = rmsprop_history.history['acc']\n",
    "hist2 = sgd_history.history['acc']\n",
    "hist3 = adam_history.history['acc']\n",
    "plt.title('linear model')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('training accuracy')\n",
    "plt.plot(hist1,label='rmsprop')\n",
    "plt.plot(hist2,label='SGD')\n",
    "plt.plot(hist3,label='adam')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(hist1[-1],hist3[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(model,data,filename,file_columns = None):\n",
    "    predictions = model.predict(test_x)\n",
    "    result = predictions.argmax(axis =1 )\n",
    "    result = result.reshape((28000,1))\n",
    "    ids = (np.arange(28000)+1).reshape((28000,1))\n",
    "    result = np.hstack((ids,result))\n",
    "    m = to_kaggle_csv(result,file_columns, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[[1 2]\n",
      " [2 0]\n",
      " [3 9]\n",
      " [4 7]] (28000, 2)\n",
      "   ImageId  Label\n",
      "0        1      2\n",
      "1        2      0\n",
      "2        3      9\n",
      "3        4      7\n",
      "4        5      3\n"
     ]
    }
   ],
   "source": [
    "predictions.shape\n",
    "result = predictions.argmax(axis =1 )\n",
    "result.shape\n",
    "result = result.reshape((28000,1))\n",
    "ids = (np.arange(28000)+1).reshape((28000,1))\n",
    "result = np.hstack((ids,result))\n",
    "print(result[:4,:],result.shape)\n",
    "m = to_kaggle_csv(result,['ImageId','Label'],'submission_adam_model_300ep_no_regularization.csv')\n",
    "print(m.head())\n",
    "#plt.imshow(test_x[3,:].reshape((28,28)))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary for the linear model\n",
    "####  The output of the linear model\n",
    "the submission got the 1653th place out of 1879, this is the top 87.9% with accuracy of 91.6%, which is really bad for mnist dataset, as it's the easiest task you'll ever workon and it's far below our human level accuracy.\n",
    "Note that training accuracy is around 94.4% and the submission accuracy is around 91.6%.\n",
    "We've been training for around 270 epochs\n",
    "<img src=\"1-linearmodel.PNG\" height=\"300\" width = \"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models with regularization\n",
    "\n",
    "now the model is overfitting --> training acc is 94.4 and submission accuracy is 91.6.\n",
    "Next step is to add regularization and reduce overfitting --> thus we need a separate validation set to monitor overfitting.\n",
    "We'll find that a linear model isn't capable of overfitting and any further regularization will only reduce the training error, as the model doesn't have the capacity to overfit\n",
    "\n",
    "That's why it's a best practice in neural networks to overfit the data first to make sure that your model is good, then add regularization to generalize better, but you shouldn't spend anytime trying to prevent overfitting using a model with high bias (simple models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00392156862745\n",
      "1.0\n",
      "0.00392156862745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## validation set\n",
    "y = train_data[:,0]\n",
    "X = train_data[:,1:]\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(y.reshape((y.shape[0],1)))\n",
    "train_y = enc.transform(y.reshape((y.shape[0],1))).toarray()\n",
    "X /= 255\n",
    "test_x  = test_data/255\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_x.max())\n",
    "print(test_x.max())\n",
    "print(valid_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/150\n",
      "33600/33600 [==============================] - 4s 123us/step - loss: 2.3712 - acc: 0.1132 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 2/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 3/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 4/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 5/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 6/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 7/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 8/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 9/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 10/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 11/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 12/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 13/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3015 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 14/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 15/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 16/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 17/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 18/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 19/150\n",
      "33600/33600 [==============================] - 3s 101us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 20/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 21/150\n",
      "33600/33600 [==============================] - 4s 107us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 22/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 23/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 24/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 25/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 26/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 27/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 28/150\n",
      "33600/33600 [==============================] - 5s 135us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 29/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 30/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 31/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 32/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1082\n",
      "Epoch 33/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 34/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 35/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 36/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 37/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 38/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 39/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 40/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 41/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 42/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 43/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 44/150\n",
      "33600/33600 [==============================] - 5s 136us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 45/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1082\n",
      "Epoch 46/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 47/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 48/150\n",
      "33600/33600 [==============================] - 2s 63us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 49/150\n",
      "33600/33600 [==============================] - 4s 123us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 50/150\n",
      "33600/33600 [==============================] - 4s 122us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 51/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 52/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 53/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 54/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 55/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 56/150\n",
      "33600/33600 [==============================] - 4s 122us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 57/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 58/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 59/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 60/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 61/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 62/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 63/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 64/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1082\n",
      "Epoch 65/150\n",
      "33600/33600 [==============================] - 4s 122us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 66/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 67/150\n",
      "33600/33600 [==============================] - 4s 123us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 68/150\n",
      "33600/33600 [==============================] - 4s 122us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 69/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 70/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 71/150\n",
      "33600/33600 [==============================] - 4s 122us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 72/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 73/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 74/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 75/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 76/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 77/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 78/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 79/150\n",
      "33600/33600 [==============================] - 4s 123us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 80/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 81/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 82/150\n",
      "33600/33600 [==============================] - 4s 123us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 83/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 84/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 85/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 86/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 87/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 88/150\n",
      "33600/33600 [==============================] - 2s 60us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 89/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 90/150\n",
      "33600/33600 [==============================] - 4s 125us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 91/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 92/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 93/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 94/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 95/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 96/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 97/150\n",
      "33600/33600 [==============================] - 4s 127us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 98/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 99/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 100/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 101/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 102/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 103/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 104/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 105/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 106/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 107/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 108/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 109/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 110/150\n",
      "33600/33600 [==============================] - 4s 127us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 111/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 112/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 113/150\n",
      "33600/33600 [==============================] - 4s 126us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 114/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 115/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 116/150\n",
      "33600/33600 [==============================] - 2s 60us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 117/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 3s 83us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 118/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 119/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 120/150\n",
      "33600/33600 [==============================] - 4s 113us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 121/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 122/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 123/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 124/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 125/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 126/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 127/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 128/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 129/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 130/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 131/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 132/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 133/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 134/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 135/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 136/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 137/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 138/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 139/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 140/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 141/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1082\n",
      "Epoch 142/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 143/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 144/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1082\n",
      "Epoch 145/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 146/150\n",
      "33600/33600 [==============================] - 4s 122us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1082\n",
      "Epoch 147/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 148/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3013 - val_acc: 0.1082\n",
      "Epoch 149/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Epoch 150/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1082\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/150\n",
      "33600/33600 [==============================] - 4s 122us/step - loss: 2.3084 - acc: 0.1129 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 2/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 3/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 4/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 5/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 6/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 7/150\n",
      "33600/33600 [==============================] - 3s 97us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 8/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 9/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 10/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 11/150\n",
      "33600/33600 [==============================] - 4s 126us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 12/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 13/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1082\n",
      "Epoch 14/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 15/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3005 - val_acc: 0.1082\n",
      "Epoch 16/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3005 - val_acc: 0.1082\n",
      "Epoch 17/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 18/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 19/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 20/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 21/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 22/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 23/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 24/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1082\n",
      "Epoch 25/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 26/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 27/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 28/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 29/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 30/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 31/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 32/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 33/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 34/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 35/150\n",
      "33600/33600 [==============================] - 4s 122us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 36/150\n",
      "33600/33600 [==============================] - 2s 62us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 37/150\n",
      "33600/33600 [==============================] - 3s 82us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 38/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 39/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 40/150\n",
      "33600/33600 [==============================] - 4s 112us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 41/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1082\n",
      "Epoch 42/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 43/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 44/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 45/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 46/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3005 - val_acc: 0.1082\n",
      "Epoch 47/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 48/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 49/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 50/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 51/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 52/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 53/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 54/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 55/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 56/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 57/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 58/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3005 - val_acc: 0.1082\n",
      "Epoch 59/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3005 - val_acc: 0.1082\n",
      "Epoch 60/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 61/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 62/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 63/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 64/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 65/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 66/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 67/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 68/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 69/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1082\n",
      "Epoch 70/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 71/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 72/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 73/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 74/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 75/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 76/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 77/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 78/150\n",
      "33600/33600 [==============================] - 2s 60us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 79/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3005 - val_acc: 0.1082\n",
      "Epoch 80/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 81/150\n",
      "33600/33600 [==============================] - 4s 126us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 82/150\n",
      "33600/33600 [==============================] - 5s 135us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3005 - val_acc: 0.1082\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 84/150\n",
      "33600/33600 [==============================] - 5s 135us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 85/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 86/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 87/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 88/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 89/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 90/150\n",
      "33600/33600 [==============================] - 5s 135us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 91/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 92/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 93/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 94/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1082\n",
      "Epoch 95/150\n",
      "33600/33600 [==============================] - 4s 127us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 96/150\n",
      "33600/33600 [==============================] - 5s 134us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 97/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 98/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 99/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 100/150\n",
      "33600/33600 [==============================] - 5s 134us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 101/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 102/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 103/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 104/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 105/150\n",
      "33600/33600 [==============================] - 4s 125us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 106/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 107/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 108/150\n",
      "33600/33600 [==============================] - 4s 113us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 109/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 110/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 111/150\n",
      "33600/33600 [==============================] - 4s 111us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 112/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 113/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1082\n",
      "Epoch 114/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 115/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 116/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1082\n",
      "Epoch 117/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 118/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 119/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 120/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 121/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 122/150\n",
      "33600/33600 [==============================] - 4s 122us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 123/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 124/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1082\n",
      "Epoch 125/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 126/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3005 - val_acc: 0.1082\n",
      "Epoch 127/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 128/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 129/150\n",
      "33600/33600 [==============================] - 4s 114us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 130/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 131/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1082\n",
      "Epoch 132/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 133/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 134/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 135/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 136/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1082\n",
      "Epoch 137/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1082\n",
      "Epoch 138/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3005 - val_acc: 0.1082\n",
      "Epoch 139/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 140/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 141/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 142/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 143/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 144/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 145/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 146/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3008 - val_acc: 0.1082\n",
      "Epoch 147/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 148/150\n",
      "33600/33600 [==============================] - 2s 71us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3007 - val_acc: 0.1082\n",
      "Epoch 149/150\n",
      "33600/33600 [==============================] - 2s 60us/step - loss: 2.3010 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Epoch 150/150\n",
      "33600/33600 [==============================] - 4s 125us/step - loss: 2.3011 - acc: 0.1124 - val_loss: 2.3006 - val_acc: 0.1082\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2989 - acc: 0.1704 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 2/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2966 - val_acc: 0.1082\n",
      "Epoch 3/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 4/150\n",
      "33600/33600 [==============================] - 4s 127us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 5/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 6/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 7/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 8/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 9/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 10/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 11/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 12/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 13/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 14/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 15/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 16/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 17/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2966 - val_acc: 0.1082\n",
      "Epoch 18/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 19/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 20/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 21/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 22/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 23/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 24/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 25/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 26/150\n",
      "33600/33600 [==============================] - 4s 114us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 27/150\n",
      "33600/33600 [==============================] - 2s 60us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 28/150\n",
      "33600/33600 [==============================] - 2s 71us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 29/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 30/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 31/150\n",
      "33600/33600 [==============================] - 4s 111us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 32/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 33/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 34/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 35/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 36/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 37/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 38/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 39/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 40/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 41/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 42/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 43/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 44/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 45/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 46/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 47/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 48/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 49/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 50/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2966 - val_acc: 0.1082\n",
      "Epoch 51/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 52/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 53/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 54/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 55/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 56/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 57/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2966 - val_acc: 0.1082\n",
      "Epoch 58/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 59/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 60/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 61/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 62/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 63/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 64/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 65/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 66/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 67/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 68/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2966 - val_acc: 0.1082\n",
      "Epoch 69/150\n",
      "33600/33600 [==============================] - 2s 65us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 70/150\n",
      "33600/33600 [==============================] - 2s 60us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 71/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 72/150\n",
      "33600/33600 [==============================] - 4s 126us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 73/150\n",
      "33600/33600 [==============================] - 4s 126us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 74/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 75/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 76/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.2971 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 77/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 78/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 79/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 80/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 81/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 82/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 83/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2972 - val_acc: 0.1082\n",
      "Epoch 84/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 85/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 86/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 87/150\n",
      "33600/33600 [==============================] - 4s 126us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 88/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 89/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 90/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 91/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2971 - val_acc: 0.1082\n",
      "Epoch 92/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 93/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 94/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 95/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 96/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 97/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 98/150\n",
      "33600/33600 [==============================] - 2s 67us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2966 - val_acc: 0.1082\n",
      "Epoch 99/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 100/150\n",
      "33600/33600 [==============================] - 4s 109us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 101/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 102/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 103/150\n",
      "33600/33600 [==============================] - 4s 110us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 104/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 105/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 106/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 107/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 108/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 109/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 110/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 111/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 112/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 113/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 114/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 115/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 116/150\n",
      "33600/33600 [==============================] - 4s 114us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 117/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 118/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 119/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 120/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 121/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 122/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 123/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 124/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 125/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 126/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 127/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 128/150\n",
      "33600/33600 [==============================] - 4s 114us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 129/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 130/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 131/150\n",
      "33600/33600 [==============================] - 4s 114us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 132/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 133/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 134/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 135/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 136/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 137/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 138/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2971 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 139/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 140/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 141/150\n",
      "33600/33600 [==============================] - 3s 82us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 142/150\n",
      "33600/33600 [==============================] - 2s 60us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 143/150\n",
      "33600/33600 [==============================] - 3s 101us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2967 - val_acc: 0.1082\n",
      "Epoch 144/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 145/150\n",
      "33600/33600 [==============================] - 4s 123us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 146/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 147/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2968 - val_acc: 0.1082\n",
      "Epoch 148/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Epoch 149/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2970 - val_acc: 0.1082\n",
      "Epoch 150/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2972 - acc: 0.1124 - val_loss: 2.2969 - val_acc: 0.1082\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/150\n",
      "33600/33600 [==============================] - 4s 133us/step - loss: 2.2792 - acc: 0.3959 - val_loss: 2.2652 - val_acc: 0.5026\n",
      "Epoch 2/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2624 - acc: 0.4563 - val_loss: 2.2600 - val_acc: 0.4537\n",
      "Epoch 3/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.2600 - acc: 0.4127 - val_loss: 2.2593 - val_acc: 0.4313\n",
      "Epoch 4/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2597 - acc: 0.4182 - val_loss: 2.2592 - val_acc: 0.3707\n",
      "Epoch 5/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.2596 - acc: 0.3760 - val_loss: 2.2594 - val_acc: 0.3782\n",
      "Epoch 6/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2596 - acc: 0.3881 - val_loss: 2.2592 - val_acc: 0.3690\n",
      "Epoch 7/150\n",
      "33600/33600 [==============================] - 4s 130us/step - loss: 2.2596 - acc: 0.3833 - val_loss: 2.2592 - val_acc: 0.3582\n",
      "Epoch 8/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2596 - acc: 0.3932 - val_loss: 2.2594 - val_acc: 0.3430\n",
      "Epoch 9/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2596 - acc: 0.3682 - val_loss: 2.2593 - val_acc: 0.3631\n",
      "Epoch 10/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2596 - acc: 0.3625 - val_loss: 2.2592 - val_acc: 0.3821\n",
      "Epoch 11/150\n",
      "33600/33600 [==============================] - 4s 128us/step - loss: 2.2596 - acc: 0.3759 - val_loss: 2.2592 - val_acc: 0.3783\n",
      "Epoch 12/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2596 - acc: 0.3615 - val_loss: 2.2590 - val_acc: 0.4115\n",
      "Epoch 13/150\n",
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2596 - acc: 0.3879 - val_loss: 2.2593 - val_acc: 0.3733\n",
      "Epoch 14/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2596 - acc: 0.3836 - val_loss: 2.2593 - val_acc: 0.3614\n",
      "Epoch 15/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 4s 131us/step - loss: 2.2596 - acc: 0.3756 - val_loss: 2.2592 - val_acc: 0.3699\n",
      "Epoch 16/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2596 - acc: 0.3710 - val_loss: 2.2595 - val_acc: 0.3667\n",
      "Epoch 17/150\n",
      "33600/33600 [==============================] - 4s 129us/step - loss: 2.2596 - acc: 0.3777 - val_loss: 2.2594 - val_acc: 0.3763\n",
      "Epoch 18/150\n",
      "33600/33600 [==============================] - 4s 126us/step - loss: 2.2596 - acc: 0.3701 - val_loss: 2.2595 - val_acc: 0.3800\n",
      "Epoch 19/150\n",
      "33600/33600 [==============================] - 4s 132us/step - loss: 2.2596 - acc: 0.3833 - val_loss: 2.2594 - val_acc: 0.3586\n",
      "Epoch 20/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 2.2596 - acc: 0.3526 - val_loss: 2.2590 - val_acc: 0.4089\n",
      "Epoch 21/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.2596 - acc: 0.3771 - val_loss: 2.2592 - val_acc: 0.3799\n",
      "Epoch 22/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.2596 - acc: 0.3615 - val_loss: 2.2592 - val_acc: 0.4007\n",
      "Epoch 23/150\n",
      "33600/33600 [==============================] - 4s 124us/step - loss: 2.2596 - acc: 0.3931 - val_loss: 2.2591 - val_acc: 0.3907\n",
      "Epoch 24/150\n",
      "33600/33600 [==============================] - 4s 114us/step - loss: 2.2596 - acc: 0.3847 - val_loss: 2.2592 - val_acc: 0.3812\n",
      "Epoch 25/150\n",
      "33600/33600 [==============================] - 4s 122us/step - loss: 2.2596 - acc: 0.3987 - val_loss: 2.2594 - val_acc: 0.3529\n",
      "Epoch 26/150\n",
      "33600/33600 [==============================] - 4s 105us/step - loss: 2.2596 - acc: 0.3803 - val_loss: 2.2595 - val_acc: 0.3469\n",
      "Epoch 27/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.2596 - acc: 0.3761 - val_loss: 2.2594 - val_acc: 0.3449\n",
      "Epoch 28/150\n",
      "33600/33600 [==============================] - 2s 60us/step - loss: 2.2596 - acc: 0.3617 - val_loss: 2.2595 - val_acc: 0.3751\n",
      "Epoch 29/150\n",
      "33600/33600 [==============================] - 3s 98us/step - loss: 2.2596 - acc: 0.3627 - val_loss: 2.2593 - val_acc: 0.3900\n",
      "Epoch 30/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2596 - acc: 0.3826 - val_loss: 2.2591 - val_acc: 0.3889\n",
      "Epoch 31/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2596 - acc: 0.3751 - val_loss: 2.2592 - val_acc: 0.3837\n",
      "Epoch 32/150\n",
      "33600/33600 [==============================] - 4s 115us/step - loss: 2.2596 - acc: 0.3717 - val_loss: 2.2592 - val_acc: 0.3892\n",
      "Epoch 33/150\n",
      "33600/33600 [==============================] - 2s 70us/step - loss: 2.2596 - acc: 0.3609 - val_loss: 2.2593 - val_acc: 0.4137\n",
      "Epoch 34/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.2596 - acc: 0.3897 - val_loss: 2.2593 - val_acc: 0.3848\n",
      "Epoch 35/150\n",
      "33600/33600 [==============================] - 2s 60us/step - loss: 2.2596 - acc: 0.3871 - val_loss: 2.2592 - val_acc: 0.3598\n",
      "Epoch 36/150\n",
      "33600/33600 [==============================] - 4s 106us/step - loss: 2.2596 - acc: 0.3725 - val_loss: 2.2592 - val_acc: 0.3649\n",
      "Epoch 37/150\n",
      "33600/33600 [==============================] - 4s 112us/step - loss: 2.2596 - acc: 0.3650 - val_loss: 2.2594 - val_acc: 0.3896\n",
      "Epoch 38/150\n",
      "33600/33600 [==============================] - 4s 113us/step - loss: 2.2596 - acc: 0.3804 - val_loss: 2.2592 - val_acc: 0.3696\n",
      "Epoch 39/150\n",
      "33600/33600 [==============================] - 4s 107us/step - loss: 2.2596 - acc: 0.3782 - val_loss: 2.2592 - val_acc: 0.3633\n",
      "Epoch 40/150\n",
      "33600/33600 [==============================] - 2s 74us/step - loss: 2.2596 - acc: 0.3855 - val_loss: 2.2593 - val_acc: 0.3698\n",
      "Epoch 41/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.2596 - acc: 0.3736 - val_loss: 2.2594 - val_acc: 0.3806\n",
      "Epoch 42/150\n",
      "33600/33600 [==============================] - 2s 63us/step - loss: 2.2596 - acc: 0.3725 - val_loss: 2.2593 - val_acc: 0.3892\n",
      "Epoch 43/150\n",
      "33600/33600 [==============================] - 4s 120us/step - loss: 2.2595 - acc: 0.3947 - val_loss: 2.2593 - val_acc: 0.3505\n",
      "Epoch 44/150\n",
      "33600/33600 [==============================] - 4s 122us/step - loss: 2.2596 - acc: 0.3713 - val_loss: 2.2593 - val_acc: 0.3627\n",
      "Epoch 45/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2596 - acc: 0.3591 - val_loss: 2.2592 - val_acc: 0.4014\n",
      "Epoch 46/150\n",
      "33600/33600 [==============================] - 4s 106us/step - loss: 2.2596 - acc: 0.3999 - val_loss: 2.2592 - val_acc: 0.3682\n",
      "Epoch 47/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.2596 - acc: 0.3756 - val_loss: 2.2592 - val_acc: 0.3579\n",
      "Epoch 48/150\n",
      "33600/33600 [==============================] - 2s 60us/step - loss: 2.2596 - acc: 0.3580 - val_loss: 2.2592 - val_acc: 0.3895\n",
      "Epoch 49/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 2.2596 - acc: 0.3755 - val_loss: 2.2591 - val_acc: 0.4093\n",
      "Epoch 50/150\n",
      "33600/33600 [==============================] - 4s 114us/step - loss: 2.2596 - acc: 0.3890 - val_loss: 2.2591 - val_acc: 0.3851\n",
      "Epoch 51/150\n",
      "33600/33600 [==============================] - 4s 119us/step - loss: 2.2596 - acc: 0.3904 - val_loss: 2.2593 - val_acc: 0.3588\n",
      "Epoch 52/150\n",
      "33600/33600 [==============================] - 4s 117us/step - loss: 2.2596 - acc: 0.3700 - val_loss: 2.2593 - val_acc: 0.3989\n",
      "Epoch 53/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 2.2596 - acc: 0.3909 - val_loss: 2.2592 - val_acc: 0.4036\n",
      "Epoch 54/150\n",
      "33600/33600 [==============================] - 2s 61us/step - loss: 2.2596 - acc: 0.3865 - val_loss: 2.2593 - val_acc: 0.3763\n",
      "Epoch 55/150\n",
      "33600/33600 [==============================] - 2s 59us/step - loss: 2.2596 - acc: 0.3732 - val_loss: 2.2592 - val_acc: 0.3745\n",
      "Epoch 56/150\n",
      "33600/33600 [==============================] - 3s 99us/step - loss: 2.2596 - acc: 0.3687 - val_loss: 2.2592 - val_acc: 0.3764\n",
      "Epoch 57/150\n",
      "33600/33600 [==============================] - 4s 116us/step - loss: 2.2596 - acc: 0.3570 - val_loss: 2.2590 - val_acc: 0.4245\n",
      "Epoch 58/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2596 - acc: 0.4039 - val_loss: 2.2592 - val_acc: 0.3831\n",
      "Epoch 59/150\n",
      "33600/33600 [==============================] - 4s 118us/step - loss: 2.2596 - acc: 0.3686 - val_loss: 2.2591 - val_acc: 0.3837\n",
      "Epoch 60/150\n",
      "33600/33600 [==============================] - 2s 70us/step - loss: 2.2596 - acc: 0.3870 - val_loss: 2.2593 - val_acc: 0.3554\n",
      "Epoch 61/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3540 - val_loss: 2.2593 - val_acc: 0.3962\n",
      "Epoch 62/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 2.2596 - acc: 0.3871 - val_loss: 2.2594 - val_acc: 0.3795\n",
      "Epoch 63/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3752 - val_loss: 2.2592 - val_acc: 0.3755\n",
      "Epoch 64/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3787 - val_loss: 2.2593 - val_acc: 0.3843\n",
      "Epoch 65/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3854 - val_loss: 2.2594 - val_acc: 0.3760\n",
      "Epoch 66/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3655 - val_loss: 2.2592 - val_acc: 0.4049\n",
      "Epoch 67/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3919 - val_loss: 2.2592 - val_acc: 0.3868\n",
      "Epoch 68/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3893 - val_loss: 2.2592 - val_acc: 0.3643\n",
      "Epoch 69/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3850 - val_loss: 2.2591 - val_acc: 0.3467\n",
      "Epoch 70/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3604 - val_loss: 2.2592 - val_acc: 0.3899\n",
      "Epoch 71/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3996 - val_loss: 2.2591 - val_acc: 0.3415\n",
      "Epoch 72/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3585 - val_loss: 2.2591 - val_acc: 0.3787\n",
      "Epoch 73/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3710 - val_loss: 2.2595 - val_acc: 0.3833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3885 - val_loss: 2.2592 - val_acc: 0.3704\n",
      "Epoch 75/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 2.2596 - acc: 0.3776 - val_loss: 2.2592 - val_acc: 0.3743\n",
      "Epoch 76/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 2.2596 - acc: 0.3774 - val_loss: 2.2596 - val_acc: 0.3615\n",
      "Epoch 77/150\n",
      "33600/33600 [==============================] - 2s 53us/step - loss: 2.2596 - acc: 0.3710 - val_loss: 2.2592 - val_acc: 0.3755\n",
      "Epoch 78/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3689 - val_loss: 2.2593 - val_acc: 0.4001\n",
      "Epoch 79/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3962 - val_loss: 2.2594 - val_acc: 0.3532\n",
      "Epoch 80/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3468 - val_loss: 2.2592 - val_acc: 0.4156\n",
      "Epoch 81/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3873 - val_loss: 2.2592 - val_acc: 0.3799\n",
      "Epoch 82/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3844 - val_loss: 2.2592 - val_acc: 0.3796\n",
      "Epoch 83/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3740 - val_loss: 2.2593 - val_acc: 0.3799\n",
      "Epoch 84/150\n",
      "33600/33600 [==============================] - 2s 53us/step - loss: 2.2596 - acc: 0.3709 - val_loss: 2.2594 - val_acc: 0.4027\n",
      "Epoch 85/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3866 - val_loss: 2.2594 - val_acc: 0.3727\n",
      "Epoch 86/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3829 - val_loss: 2.2595 - val_acc: 0.3505\n",
      "Epoch 87/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3628 - val_loss: 2.2592 - val_acc: 0.3932\n",
      "Epoch 88/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3826 - val_loss: 2.2592 - val_acc: 0.3993\n",
      "Epoch 89/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.4059 - val_loss: 2.2593 - val_acc: 0.3681\n",
      "Epoch 90/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3607 - val_loss: 2.2591 - val_acc: 0.3970\n",
      "Epoch 91/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3642 - val_loss: 2.2594 - val_acc: 0.4038\n",
      "Epoch 92/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3944 - val_loss: 2.2592 - val_acc: 0.3839\n",
      "Epoch 93/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3854 - val_loss: 2.2592 - val_acc: 0.3901\n",
      "Epoch 94/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3896 - val_loss: 2.2593 - val_acc: 0.3633\n",
      "Epoch 95/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3859 - val_loss: 2.2591 - val_acc: 0.3479\n",
      "Epoch 96/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3671 - val_loss: 2.2593 - val_acc: 0.3862\n",
      "Epoch 97/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3768 - val_loss: 2.2593 - val_acc: 0.3950\n",
      "Epoch 98/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.4037 - val_loss: 2.2593 - val_acc: 0.3524\n",
      "Epoch 99/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3715 - val_loss: 2.2591 - val_acc: 0.3765\n",
      "Epoch 100/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3731 - val_loss: 2.2593 - val_acc: 0.3825\n",
      "Epoch 101/150\n",
      "33600/33600 [==============================] - 2s 53us/step - loss: 2.2596 - acc: 0.3806 - val_loss: 2.2592 - val_acc: 0.3787\n",
      "Epoch 102/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3775 - val_loss: 2.2594 - val_acc: 0.3954\n",
      "Epoch 103/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3701 - val_loss: 2.2592 - val_acc: 0.4244\n",
      "Epoch 104/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.4059 - val_loss: 2.2594 - val_acc: 0.3905\n",
      "Epoch 105/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3801 - val_loss: 2.2592 - val_acc: 0.3848\n",
      "Epoch 106/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3784 - val_loss: 2.2593 - val_acc: 0.3907\n",
      "Epoch 107/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.4001 - val_loss: 2.2593 - val_acc: 0.3714\n",
      "Epoch 108/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3743 - val_loss: 2.2592 - val_acc: 0.3942\n",
      "Epoch 109/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3945 - val_loss: 2.2593 - val_acc: 0.3568\n",
      "Epoch 110/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3630 - val_loss: 2.2593 - val_acc: 0.3786\n",
      "Epoch 111/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3903 - val_loss: 2.2592 - val_acc: 0.3654\n",
      "Epoch 112/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3860 - val_loss: 2.2592 - val_acc: 0.3675\n",
      "Epoch 113/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3512 - val_loss: 2.2594 - val_acc: 0.4144\n",
      "Epoch 114/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3822 - val_loss: 2.2592 - val_acc: 0.3927\n",
      "Epoch 115/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3721 - val_loss: 2.2593 - val_acc: 0.4039\n",
      "Epoch 116/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3864 - val_loss: 2.2594 - val_acc: 0.3805\n",
      "Epoch 117/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3960 - val_loss: 2.2592 - val_acc: 0.3592\n",
      "Epoch 118/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3609 - val_loss: 2.2593 - val_acc: 0.3868\n",
      "Epoch 119/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3878 - val_loss: 2.2594 - val_acc: 0.3642\n",
      "Epoch 120/150\n",
      "33600/33600 [==============================] - 2s 53us/step - loss: 2.2596 - acc: 0.3947 - val_loss: 2.2592 - val_acc: 0.3424\n",
      "Epoch 121/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3722 - val_loss: 2.2593 - val_acc: 0.3723\n",
      "Epoch 122/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3768 - val_loss: 2.2592 - val_acc: 0.3856\n",
      "Epoch 123/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3899 - val_loss: 2.2594 - val_acc: 0.3702\n",
      "Epoch 124/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3727 - val_loss: 2.2593 - val_acc: 0.3730\n",
      "Epoch 125/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3836 - val_loss: 2.2593 - val_acc: 0.3698\n",
      "Epoch 126/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3642 - val_loss: 2.2593 - val_acc: 0.3933\n",
      "Epoch 127/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3806 - val_loss: 2.2593 - val_acc: 0.3968\n",
      "Epoch 128/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.4006 - val_loss: 2.2592 - val_acc: 0.3923\n",
      "Epoch 129/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3798 - val_loss: 2.2591 - val_acc: 0.3952\n",
      "Epoch 130/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3935 - val_loss: 2.2592 - val_acc: 0.3812\n",
      "Epoch 131/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3861 - val_loss: 2.2592 - val_acc: 0.3687\n",
      "Epoch 132/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3762 - val_loss: 2.2590 - val_acc: 0.3930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3833 - val_loss: 2.2592 - val_acc: 0.3735\n",
      "Epoch 134/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3644 - val_loss: 2.2591 - val_acc: 0.4177\n",
      "Epoch 135/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3871 - val_loss: 2.2594 - val_acc: 0.4052\n",
      "Epoch 136/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3934 - val_loss: 2.2593 - val_acc: 0.3892\n",
      "Epoch 137/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3898 - val_loss: 2.2594 - val_acc: 0.3842\n",
      "Epoch 138/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3822 - val_loss: 2.2594 - val_acc: 0.3868\n",
      "Epoch 139/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3859 - val_loss: 2.2594 - val_acc: 0.3813\n",
      "Epoch 140/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3750 - val_loss: 2.2593 - val_acc: 0.3929\n",
      "Epoch 141/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3969 - val_loss: 2.2592 - val_acc: 0.3450\n",
      "Epoch 142/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3604 - val_loss: 2.2592 - val_acc: 0.4071\n",
      "Epoch 143/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3899 - val_loss: 2.2593 - val_acc: 0.3939\n",
      "Epoch 144/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3953 - val_loss: 2.2592 - val_acc: 0.3599\n",
      "Epoch 145/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3527 - val_loss: 2.2591 - val_acc: 0.3964\n",
      "Epoch 146/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3939 - val_loss: 2.2593 - val_acc: 0.3781\n",
      "Epoch 147/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3866 - val_loss: 2.2594 - val_acc: 0.3855\n",
      "Epoch 148/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3991 - val_loss: 2.2592 - val_acc: 0.3417\n",
      "Epoch 149/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3605 - val_loss: 2.2589 - val_acc: 0.3873\n",
      "Epoch 150/150\n",
      "33600/33600 [==============================] - 2s 54us/step - loss: 2.2596 - acc: 0.3901 - val_loss: 2.2592 - val_acc: 0.3636\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/150\n",
      "33600/33600 [==============================] - 2s 56us/step - loss: 2.2628 - acc: 0.4897 - val_loss: 2.2243 - val_acc: 0.5806\n",
      "Epoch 2/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 2.1884 - acc: 0.6024 - val_loss: 2.1511 - val_acc: 0.6385\n",
      "Epoch 3/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 2.1165 - acc: 0.6616 - val_loss: 2.0803 - val_acc: 0.6770\n",
      "Epoch 4/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 2.0468 - acc: 0.6879 - val_loss: 2.0118 - val_acc: 0.7044\n",
      "Epoch 5/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.9796 - acc: 0.7081 - val_loss: 1.9457 - val_acc: 0.7175\n",
      "Epoch 6/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.9148 - acc: 0.7224 - val_loss: 1.8818 - val_acc: 0.7318\n",
      "Epoch 7/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.8523 - acc: 0.7335 - val_loss: 1.8209 - val_acc: 0.7436\n",
      "Epoch 8/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.7920 - acc: 0.7447 - val_loss: 1.7615 - val_acc: 0.7524\n",
      "Epoch 9/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.7342 - acc: 0.7538 - val_loss: 1.7048 - val_acc: 0.7590\n",
      "Epoch 10/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.6787 - acc: 0.7600 - val_loss: 1.6504 - val_acc: 0.7646\n",
      "Epoch 11/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.6256 - acc: 0.7635 - val_loss: 1.5984 - val_acc: 0.7721\n",
      "Epoch 12/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.5746 - acc: 0.7730 - val_loss: 1.5486 - val_acc: 0.7749\n",
      "Epoch 13/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.5259 - acc: 0.7775 - val_loss: 1.5008 - val_acc: 0.7788\n",
      "Epoch 14/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.4794 - acc: 0.7820 - val_loss: 1.4553 - val_acc: 0.7833\n",
      "Epoch 15/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 1.4349 - acc: 0.7847 - val_loss: 1.4121 - val_acc: 0.7882\n",
      "Epoch 16/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 1.3926 - acc: 0.7897 - val_loss: 1.3708 - val_acc: 0.7915\n",
      "Epoch 17/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.3524 - acc: 0.7929 - val_loss: 1.3316 - val_acc: 0.7964\n",
      "Epoch 18/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.3140 - acc: 0.7961 - val_loss: 1.2942 - val_acc: 0.7994\n",
      "Epoch 19/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 1.2776 - acc: 0.7973 - val_loss: 1.2587 - val_acc: 0.8029\n",
      "Epoch 20/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.2428 - acc: 0.8019 - val_loss: 1.2246 - val_acc: 0.8046\n",
      "Epoch 21/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.2097 - acc: 0.8043 - val_loss: 1.1924 - val_acc: 0.8081\n",
      "Epoch 22/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.1783 - acc: 0.8076 - val_loss: 1.1618 - val_acc: 0.8107\n",
      "Epoch 23/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.1483 - acc: 0.8110 - val_loss: 1.1327 - val_acc: 0.8118\n",
      "Epoch 24/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.1199 - acc: 0.8127 - val_loss: 1.1050 - val_acc: 0.8146\n",
      "Epoch 25/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.0928 - acc: 0.8152 - val_loss: 1.0788 - val_acc: 0.8188\n",
      "Epoch 26/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 1.0672 - acc: 0.8178 - val_loss: 1.0538 - val_acc: 0.8213\n",
      "Epoch 27/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.0426 - acc: 0.8202 - val_loss: 1.0300 - val_acc: 0.8233\n",
      "Epoch 28/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 1.0194 - acc: 0.8218 - val_loss: 1.0072 - val_acc: 0.8256\n",
      "Epoch 29/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.9971 - acc: 0.8232 - val_loss: 0.9857 - val_acc: 0.8282\n",
      "Epoch 30/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.9759 - acc: 0.8255 - val_loss: 0.9649 - val_acc: 0.8289\n",
      "Epoch 31/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.9556 - acc: 0.8270 - val_loss: 0.9452 - val_acc: 0.8313\n",
      "Epoch 32/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.9363 - acc: 0.8290 - val_loss: 0.9265 - val_acc: 0.8314\n",
      "Epoch 33/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.9179 - acc: 0.8306 - val_loss: 0.9085 - val_acc: 0.8332\n",
      "Epoch 34/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.9004 - acc: 0.8328 - val_loss: 0.8916 - val_acc: 0.8351\n",
      "Epoch 35/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.8837 - acc: 0.8345 - val_loss: 0.8753 - val_acc: 0.8352\n",
      "Epoch 36/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.8677 - acc: 0.8367 - val_loss: 0.8597 - val_acc: 0.8373\n",
      "Epoch 37/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.8523 - acc: 0.8380 - val_loss: 0.8447 - val_acc: 0.8386\n",
      "Epoch 38/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.8377 - acc: 0.8392 - val_loss: 0.8306 - val_acc: 0.8407\n",
      "Epoch 39/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.8238 - acc: 0.8404 - val_loss: 0.8172 - val_acc: 0.8411\n",
      "Epoch 40/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.8104 - acc: 0.8423 - val_loss: 0.8040 - val_acc: 0.8430\n",
      "Epoch 41/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.7976 - acc: 0.8432 - val_loss: 0.7917 - val_acc: 0.8442\n",
      "Epoch 42/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.7853 - acc: 0.8453 - val_loss: 0.7797 - val_acc: 0.8450\n",
      "Epoch 43/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.7735 - acc: 0.8462 - val_loss: 0.7681 - val_acc: 0.8460\n",
      "Epoch 44/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.7621 - acc: 0.8471 - val_loss: 0.7570 - val_acc: 0.8473\n",
      "Epoch 45/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.7512 - acc: 0.8482 - val_loss: 0.7465 - val_acc: 0.8499\n",
      "Epoch 46/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.7408 - acc: 0.8493 - val_loss: 0.7363 - val_acc: 0.8510\n",
      "Epoch 47/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.7307 - acc: 0.8511 - val_loss: 0.7266 - val_acc: 0.8521\n",
      "Epoch 48/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.7210 - acc: 0.8519 - val_loss: 0.7172 - val_acc: 0.8525\n",
      "Epoch 49/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.7116 - acc: 0.8536 - val_loss: 0.7081 - val_acc: 0.8548\n",
      "Epoch 50/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.7027 - acc: 0.8541 - val_loss: 0.6994 - val_acc: 0.8562\n",
      "Epoch 51/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.6941 - acc: 0.8550 - val_loss: 0.6909 - val_acc: 0.8558\n",
      "Epoch 52/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.6857 - acc: 0.8558 - val_loss: 0.6828 - val_acc: 0.8568\n",
      "Epoch 53/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.6777 - acc: 0.8565 - val_loss: 0.6752 - val_acc: 0.8586\n",
      "Epoch 54/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.6700 - acc: 0.8585 - val_loss: 0.6676 - val_acc: 0.8592\n",
      "Epoch 55/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.6625 - acc: 0.8590 - val_loss: 0.6603 - val_acc: 0.8596\n",
      "Epoch 56/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.6552 - acc: 0.8599 - val_loss: 0.6535 - val_acc: 0.8605\n",
      "Epoch 57/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.6482 - acc: 0.8617 - val_loss: 0.6464 - val_acc: 0.8615\n",
      "Epoch 58/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.6415 - acc: 0.8626 - val_loss: 0.6398 - val_acc: 0.8623\n",
      "Epoch 59/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.6349 - acc: 0.8631 - val_loss: 0.6335 - val_acc: 0.8631\n",
      "Epoch 60/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.6286 - acc: 0.8646 - val_loss: 0.6274 - val_acc: 0.8636\n",
      "Epoch 61/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.6225 - acc: 0.8650 - val_loss: 0.6215 - val_acc: 0.8646\n",
      "Epoch 62/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.6165 - acc: 0.8651 - val_loss: 0.6157 - val_acc: 0.8651\n",
      "Epoch 63/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.6108 - acc: 0.8659 - val_loss: 0.6102 - val_acc: 0.8650\n",
      "Epoch 64/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.6052 - acc: 0.8661 - val_loss: 0.6047 - val_acc: 0.8660\n",
      "Epoch 65/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5998 - acc: 0.8671 - val_loss: 0.5994 - val_acc: 0.8664\n",
      "Epoch 66/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5945 - acc: 0.8678 - val_loss: 0.5942 - val_acc: 0.8669\n",
      "Epoch 67/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5895 - acc: 0.8682 - val_loss: 0.5894 - val_acc: 0.8673\n",
      "Epoch 68/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5846 - acc: 0.8689 - val_loss: 0.5846 - val_acc: 0.8671\n",
      "Epoch 69/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5798 - acc: 0.8691 - val_loss: 0.5801 - val_acc: 0.8675\n",
      "Epoch 70/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5751 - acc: 0.8699 - val_loss: 0.5757 - val_acc: 0.8683\n",
      "Epoch 71/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5706 - acc: 0.8705 - val_loss: 0.5710 - val_acc: 0.8695\n",
      "Epoch 72/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.5662 - acc: 0.8706 - val_loss: 0.5668 - val_acc: 0.8689\n",
      "Epoch 73/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5619 - acc: 0.8715 - val_loss: 0.5626 - val_acc: 0.8696\n",
      "Epoch 74/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5577 - acc: 0.8717 - val_loss: 0.5586 - val_acc: 0.8702\n",
      "Epoch 75/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5537 - acc: 0.8724 - val_loss: 0.5547 - val_acc: 0.8705\n",
      "Epoch 76/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.5498 - acc: 0.8726 - val_loss: 0.5508 - val_acc: 0.8708\n",
      "Epoch 77/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5459 - acc: 0.8730 - val_loss: 0.5471 - val_acc: 0.8706\n",
      "Epoch 78/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5422 - acc: 0.8734 - val_loss: 0.5435 - val_acc: 0.8708\n",
      "Epoch 79/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5385 - acc: 0.8745 - val_loss: 0.5398 - val_acc: 0.8715\n",
      "Epoch 80/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5350 - acc: 0.8746 - val_loss: 0.5366 - val_acc: 0.8718\n",
      "Epoch 81/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5315 - acc: 0.8750 - val_loss: 0.5331 - val_acc: 0.8726\n",
      "Epoch 82/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5281 - acc: 0.8760 - val_loss: 0.5299 - val_acc: 0.8725\n",
      "Epoch 83/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5248 - acc: 0.8762 - val_loss: 0.5267 - val_acc: 0.8731\n",
      "Epoch 84/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.5216 - acc: 0.8771 - val_loss: 0.5236 - val_acc: 0.8735\n",
      "Epoch 85/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.5184 - acc: 0.8775 - val_loss: 0.5205 - val_acc: 0.8738\n",
      "Epoch 86/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.5154 - acc: 0.8777 - val_loss: 0.5175 - val_acc: 0.8745\n",
      "Epoch 87/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5124 - acc: 0.8786 - val_loss: 0.5146 - val_acc: 0.8751\n",
      "Epoch 88/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5095 - acc: 0.8786 - val_loss: 0.5118 - val_acc: 0.8756\n",
      "Epoch 89/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5066 - acc: 0.8799 - val_loss: 0.5091 - val_acc: 0.8760\n",
      "Epoch 90/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.5038 - acc: 0.8795 - val_loss: 0.5064 - val_acc: 0.8765\n",
      "Epoch 91/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.5010 - acc: 0.8804 - val_loss: 0.5037 - val_acc: 0.8767\n",
      "Epoch 92/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4983 - acc: 0.8806 - val_loss: 0.5010 - val_acc: 0.8770\n",
      "Epoch 93/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4958 - acc: 0.8814 - val_loss: 0.4985 - val_acc: 0.8777\n",
      "Epoch 94/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4932 - acc: 0.8815 - val_loss: 0.4960 - val_acc: 0.8776\n",
      "Epoch 95/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4906 - acc: 0.8819 - val_loss: 0.4937 - val_acc: 0.8790\n",
      "Epoch 96/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4882 - acc: 0.8825 - val_loss: 0.4912 - val_acc: 0.8785\n",
      "Epoch 97/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4858 - acc: 0.8825 - val_loss: 0.4890 - val_acc: 0.8786\n",
      "Epoch 98/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4835 - acc: 0.8829 - val_loss: 0.4869 - val_acc: 0.8789\n",
      "Epoch 99/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4812 - acc: 0.8832 - val_loss: 0.4845 - val_acc: 0.8794\n",
      "Epoch 100/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4789 - acc: 0.8834 - val_loss: 0.4824 - val_acc: 0.8802\n",
      "Epoch 101/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4767 - acc: 0.8842 - val_loss: 0.4803 - val_acc: 0.8799\n",
      "Epoch 102/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4745 - acc: 0.8841 - val_loss: 0.4781 - val_acc: 0.8807\n",
      "Epoch 103/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4724 - acc: 0.8847 - val_loss: 0.4760 - val_acc: 0.8817\n",
      "Epoch 104/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4703 - acc: 0.8847 - val_loss: 0.4742 - val_acc: 0.8817\n",
      "Epoch 105/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4683 - acc: 0.8857 - val_loss: 0.4722 - val_acc: 0.8812\n",
      "Epoch 106/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4663 - acc: 0.8854 - val_loss: 0.4702 - val_acc: 0.8821\n",
      "Epoch 107/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4644 - acc: 0.8860 - val_loss: 0.4684 - val_acc: 0.8824\n",
      "Epoch 108/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4624 - acc: 0.8861 - val_loss: 0.4665 - val_acc: 0.8826\n",
      "Epoch 109/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4606 - acc: 0.8863 - val_loss: 0.4647 - val_acc: 0.8831\n",
      "Epoch 110/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4587 - acc: 0.8862 - val_loss: 0.4630 - val_acc: 0.8833\n",
      "Epoch 111/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4569 - acc: 0.8866 - val_loss: 0.4613 - val_acc: 0.8833\n",
      "Epoch 112/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4551 - acc: 0.8868 - val_loss: 0.4595 - val_acc: 0.8836\n",
      "Epoch 113/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4533 - acc: 0.8871 - val_loss: 0.4578 - val_acc: 0.8837\n",
      "Epoch 114/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4516 - acc: 0.8875 - val_loss: 0.4561 - val_acc: 0.8838\n",
      "Epoch 115/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4499 - acc: 0.8872 - val_loss: 0.4545 - val_acc: 0.8840\n",
      "Epoch 116/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4482 - acc: 0.8875 - val_loss: 0.4529 - val_acc: 0.8848\n",
      "Epoch 117/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4466 - acc: 0.8878 - val_loss: 0.4514 - val_acc: 0.8850\n",
      "Epoch 118/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4450 - acc: 0.8881 - val_loss: 0.4497 - val_acc: 0.8849\n",
      "Epoch 119/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4434 - acc: 0.8884 - val_loss: 0.4483 - val_acc: 0.8860\n",
      "Epoch 120/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4419 - acc: 0.8886 - val_loss: 0.4468 - val_acc: 0.8863\n",
      "Epoch 121/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4404 - acc: 0.8890 - val_loss: 0.4453 - val_acc: 0.8861\n",
      "Epoch 122/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4389 - acc: 0.8892 - val_loss: 0.4439 - val_acc: 0.8860\n",
      "Epoch 123/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4374 - acc: 0.8894 - val_loss: 0.4424 - val_acc: 0.8867\n",
      "Epoch 124/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4359 - acc: 0.8893 - val_loss: 0.4412 - val_acc: 0.8864\n",
      "Epoch 125/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4345 - acc: 0.8898 - val_loss: 0.4397 - val_acc: 0.8870\n",
      "Epoch 126/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4331 - acc: 0.8901 - val_loss: 0.4384 - val_acc: 0.8869\n",
      "Epoch 127/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4317 - acc: 0.8903 - val_loss: 0.4371 - val_acc: 0.8868\n",
      "Epoch 128/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4303 - acc: 0.8904 - val_loss: 0.4358 - val_acc: 0.8868\n",
      "Epoch 129/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4290 - acc: 0.8907 - val_loss: 0.4345 - val_acc: 0.8874\n",
      "Epoch 130/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4277 - acc: 0.8912 - val_loss: 0.4332 - val_acc: 0.8877\n",
      "Epoch 131/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4264 - acc: 0.8912 - val_loss: 0.4319 - val_acc: 0.8875\n",
      "Epoch 132/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4251 - acc: 0.8915 - val_loss: 0.4308 - val_acc: 0.8875\n",
      "Epoch 133/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4238 - acc: 0.8918 - val_loss: 0.4298 - val_acc: 0.8883\n",
      "Epoch 134/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4226 - acc: 0.8920 - val_loss: 0.4283 - val_acc: 0.8879\n",
      "Epoch 135/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4214 - acc: 0.8920 - val_loss: 0.4273 - val_acc: 0.8885\n",
      "Epoch 136/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4202 - acc: 0.8924 - val_loss: 0.4261 - val_acc: 0.8889\n",
      "Epoch 137/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4190 - acc: 0.8924 - val_loss: 0.4250 - val_acc: 0.8892\n",
      "Epoch 138/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4178 - acc: 0.8926 - val_loss: 0.4238 - val_acc: 0.8896\n",
      "Epoch 139/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4166 - acc: 0.8930 - val_loss: 0.4229 - val_acc: 0.8899\n",
      "Epoch 140/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4155 - acc: 0.8931 - val_loss: 0.4218 - val_acc: 0.8898\n",
      "Epoch 141/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4144 - acc: 0.8934 - val_loss: 0.4207 - val_acc: 0.8901\n",
      "Epoch 142/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4133 - acc: 0.8932 - val_loss: 0.4196 - val_acc: 0.8904\n",
      "Epoch 143/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4122 - acc: 0.8935 - val_loss: 0.4185 - val_acc: 0.8904\n",
      "Epoch 144/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4112 - acc: 0.8938 - val_loss: 0.4175 - val_acc: 0.8904\n",
      "Epoch 145/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4101 - acc: 0.8941 - val_loss: 0.4166 - val_acc: 0.8904\n",
      "Epoch 146/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4090 - acc: 0.8942 - val_loss: 0.4156 - val_acc: 0.8910\n",
      "Epoch 147/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4080 - acc: 0.8941 - val_loss: 0.4145 - val_acc: 0.8911\n",
      "Epoch 148/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4070 - acc: 0.8942 - val_loss: 0.4136 - val_acc: 0.8915\n",
      "Epoch 149/150\n",
      "33600/33600 [==============================] - 2s 52us/step - loss: 0.4060 - acc: 0.8947 - val_loss: 0.4126 - val_acc: 0.8917\n",
      "Epoch 150/150\n",
      "33600/33600 [==============================] - 2s 51us/step - loss: 0.4050 - acc: 0.8945 - val_loss: 0.4116 - val_acc: 0.8914\n"
     ]
    }
   ],
   "source": [
    "regs = [0.1,0.01,0.001,0.0001,0]\n",
    "models = []\n",
    "optimizer = 'rmsprop'\n",
    "histories = []\n",
    "for reg in regs:\n",
    "    model,history = linear_model(optimizer,150, train_x,train_y,regularization=reg,valid_x=valid_x,valid_y=valid_y)\n",
    "    models.append(model)\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularizaiton training acc validation acc\n",
      "0.1 0.112351190476 0.108214285714\n",
      "0.01 0.112351190476 0.108214285714\n",
      "0.001 0.112351190476 0.108214285714\n",
      "0.0001 0.39005952381 0.363571428571\n",
      "0 0.894464285714 0.891428571429\n"
     ]
    }
   ],
   "source": [
    "print('regularizaiton','training acc','validation acc')\n",
    "for idx, histor in enumerate(histories):\n",
    "    tr_acc    = histor.history['acc'][-1]\n",
    "    valid_acc = histor.history['val_acc'][-1]\n",
    "    print(regs[idx],tr_acc,valid_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the best model is the one with 0.0001 reg\n",
    "model = models[-1]\n",
    "generate_submission(model,test_data,'sub_rmsprop_reg0_150ep_94.4tr_91.9valid.csv',['ImageId','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Regularization  | Training acc  | Validation acc  |\n",
    "| -------------   |:-------------:| ---------------:|\n",
    "| 0.1             | 83.9          |    83.24        |\n",
    "| 0.01            | 8899          |    8852         |\n",
    "| 0.001           | 9159          |    9114         |\n",
    "\n",
    "<img src=\"1-linearmodel-latest.PNG\" height=\"300\" width = \"800\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
