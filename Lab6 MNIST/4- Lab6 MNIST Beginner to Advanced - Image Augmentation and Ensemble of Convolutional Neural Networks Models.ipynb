{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an ensemble of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "notebook_num = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_appending(history1,history2):\n",
    "    history1.history['acc'] += history2.history['acc']\n",
    "    history1.history['val_acc'] += history2.history['val_acc']\n",
    "    history1.history['val_loss'] += history2.history['val_loss']\n",
    "    history1.history['loss'] += history2.history['loss']\n",
    "    return history1\n",
    "\n",
    "def history_from_list(history_list):\n",
    "    for i in range(len(history_list)-1):\n",
    "        history_list[0].history['acc'] += history_list[i+1].history['acc']\n",
    "        history_list[0].history['val_acc'] += history_list[i+1].history['val_acc']\n",
    "        history_list[0].history['val_loss'] += history_list[i+1].history['val_loss']\n",
    "        history_list[0].history['loss'] += history_list[i+1].history['loss']\n",
    "    return history_list[0]\n",
    "    \n",
    "def to_kaggle_csv(matrix, header,filename):\n",
    "    frame = pd.DataFrame(data = matrix,columns=header)\n",
    "    frame.to_csv(path_or_buf  = filename,index = False,sep =',')\n",
    "    return frame\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "def load_train_valid_test_data(train_file,test_file,validation_percentage):\n",
    "    train_data = np.genfromtxt(delimiter=',',fname=train_file,skip_header=True)\n",
    "    test_data = np.genfromtxt(delimiter=',',fname=test_file,skip_header=True)\n",
    "    train_y = train_data[:,0]\n",
    "    train_x = train_data[:,1:]\n",
    "    train_x = train_x.reshape((train_data.shape[0],28,28,1))\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(train_y.reshape((train_y.shape[0],1)))\n",
    "    train_y = enc.transform(train_y.reshape((train_y.shape[0],1))).toarray()\n",
    "    train_x /= 255\n",
    "    test_x = test_data.reshape((test_data.shape[0],28,28,1))\n",
    "    test_x  = test_x/255\n",
    "    train_x = train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return train_x, valid_x, test_x, train_y, valid_y\n",
    "\n",
    "def generate_submission(model,data,filename,file_columns = None):\n",
    "    predictions = model.predict(data)\n",
    "    result = predictions.argmax(axis =1 )\n",
    "    imgs_count = data.shape[0]\n",
    "    result = result.reshape((imgs_count,1))\n",
    "    ids = (np.arange(imgs_count)+1).reshape((imgs_count,1))\n",
    "    result = np.hstack((ids,result))\n",
    "    m = to_kaggle_csv(result,file_columns, filename)\n",
    "\n",
    "def ensemble_submission(models,test_data,filename,file_columns = None):\n",
    "    predictions = model[0].predict(data)\n",
    "    for idx in range(len(models - 1)):\n",
    "        predictions = predictions + model[idx+1].predict(data)\n",
    "    result = predictions.argmax(axis =1)\n",
    "    imgs_count = data.shape[0]\n",
    "    result = result.reshape((imgs_count,1))\n",
    "    ids = (np.arange(imgs_count)+1).reshape((imgs_count,1))\n",
    "    result = np.hstack((ids,result))\n",
    "    m = to_kaggle_csv(result,file_columns, filename)\n",
    "    \n",
    "def validate_preprocessing(train_x, valid_x, test_x, train_y, valid_y):\n",
    "    print('max train',train_x.max())\n",
    "    print('max valid',train_x.max())\n",
    "    print('max test',train_x.max())\n",
    "    \n",
    "    print('train_x shape',train_x.shape)\n",
    "    print('valid_x shape',valid_x.shape)\n",
    "    print('test_x shape',test_x.shape)\n",
    "    print('train_y shape',train_y.shape)\n",
    "    print('valid_y shape',valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, test_x, train_y, valid_y = load_train_valid_test_data('train.csv','test.csv',0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max train 1.0\n",
      "max valid 1.0\n",
      "max test 1.0\n",
      "train_x shape (33600, 28, 28, 1)\n",
      "valid_x shape (8400, 28, 28, 1)\n",
      "test_x shape (28000, 28, 28, 1)\n",
      "train_y shape (33600, 10)\n",
      "valid_y shape (8400, 10)\n"
     ]
    }
   ],
   "source": [
    "validate_preprocessing(train_x, valid_x, test_x, train_y, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training a convolutional neural network with data augmentation\n",
    "multilayer networks have the capacity to learn features of the input and build upon it in subsequent layers, so it can overfit to the training data, we'll need to make sure it doesn't, that's why we'll be monitoring the training process through both validation and training accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential,Model\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.layers.core import Dense, Activation# defining the layers\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten\n",
    "from keras.optimizers import RMSprop, SGD , Adam\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.models import clone_model\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_conv(model,filters):\n",
    "    model.add(Conv2D(filters,(3,3),padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    return model\n",
    "    \n",
    "\n",
    "def CNN_architecture(Batch_Norm,dropout):\n",
    "    cnn_model = Sequential()    \n",
    "    cnn_model.add(Conv2D(32,(3,3),padding='same',input_shape=(28,28,1)))\n",
    "    cnn_model.add(Activation('relu'))\n",
    "    cnn_model = add_conv(cnn_model,32)\n",
    "    cnn_model.add(MaxPooling2D(pool_size =(2,2),strides = (2,2)))\n",
    "    cnn_model.add(Dropout(0.2))\n",
    "    cnn_model = add_conv(cnn_model,64)\n",
    "    cnn_model = add_conv(cnn_model,64)\n",
    "    cnn_model.add(MaxPooling2D(pool_size = (2,2),strides = (2,2)))\n",
    "    cnn_model.add(Dropout(0.2))\n",
    "    cnn_model.add(Flatten())\n",
    "    cnn_model.add(Dense(64))\n",
    "    cnn_model.add(Activation('relu'))\n",
    "    if Batch_Norm:\n",
    "        cnn_model.add(BatchNormalization())\n",
    "    cnn_model.add(Dropout(drop_out))\n",
    "    cnn_model.add(Dense(32))\n",
    "    cnn_model.add(Activation('relu'))\n",
    "    if Batch_Norm:\n",
    "        cnn_model.add(BatchNormalization())\n",
    "    cnn_model.add(Dropout(drop_out))\n",
    "    cnn_model.add(Dense(10))\n",
    "    cnn_model.add(Activation('softmax'))\n",
    "    return cnn_model\n",
    "\n",
    "def CNN_model_augmentation(optimizer,epochs,train_x,train_y,valid_x = None,valid_y = None, Batch_Norm = True,drop_out = 0.5):\n",
    "    cnn_model = CNN_architecture(Batch_Norm,drop_out)\n",
    "    datagen = ImageDataGenerator( rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.1,\n",
    "        zoom_range=0.2, fill_mode='nearest')\n",
    "    batch_size = 64\n",
    "    train_generator = datagen.flow(train_x,train_y,batch_size = 64)\n",
    "    steps_per_epoch = int(train_x.shape[0]/batch_size)\n",
    "    optimizer = Adam(lr= 0.001)\n",
    "    cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,#RMSprop()\n",
    "                  metrics=['accuracy'])\n",
    "    history = cnn_model.fit_generator(train_generator,epochs=int(epochs*2/3),\n",
    "                        verbose=1,validation_data=(valid_x, valid_y),steps_per_epoch= steps_per_epoch)\n",
    "    optimizer = Adam(lr= 0.0001)\n",
    "    cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,#RMSprop()\n",
    "                  metrics=['accuracy'])\n",
    "    history = cnn_model.fit_generator(train_generator,epochs=int(epochs*2/3),\n",
    "                        verbose=1,validation_data=(valid_x, valid_y),steps_per_epoch= steps_per_epoch)\n",
    "    optimizer = Adam(lr= 0.00001)\n",
    "    cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,#RMSprop()\n",
    "                  metrics=['accuracy'])\n",
    "    history = cnn_model.fit_generator(train_generator,epochs=int(epochs*2/3),\n",
    "                        verbose=1,validation_data=(valid_x, valid_y),steps_per_epoch= steps_per_epoch)\n",
    "    \n",
    "    return cnn_model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "525/525 [==============================] - 19s 37ms/step - loss: 0.4820 - acc: 0.8567 - val_loss: 0.0776 - val_acc: 0.9771\n",
      "Epoch 1/1\n",
      "525/525 [==============================] - 19s 36ms/step - loss: 0.1458 - acc: 0.9566 - val_loss: 0.0404 - val_acc: 0.9883\n",
      "Epoch 1/1\n",
      "525/525 [==============================] - 20s 37ms/step - loss: 0.1238 - acc: 0.9634 - val_loss: 0.0365 - val_acc: 0.9905\n",
      "Epoch 1/1\n",
      "525/525 [==============================] - 20s 38ms/step - loss: 1.0550 - acc: 0.6669 - val_loss: 0.1077 - val_acc: 0.9718\n",
      "Epoch 1/1\n",
      "525/525 [==============================] - 20s 38ms/step - loss: 0.3742 - acc: 0.8955 - val_loss: 0.0711 - val_acc: 0.9808\n",
      "Epoch 1/1\n",
      "525/525 [==============================] - 20s 39ms/step - loss: 0.3334 - acc: 0.9091 - val_loss: 0.0654 - val_acc: 0.9824\n"
     ]
    }
   ],
   "source": [
    "drop_outs = [0,0.35]\n",
    "models = []\n",
    "histories = []\n",
    "optimizer = 'adam'\n",
    "n_epochs = 2\n",
    "for drop_out in drop_outs:\n",
    "    history,model = CNN_model_augmentation(optimizer,n_epochs, train_x, train_y, valid_x, valid_y,drop_out = drop_out)\n",
    "    #print(\"Batch Normalization: \", batch)\n",
    "    #print(\"Technique \", technique_dict[tech])\n",
    "    #print(\"drop_out \", drop_out)\n",
    "    models.append(model)\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an ensemble of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model_3(optimizer,epochs,train_x,train_y,valid_x = None,valid_y = None, Batch_Norm = False,drop_out = 0.5):\n",
    "    cnn_model = CNN_architecture(Batch_Norm,drop_out)\n",
    "    history_list = []\n",
    "    for i in range(3):\n",
    "        optimizer = Adam(lr= 0.001)\n",
    "        cnn_model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,#RMSprop()\n",
    "                      metrics=['accuracy'])\n",
    "        history = cnn_model.fit(train_x,train_y,\n",
    "                            batch_size = 64,epochs=int(epochs*2/3),\n",
    "                            verbose=1,validation_data=(valid_x, valid_y))#,validation_data=(test, Y_test))\n",
    "        history_list.append(history)\n",
    "        optimizer = Adam(lr= 0.0001)\n",
    "        cnn_model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,#RMSprop()\n",
    "                      metrics=['accuracy'])\n",
    "        history = cnn_model.fit(train_x,train_y,\n",
    "                            batch_size = 128,epochs=int(epochs*2/3),\n",
    "                            verbose=1,validation_data=(valid_x, valid_y))#,validation_data=(test, Y_test))\n",
    "        history_list.append(history)\n",
    "    return cnn_model, history_from_list(history_list)\n",
    "\n",
    "def CNN_ensemble_models(optimizer,epochs,train_x,train_y,valid_x = None,valid_y = None, Batch_Norm = False,drop_out = 0.5):\n",
    "    cnn_model = CNN_architecture(Batch_Norm,drop_out)\n",
    "    history_list = []\n",
    "    for i in range(3):\n",
    "        optimizer = Adam(lr= 0.001)\n",
    "        cnn_model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,#RMSprop()\n",
    "                      metrics=['accuracy'])\n",
    "        history = cnn_model.fit(train_x,train_y,\n",
    "                            batch_size = 64,epochs=int(epochs*2/3),\n",
    "                            verbose=1,validation_data=(valid_x, valid_y))#,validation_data=(test, Y_test))\n",
    "        history_list.append(history)\n",
    "        optimizer = Adam(lr= 0.0001)\n",
    "        cnn_model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,#RMSprop()\n",
    "                      metrics=['accuracy'])\n",
    "        history = cnn_model.fit(train_x,train_y,\n",
    "                            batch_size = 128,epochs=int(epochs*2/3),\n",
    "                            verbose=1,validation_data=(valid_x, valid_y))#,validation_data=(test, Y_test))\n",
    "        history_list.append(history)\n",
    "        models.append(clone_model(cnn_model))\n",
    "    return models,history_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "\n",
      "\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 16s 463us/step - loss: 0.7856 - acc: 0.7391 - val_loss: 0.1012 - val_acc: 0.9725\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.3219 - acc: 0.9038 - val_loss: 0.0795 - val_acc: 0.9789\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.2373 - acc: 0.9296 - val_loss: 0.0606 - val_acc: 0.9855\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.1946 - acc: 0.9429 - val_loss: 0.0622 - val_acc: 0.9848\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.1723 - acc: 0.9505 - val_loss: 0.0556 - val_acc: 0.9864\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.1633 - acc: 0.9536 - val_loss: 0.0488 - val_acc: 0.9880\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.1419 - acc: 0.9598 - val_loss: 0.0470 - val_acc: 0.9889\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.1348 - acc: 0.9616 - val_loss: 0.0439 - val_acc: 0.9901\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.1213 - acc: 0.9662 - val_loss: 0.0469 - val_acc: 0.9901\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.1128 - acc: 0.9693 - val_loss: 0.0439 - val_acc: 0.9901\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.1085 - acc: 0.9669 - val_loss: 0.0390 - val_acc: 0.9915\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 10s 287us/step - loss: 0.1014 - acc: 0.9723 - val_loss: 0.0537 - val_acc: 0.9889\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0963 - acc: 0.9722 - val_loss: 0.0366 - val_acc: 0.9919\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0905 - acc: 0.9731 - val_loss: 0.0444 - val_acc: 0.9899\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0877 - acc: 0.9745 - val_loss: 0.0292 - val_acc: 0.9921\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.0837 - acc: 0.9763 - val_loss: 0.0412 - val_acc: 0.9908\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 10s 287us/step - loss: 0.0796 - acc: 0.9764 - val_loss: 0.0372 - val_acc: 0.9917\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.0802 - acc: 0.9760 - val_loss: 0.0377 - val_acc: 0.9925\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.0782 - acc: 0.9763 - val_loss: 0.0498 - val_acc: 0.9880\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0781 - acc: 0.9763 - val_loss: 0.0309 - val_acc: 0.9924\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0722 - acc: 0.9799 - val_loss: 0.0368 - val_acc: 0.9921\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0730 - acc: 0.9780 - val_loss: 0.0308 - val_acc: 0.9936\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 10s 287us/step - loss: 0.0749 - acc: 0.9776 - val_loss: 0.0334 - val_acc: 0.9930\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0723 - acc: 0.9783 - val_loss: 0.0308 - val_acc: 0.9921\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.0661 - acc: 0.9802 - val_loss: 0.0324 - val_acc: 0.9925\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0691 - acc: 0.9798 - val_loss: 0.0317 - val_acc: 0.9925\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 13s 396us/step - loss: 0.0551 - acc: 0.9830 - val_loss: 0.0291 - val_acc: 0.9933\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0499 - acc: 0.9847 - val_loss: 0.0280 - val_acc: 0.9937\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0478 - acc: 0.9852 - val_loss: 0.0271 - val_acc: 0.9943\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0482 - acc: 0.9849 - val_loss: 0.0280 - val_acc: 0.9938\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 7s 220us/step - loss: 0.0451 - acc: 0.9849 - val_loss: 0.0277 - val_acc: 0.9943\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0471 - acc: 0.9856 - val_loss: 0.0276 - val_acc: 0.9940\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0426 - acc: 0.9865 - val_loss: 0.0283 - val_acc: 0.9938\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 7s 220us/step - loss: 0.0417 - acc: 0.9865 - val_loss: 0.0290 - val_acc: 0.9935\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0425 - acc: 0.9868 - val_loss: 0.0290 - val_acc: 0.9939\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0442 - acc: 0.9864 - val_loss: 0.0294 - val_acc: 0.9936\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0410 - acc: 0.9869 - val_loss: 0.0296 - val_acc: 0.9936\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0418 - acc: 0.9867 - val_loss: 0.0304 - val_acc: 0.9940\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0404 - acc: 0.9876 - val_loss: 0.0303 - val_acc: 0.9942\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0405 - acc: 0.9865 - val_loss: 0.0296 - val_acc: 0.9943\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0392 - acc: 0.9878 - val_loss: 0.0306 - val_acc: 0.9940\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0399 - acc: 0.9867 - val_loss: 0.0306 - val_acc: 0.9940\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0378 - acc: 0.9888 - val_loss: 0.0312 - val_acc: 0.9940\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0403 - acc: 0.9876 - val_loss: 0.0307 - val_acc: 0.9942\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0399 - acc: 0.9868 - val_loss: 0.0298 - val_acc: 0.9944\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.0381 - acc: 0.9876 - val_loss: 0.0302 - val_acc: 0.9943\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0374 - acc: 0.9881 - val_loss: 0.0281 - val_acc: 0.9942\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0375 - acc: 0.9883 - val_loss: 0.0282 - val_acc: 0.9943\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0371 - acc: 0.9884 - val_loss: 0.0316 - val_acc: 0.9940\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0380 - acc: 0.9876 - val_loss: 0.0316 - val_acc: 0.9942\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0359 - acc: 0.9895 - val_loss: 0.0318 - val_acc: 0.9942\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0354 - acc: 0.9891 - val_loss: 0.0332 - val_acc: 0.9940\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 16s 483us/step - loss: 0.0575 - acc: 0.9837 - val_loss: 0.0313 - val_acc: 0.9946\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.0613 - acc: 0.9823 - val_loss: 0.0362 - val_acc: 0.9927\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0551 - acc: 0.9826 - val_loss: 0.0432 - val_acc: 0.9930\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0624 - acc: 0.9831 - val_loss: 0.0446 - val_acc: 0.9917\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.0632 - acc: 0.9815 - val_loss: 0.0446 - val_acc: 0.9915\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0613 - acc: 0.9824 - val_loss: 0.0339 - val_acc: 0.9926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.0541 - acc: 0.9837 - val_loss: 0.0436 - val_acc: 0.9923\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.0544 - acc: 0.9829 - val_loss: 0.0407 - val_acc: 0.9926\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 10s 288us/step - loss: 0.0495 - acc: 0.9848 - val_loss: 0.0391 - val_acc: 0.9927\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0617 - acc: 0.9824 - val_loss: 0.0470 - val_acc: 0.9907\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0546 - acc: 0.9840 - val_loss: 0.0377 - val_acc: 0.9923\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0509 - acc: 0.9841 - val_loss: 0.0350 - val_acc: 0.9933\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0504 - acc: 0.9847 - val_loss: 0.0350 - val_acc: 0.9927\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 10s 288us/step - loss: 0.0543 - acc: 0.9833 - val_loss: 0.0359 - val_acc: 0.9929\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0490 - acc: 0.9848 - val_loss: 0.0431 - val_acc: 0.9927\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.0502 - acc: 0.9852 - val_loss: 0.0344 - val_acc: 0.9939\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0481 - acc: 0.9849 - val_loss: 0.0489 - val_acc: 0.9919\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.0536 - acc: 0.9845 - val_loss: 0.0412 - val_acc: 0.9930\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.0561 - acc: 0.9831 - val_loss: 0.0451 - val_acc: 0.9917\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0534 - acc: 0.9844 - val_loss: 0.0604 - val_acc: 0.9910\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 10s 287us/step - loss: 0.0547 - acc: 0.9836 - val_loss: 0.0379 - val_acc: 0.9938\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0526 - acc: 0.9845 - val_loss: 0.0378 - val_acc: 0.9936\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0463 - acc: 0.9859 - val_loss: 0.0393 - val_acc: 0.9929\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.0475 - acc: 0.9855 - val_loss: 0.0460 - val_acc: 0.9929\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0432 - acc: 0.9868 - val_loss: 0.0459 - val_acc: 0.9926\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.0533 - acc: 0.9840 - val_loss: 0.0402 - val_acc: 0.9931\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 13s 399us/step - loss: 0.0425 - acc: 0.9873 - val_loss: 0.0409 - val_acc: 0.9936\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0407 - acc: 0.9880 - val_loss: 0.0389 - val_acc: 0.9943\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0372 - acc: 0.9885 - val_loss: 0.0377 - val_acc: 0.9940\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0382 - acc: 0.9880 - val_loss: 0.0391 - val_acc: 0.9939\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0354 - acc: 0.9887 - val_loss: 0.0386 - val_acc: 0.9942\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 7s 216us/step - loss: 0.0339 - acc: 0.9893 - val_loss: 0.0371 - val_acc: 0.9945\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0325 - acc: 0.9896 - val_loss: 0.0393 - val_acc: 0.9942\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0347 - acc: 0.9885 - val_loss: 0.0379 - val_acc: 0.9942\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0314 - acc: 0.9897 - val_loss: 0.0358 - val_acc: 0.9944\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0335 - acc: 0.9890 - val_loss: 0.0378 - val_acc: 0.9943\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0310 - acc: 0.9900 - val_loss: 0.0379 - val_acc: 0.9944\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0325 - acc: 0.9888 - val_loss: 0.0370 - val_acc: 0.9946\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0300 - acc: 0.9902 - val_loss: 0.0369 - val_acc: 0.9940\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0316 - acc: 0.9902 - val_loss: 0.0368 - val_acc: 0.9943\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0329 - acc: 0.9890 - val_loss: 0.0368 - val_acc: 0.9940\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 8s 224us/step - loss: 0.0312 - acc: 0.9894 - val_loss: 0.0374 - val_acc: 0.9943\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 8s 225us/step - loss: 0.0311 - acc: 0.9898 - val_loss: 0.0371 - val_acc: 0.9942\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0330 - acc: 0.9890 - val_loss: 0.0360 - val_acc: 0.9944\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.0296 - acc: 0.9903 - val_loss: 0.0366 - val_acc: 0.9946\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.0300 - acc: 0.9901 - val_loss: 0.0355 - val_acc: 0.9943\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 8s 224us/step - loss: 0.0296 - acc: 0.9900 - val_loss: 0.0349 - val_acc: 0.9942\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.0307 - acc: 0.9898 - val_loss: 0.0362 - val_acc: 0.9940\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.0281 - acc: 0.9902 - val_loss: 0.0358 - val_acc: 0.9946\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 8s 226us/step - loss: 0.0305 - acc: 0.9899 - val_loss: 0.0380 - val_acc: 0.9942\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 8s 229us/step - loss: 0.0312 - acc: 0.9896 - val_loss: 0.0378 - val_acc: 0.9944\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 8s 224us/step - loss: 0.0282 - acc: 0.9912 - val_loss: 0.0379 - val_acc: 0.9948\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 16s 479us/step - loss: 0.0508 - acc: 0.9860 - val_loss: 0.0530 - val_acc: 0.9921\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 10s 290us/step - loss: 0.0474 - acc: 0.9865 - val_loss: 0.0421 - val_acc: 0.9926\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 10s 293us/step - loss: 0.0466 - acc: 0.9857 - val_loss: 0.0463 - val_acc: 0.9933\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 10s 301us/step - loss: 0.0483 - acc: 0.9854 - val_loss: 0.0377 - val_acc: 0.9936\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.0468 - acc: 0.9863 - val_loss: 0.0395 - val_acc: 0.9932\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 10s 294us/step - loss: 0.0463 - acc: 0.9860 - val_loss: 0.0378 - val_acc: 0.9933\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.0459 - acc: 0.9863 - val_loss: 0.0427 - val_acc: 0.9938\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.0463 - acc: 0.9862 - val_loss: 0.0359 - val_acc: 0.9940\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0414 - acc: 0.9869 - val_loss: 0.0447 - val_acc: 0.9935\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 10s 287us/step - loss: 0.0531 - acc: 0.9847 - val_loss: 0.0420 - val_acc: 0.9937\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 10s 287us/step - loss: 0.0454 - acc: 0.9863 - val_loss: 0.0403 - val_acc: 0.9926\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.0473 - acc: 0.9860 - val_loss: 0.0386 - val_acc: 0.9939\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 10s 284us/step - loss: 0.0472 - acc: 0.9862 - val_loss: 0.0422 - val_acc: 0.9932\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0436 - acc: 0.9867 - val_loss: 0.0477 - val_acc: 0.9936\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0459 - acc: 0.9854 - val_loss: 0.0559 - val_acc: 0.9920\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.0462 - acc: 0.9864 - val_loss: 0.0464 - val_acc: 0.9929\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0476 - acc: 0.9857 - val_loss: 0.0511 - val_acc: 0.9925\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0443 - acc: 0.9859 - val_loss: 0.0539 - val_acc: 0.9929\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0461 - acc: 0.9867 - val_loss: 0.0462 - val_acc: 0.9927\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0423 - acc: 0.9876 - val_loss: 0.0523 - val_acc: 0.9927\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0436 - acc: 0.9864 - val_loss: 0.0478 - val_acc: 0.9931\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.0511 - acc: 0.9854 - val_loss: 0.0437 - val_acc: 0.9926\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 10s 288us/step - loss: 0.0468 - acc: 0.9860 - val_loss: 0.0420 - val_acc: 0.9935\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.0450 - acc: 0.9869 - val_loss: 0.0451 - val_acc: 0.9926\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.0392 - acc: 0.9882 - val_loss: 0.0477 - val_acc: 0.9924\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 10s 285us/step - loss: 0.0424 - acc: 0.9865 - val_loss: 0.0405 - val_acc: 0.9931\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 13s 401us/step - loss: 0.0375 - acc: 0.9886 - val_loss: 0.0408 - val_acc: 0.9933\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0372 - acc: 0.9884 - val_loss: 0.0381 - val_acc: 0.9936\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0341 - acc: 0.9894 - val_loss: 0.0395 - val_acc: 0.9937\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0341 - acc: 0.9896 - val_loss: 0.0393 - val_acc: 0.9936\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0312 - acc: 0.9900 - val_loss: 0.0396 - val_acc: 0.9939\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0314 - acc: 0.9904 - val_loss: 0.0404 - val_acc: 0.9940\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 7s 220us/step - loss: 0.0325 - acc: 0.9897 - val_loss: 0.0392 - val_acc: 0.9939\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0313 - acc: 0.9899 - val_loss: 0.0397 - val_acc: 0.9940\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0290 - acc: 0.9899 - val_loss: 0.0425 - val_acc: 0.9937\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0282 - acc: 0.9910 - val_loss: 0.0411 - val_acc: 0.9942\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0308 - acc: 0.9904 - val_loss: 0.0423 - val_acc: 0.9940\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0277 - acc: 0.9905 - val_loss: 0.0390 - val_acc: 0.9942\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 7s 217us/step - loss: 0.0280 - acc: 0.9912 - val_loss: 0.0391 - val_acc: 0.9945\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 7s 216us/step - loss: 0.0272 - acc: 0.9915 - val_loss: 0.0381 - val_acc: 0.9943\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0289 - acc: 0.9900 - val_loss: 0.0382 - val_acc: 0.9945\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0268 - acc: 0.9917 - val_loss: 0.0394 - val_acc: 0.9943\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0297 - acc: 0.9902 - val_loss: 0.0390 - val_acc: 0.9949\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0299 - acc: 0.9899 - val_loss: 0.0384 - val_acc: 0.9944\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0296 - acc: 0.9899 - val_loss: 0.0384 - val_acc: 0.9946\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0265 - acc: 0.9917 - val_loss: 0.0389 - val_acc: 0.9944\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0265 - acc: 0.9914 - val_loss: 0.0402 - val_acc: 0.9948\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 7s 220us/step - loss: 0.0279 - acc: 0.9908 - val_loss: 0.0420 - val_acc: 0.9944\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.0275 - acc: 0.9911 - val_loss: 0.0418 - val_acc: 0.9939\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 8s 231us/step - loss: 0.0323 - acc: 0.9898 - val_loss: 0.0424 - val_acc: 0.9942\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.0312 - acc: 0.9895 - val_loss: 0.0408 - val_acc: 0.9938\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0261 - acc: 0.9913 - val_loss: 0.0410 - val_acc: 0.9939\n",
      "starting\n",
      "\n",
      "\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 16s 475us/step - loss: 0.6302 - acc: 0.7940 - val_loss: 0.0907 - val_acc: 0.9765\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 10s 297us/step - loss: 0.2318 - acc: 0.9360 - val_loss: 0.0684 - val_acc: 0.9817\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1747 - acc: 0.9526 - val_loss: 0.0535 - val_acc: 0.9857\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1418 - acc: 0.9613 - val_loss: 0.0484 - val_acc: 0.9877\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1221 - acc: 0.9669 - val_loss: 0.0585 - val_acc: 0.9850\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 10s 290us/step - loss: 0.1113 - acc: 0.9709 - val_loss: 0.0522 - val_acc: 0.9864\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 10s 293us/step - loss: 0.0956 - acc: 0.9729 - val_loss: 0.0444 - val_acc: 0.9880\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 10s 295us/step - loss: 0.0962 - acc: 0.9748 - val_loss: 0.0412 - val_acc: 0.9898\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 10s 287us/step - loss: 0.0839 - acc: 0.9776 - val_loss: 0.0417 - val_acc: 0.9885\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.0797 - acc: 0.9782 - val_loss: 0.0384 - val_acc: 0.9893\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 10s 290us/step - loss: 0.0730 - acc: 0.9800 - val_loss: 0.0492 - val_acc: 0.9885\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.0697 - acc: 0.9809 - val_loss: 0.0421 - val_acc: 0.9901\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 10s 288us/step - loss: 0.0682 - acc: 0.9818 - val_loss: 0.0386 - val_acc: 0.9907\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 10s 288us/step - loss: 0.0607 - acc: 0.9823 - val_loss: 0.0393 - val_acc: 0.9898\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 10s 290us/step - loss: 0.0621 - acc: 0.9825 - val_loss: 0.0395 - val_acc: 0.9902\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.0526 - acc: 0.9856 - val_loss: 0.0453 - val_acc: 0.9912\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.0569 - acc: 0.9843 - val_loss: 0.0341 - val_acc: 0.9908\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 10s 288us/step - loss: 0.0504 - acc: 0.9862 - val_loss: 0.0367 - val_acc: 0.9917\n",
      "Epoch 19/26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 10s 291us/step - loss: 0.0509 - acc: 0.9857 - val_loss: 0.0349 - val_acc: 0.9927\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.0552 - acc: 0.9845 - val_loss: 0.0362 - val_acc: 0.9918\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 10s 287us/step - loss: 0.0495 - acc: 0.9852 - val_loss: 0.0358 - val_acc: 0.9915\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.0455 - acc: 0.9867 - val_loss: 0.0329 - val_acc: 0.9921\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 10s 291us/step - loss: 0.0462 - acc: 0.9868 - val_loss: 0.0377 - val_acc: 0.9911\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 10s 288us/step - loss: 0.0443 - acc: 0.9868 - val_loss: 0.0307 - val_acc: 0.9925\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 10s 297us/step - loss: 0.0432 - acc: 0.9878 - val_loss: 0.0385 - val_acc: 0.9929\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 10s 288us/step - loss: 0.0397 - acc: 0.9888 - val_loss: 0.0379 - val_acc: 0.9932\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 14s 408us/step - loss: 0.0336 - acc: 0.9903 - val_loss: 0.0348 - val_acc: 0.9938\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0289 - acc: 0.9914 - val_loss: 0.0354 - val_acc: 0.9939\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0265 - acc: 0.9917 - val_loss: 0.0344 - val_acc: 0.9938\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 7s 218us/step - loss: 0.0252 - acc: 0.9922 - val_loss: 0.0345 - val_acc: 0.9939\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0267 - acc: 0.9912 - val_loss: 0.0336 - val_acc: 0.9937\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0234 - acc: 0.9925 - val_loss: 0.0329 - val_acc: 0.9940\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 8s 224us/step - loss: 0.0244 - acc: 0.9917 - val_loss: 0.0323 - val_acc: 0.9940\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 8s 223us/step - loss: 0.0238 - acc: 0.9923 - val_loss: 0.0315 - val_acc: 0.9943\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0219 - acc: 0.9927 - val_loss: 0.0321 - val_acc: 0.9948\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.0206 - acc: 0.9931 - val_loss: 0.0345 - val_acc: 0.9943\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.0236 - acc: 0.9924 - val_loss: 0.0328 - val_acc: 0.9945\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0206 - acc: 0.9935 - val_loss: 0.0332 - val_acc: 0.9939\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 8s 229us/step - loss: 0.0214 - acc: 0.9927 - val_loss: 0.0325 - val_acc: 0.9944\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0203 - acc: 0.9941 - val_loss: 0.0341 - val_acc: 0.9943\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0193 - acc: 0.9935 - val_loss: 0.0342 - val_acc: 0.9939\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0228 - acc: 0.9932 - val_loss: 0.0334 - val_acc: 0.9940\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.0213 - acc: 0.9927 - val_loss: 0.0327 - val_acc: 0.9942\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.0201 - acc: 0.9932 - val_loss: 0.0333 - val_acc: 0.9943\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 8s 224us/step - loss: 0.0183 - acc: 0.9935 - val_loss: 0.0353 - val_acc: 0.9944\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 8s 225us/step - loss: 0.0193 - acc: 0.9939 - val_loss: 0.0345 - val_acc: 0.9943\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 8s 226us/step - loss: 0.0196 - acc: 0.9933 - val_loss: 0.0344 - val_acc: 0.9945\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 8s 224us/step - loss: 0.0188 - acc: 0.9937 - val_loss: 0.0373 - val_acc: 0.9944\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0180 - acc: 0.9940 - val_loss: 0.0368 - val_acc: 0.9938\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 8s 223us/step - loss: 0.0195 - acc: 0.9941 - val_loss: 0.0352 - val_acc: 0.9946\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.0194 - acc: 0.9942 - val_loss: 0.0368 - val_acc: 0.9940\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0188 - acc: 0.9937 - val_loss: 0.0357 - val_acc: 0.9940\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 17s 500us/step - loss: 0.0370 - acc: 0.9906 - val_loss: 0.0372 - val_acc: 0.9932\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 10s 291us/step - loss: 0.0364 - acc: 0.9897 - val_loss: 0.0500 - val_acc: 0.9918\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 10s 302us/step - loss: 0.0423 - acc: 0.9890 - val_loss: 0.0345 - val_acc: 0.9936\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 10s 298us/step - loss: 0.0339 - acc: 0.9903 - val_loss: 0.0473 - val_acc: 0.9925\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 10s 296us/step - loss: 0.0386 - acc: 0.9891 - val_loss: 0.0378 - val_acc: 0.9908\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 10s 306us/step - loss: 0.0369 - acc: 0.9895 - val_loss: 0.0346 - val_acc: 0.9927\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 10s 295us/step - loss: 0.0323 - acc: 0.9903 - val_loss: 0.0375 - val_acc: 0.9936\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 10s 300us/step - loss: 0.0331 - acc: 0.9901 - val_loss: 0.0388 - val_acc: 0.9931\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 10s 302us/step - loss: 0.0340 - acc: 0.9901 - val_loss: 0.0429 - val_acc: 0.9930\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 10s 297us/step - loss: 0.0374 - acc: 0.9894 - val_loss: 0.0417 - val_acc: 0.9923\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 10s 295us/step - loss: 0.0327 - acc: 0.9902 - val_loss: 0.0543 - val_acc: 0.9905\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 10s 302us/step - loss: 0.0312 - acc: 0.9910 - val_loss: 0.0399 - val_acc: 0.9921\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 10s 308us/step - loss: 0.0298 - acc: 0.9913 - val_loss: 0.0468 - val_acc: 0.9927\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 10s 302us/step - loss: 0.0363 - acc: 0.9897 - val_loss: 0.0418 - val_acc: 0.9921\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 10s 294us/step - loss: 0.0313 - acc: 0.9907 - val_loss: 0.0591 - val_acc: 0.9906\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.0307 - acc: 0.9906 - val_loss: 0.0418 - val_acc: 0.9926\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 10s 298us/step - loss: 0.0327 - acc: 0.9902 - val_loss: 0.0376 - val_acc: 0.9931\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 10s 299us/step - loss: 0.0288 - acc: 0.9914 - val_loss: 0.0402 - val_acc: 0.9923\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 10s 297us/step - loss: 0.0287 - acc: 0.9907 - val_loss: 0.0355 - val_acc: 0.9923\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 10s 298us/step - loss: 0.0312 - acc: 0.9908 - val_loss: 0.0448 - val_acc: 0.9923\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 10s 293us/step - loss: 0.0292 - acc: 0.9915 - val_loss: 0.0350 - val_acc: 0.9924\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 10s 303us/step - loss: 0.0305 - acc: 0.9914 - val_loss: 0.0342 - val_acc: 0.9936\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 10s 305us/step - loss: 0.0274 - acc: 0.9917 - val_loss: 0.0395 - val_acc: 0.9943\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 10s 299us/step - loss: 0.0256 - acc: 0.9929 - val_loss: 0.0437 - val_acc: 0.9923\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 10s 299us/step - loss: 0.0284 - acc: 0.9918 - val_loss: 0.0390 - val_acc: 0.9927\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 10s 295us/step - loss: 0.0241 - acc: 0.9926 - val_loss: 0.0373 - val_acc: 0.9933\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 14s 422us/step - loss: 0.0225 - acc: 0.9927 - val_loss: 0.0345 - val_acc: 0.9945\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 8s 229us/step - loss: 0.0185 - acc: 0.9946 - val_loss: 0.0339 - val_acc: 0.9946\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 8s 229us/step - loss: 0.0185 - acc: 0.9946 - val_loss: 0.0345 - val_acc: 0.9937\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0171 - acc: 0.9953 - val_loss: 0.0337 - val_acc: 0.9939\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0164 - acc: 0.9950 - val_loss: 0.0343 - val_acc: 0.9944\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 8s 227us/step - loss: 0.0148 - acc: 0.9949 - val_loss: 0.0331 - val_acc: 0.9950\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 8s 225us/step - loss: 0.0149 - acc: 0.9949 - val_loss: 0.0333 - val_acc: 0.9946\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 8s 223us/step - loss: 0.0153 - acc: 0.9953 - val_loss: 0.0339 - val_acc: 0.9944\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 8s 230us/step - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0346 - val_acc: 0.9948\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 8s 231us/step - loss: 0.0145 - acc: 0.9954 - val_loss: 0.0353 - val_acc: 0.9949\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 8s 231us/step - loss: 0.0153 - acc: 0.9947 - val_loss: 0.0363 - val_acc: 0.9943\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 8s 225us/step - loss: 0.0146 - acc: 0.9954 - val_loss: 0.0391 - val_acc: 0.9942\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 8s 230us/step - loss: 0.0135 - acc: 0.9956 - val_loss: 0.0397 - val_acc: 0.9940\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 8s 234us/step - loss: 0.0138 - acc: 0.9955 - val_loss: 0.0381 - val_acc: 0.9940\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 8s 231us/step - loss: 0.0150 - acc: 0.9955 - val_loss: 0.0367 - val_acc: 0.9945\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 8s 232us/step - loss: 0.0144 - acc: 0.9951 - val_loss: 0.0382 - val_acc: 0.9943\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0376 - val_acc: 0.9939\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 8s 225us/step - loss: 0.0119 - acc: 0.9961 - val_loss: 0.0389 - val_acc: 0.9943\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 8s 233us/step - loss: 0.0111 - acc: 0.9967 - val_loss: 0.0399 - val_acc: 0.9945\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 8s 234us/step - loss: 0.0117 - acc: 0.9962 - val_loss: 0.0382 - val_acc: 0.9939\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 8s 229us/step - loss: 0.0130 - acc: 0.9954 - val_loss: 0.0380 - val_acc: 0.9944\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 8s 230us/step - loss: 0.0139 - acc: 0.9950 - val_loss: 0.0390 - val_acc: 0.9943\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 8s 232us/step - loss: 0.0129 - acc: 0.9956 - val_loss: 0.0389 - val_acc: 0.9948\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 8s 230us/step - loss: 0.0115 - acc: 0.9958 - val_loss: 0.0384 - val_acc: 0.9946\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 8s 231us/step - loss: 0.0121 - acc: 0.9958 - val_loss: 0.0391 - val_acc: 0.9945\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 8s 232us/step - loss: 0.0130 - acc: 0.9958 - val_loss: 0.0403 - val_acc: 0.9943\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 17s 510us/step - loss: 0.0283 - acc: 0.9925 - val_loss: 0.0523 - val_acc: 0.9907\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 10s 305us/step - loss: 0.0258 - acc: 0.9924 - val_loss: 0.0478 - val_acc: 0.9925\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 10s 305us/step - loss: 0.0291 - acc: 0.9923 - val_loss: 0.0393 - val_acc: 0.9935\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 10s 303us/step - loss: 0.0276 - acc: 0.9918 - val_loss: 0.0559 - val_acc: 0.9910\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 10s 306us/step - loss: 0.0257 - acc: 0.9924 - val_loss: 0.0461 - val_acc: 0.9926\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 10s 310us/step - loss: 0.0269 - acc: 0.9915 - val_loss: 0.0434 - val_acc: 0.9929\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 10s 296us/step - loss: 0.0257 - acc: 0.9926 - val_loss: 0.0473 - val_acc: 0.9924\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 10s 306us/step - loss: 0.0253 - acc: 0.9924 - val_loss: 0.0429 - val_acc: 0.9921\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 10s 303us/step - loss: 0.0245 - acc: 0.9927 - val_loss: 0.0478 - val_acc: 0.9925\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 10s 300us/step - loss: 0.0249 - acc: 0.9935 - val_loss: 0.0446 - val_acc: 0.9936\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 10s 295us/step - loss: 0.0233 - acc: 0.9930 - val_loss: 0.0434 - val_acc: 0.9931\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 10s 306us/step - loss: 0.0268 - acc: 0.9922 - val_loss: 0.0340 - val_acc: 0.9932\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 10s 309us/step - loss: 0.0250 - acc: 0.9924 - val_loss: 0.0381 - val_acc: 0.9937\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 10s 299us/step - loss: 0.0252 - acc: 0.9929 - val_loss: 0.0429 - val_acc: 0.9935\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 10s 306us/step - loss: 0.0262 - acc: 0.9930 - val_loss: 0.0448 - val_acc: 0.9932\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 10s 306us/step - loss: 0.0256 - acc: 0.9924 - val_loss: 0.0476 - val_acc: 0.9925\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 10s 302us/step - loss: 0.0249 - acc: 0.9930 - val_loss: 0.0473 - val_acc: 0.9930\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 10s 312us/step - loss: 0.0290 - acc: 0.9919 - val_loss: 0.0419 - val_acc: 0.9929\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 10s 304us/step - loss: 0.0235 - acc: 0.9929 - val_loss: 0.0484 - val_acc: 0.9930\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 10s 309us/step - loss: 0.0266 - acc: 0.9930 - val_loss: 0.0406 - val_acc: 0.9931\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 10s 303us/step - loss: 0.0203 - acc: 0.9940 - val_loss: 0.0448 - val_acc: 0.9923\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 10s 304us/step - loss: 0.0237 - acc: 0.9929 - val_loss: 0.0470 - val_acc: 0.9930\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 10s 308us/step - loss: 0.0289 - acc: 0.9920 - val_loss: 0.0394 - val_acc: 0.9931\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 10s 310us/step - loss: 0.0226 - acc: 0.9933 - val_loss: 0.0407 - val_acc: 0.9933\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 10s 296us/step - loss: 0.0287 - acc: 0.9923 - val_loss: 0.0521 - val_acc: 0.9925\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 10s 306us/step - loss: 0.0258 - acc: 0.9926 - val_loss: 0.0520 - val_acc: 0.9925\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 14s 431us/step - loss: 0.0241 - acc: 0.9931 - val_loss: 0.0475 - val_acc: 0.9940\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0202 - acc: 0.9946 - val_loss: 0.0477 - val_acc: 0.9937\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 8s 225us/step - loss: 0.0181 - acc: 0.9946 - val_loss: 0.0478 - val_acc: 0.9938\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 8s 226us/step - loss: 0.0149 - acc: 0.9955 - val_loss: 0.0458 - val_acc: 0.9944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 8s 229us/step - loss: 0.0184 - acc: 0.9945 - val_loss: 0.0459 - val_acc: 0.9944\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 8s 235us/step - loss: 0.0152 - acc: 0.9954 - val_loss: 0.0478 - val_acc: 0.9945\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 8s 237us/step - loss: 0.0150 - acc: 0.9956 - val_loss: 0.0466 - val_acc: 0.9945\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 8s 239us/step - loss: 0.0157 - acc: 0.9952 - val_loss: 0.0484 - val_acc: 0.9944\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 8s 237us/step - loss: 0.0136 - acc: 0.9958 - val_loss: 0.0469 - val_acc: 0.9942\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 8s 234us/step - loss: 0.0136 - acc: 0.9962 - val_loss: 0.0464 - val_acc: 0.9944\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 8s 235us/step - loss: 0.0144 - acc: 0.9951 - val_loss: 0.0479 - val_acc: 0.9945\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 8s 233us/step - loss: 0.0133 - acc: 0.9958 - val_loss: 0.0480 - val_acc: 0.9939\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 8s 236us/step - loss: 0.0149 - acc: 0.9951 - val_loss: 0.0480 - val_acc: 0.9938\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 8s 238us/step - loss: 0.0119 - acc: 0.9960 - val_loss: 0.0480 - val_acc: 0.9939\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 8s 239us/step - loss: 0.0139 - acc: 0.9958 - val_loss: 0.0470 - val_acc: 0.9939\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 8s 234us/step - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0471 - val_acc: 0.9942\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 8s 232us/step - loss: 0.0114 - acc: 0.9966 - val_loss: 0.0448 - val_acc: 0.9939\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 8s 236us/step - loss: 0.0136 - acc: 0.9958 - val_loss: 0.0471 - val_acc: 0.9942\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 8s 235us/step - loss: 0.0121 - acc: 0.9960 - val_loss: 0.0467 - val_acc: 0.9940\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 8s 234us/step - loss: 0.0131 - acc: 0.9963 - val_loss: 0.0475 - val_acc: 0.9939\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 8s 237us/step - loss: 0.0114 - acc: 0.9964 - val_loss: 0.0473 - val_acc: 0.9939\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 8s 238us/step - loss: 0.0103 - acc: 0.9963 - val_loss: 0.0464 - val_acc: 0.9944\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 8s 233us/step - loss: 0.0116 - acc: 0.9961 - val_loss: 0.0471 - val_acc: 0.9942\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 8s 227us/step - loss: 0.0092 - acc: 0.9967 - val_loss: 0.0470 - val_acc: 0.9945\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 8s 231us/step - loss: 0.0133 - acc: 0.9960 - val_loss: 0.0457 - val_acc: 0.9944\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 8s 230us/step - loss: 0.0115 - acc: 0.9964 - val_loss: 0.0458 - val_acc: 0.9945\n",
      "starting\n",
      "\n",
      "\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 17s 500us/step - loss: 0.9391 - acc: 0.6743 - val_loss: 0.1360 - val_acc: 0.9640\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 10s 298us/step - loss: 0.4360 - acc: 0.8604 - val_loss: 0.0903 - val_acc: 0.9774\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 10s 295us/step - loss: 0.3494 - acc: 0.8907 - val_loss: 0.0642 - val_acc: 0.9843\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 10s 304us/step - loss: 0.2973 - acc: 0.9043 - val_loss: 0.0606 - val_acc: 0.9845\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 10s 301us/step - loss: 0.2652 - acc: 0.9161 - val_loss: 0.0463 - val_acc: 0.9876\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 10s 302us/step - loss: 0.2568 - acc: 0.9178 - val_loss: 0.0504 - val_acc: 0.9871\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 10s 296us/step - loss: 0.2400 - acc: 0.9240 - val_loss: 0.0495 - val_acc: 0.9885\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 10s 295us/step - loss: 0.2312 - acc: 0.9235 - val_loss: 0.0531 - val_acc: 0.9873\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 10s 294us/step - loss: 0.2248 - acc: 0.9265 - val_loss: 0.0399 - val_acc: 0.9902\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 10s 294us/step - loss: 0.2163 - acc: 0.9304 - val_loss: 0.0462 - val_acc: 0.9873\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.2078 - acc: 0.9310 - val_loss: 0.0461 - val_acc: 0.9882\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 10s 291us/step - loss: 0.2016 - acc: 0.9326 - val_loss: 0.0449 - val_acc: 0.9894\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 10s 293us/step - loss: 0.1911 - acc: 0.9343 - val_loss: 0.0428 - val_acc: 0.9908\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 10s 296us/step - loss: 0.1954 - acc: 0.9336 - val_loss: 0.0353 - val_acc: 0.9914\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 10s 294us/step - loss: 0.1788 - acc: 0.9401 - val_loss: 0.0346 - val_acc: 0.9918\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1723 - acc: 0.9430 - val_loss: 0.0405 - val_acc: 0.9907\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 10s 291us/step - loss: 0.1765 - acc: 0.9412 - val_loss: 0.0398 - val_acc: 0.9913\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 10s 293us/step - loss: 0.1811 - acc: 0.9385 - val_loss: 0.0346 - val_acc: 0.9912\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1656 - acc: 0.9438 - val_loss: 0.0401 - val_acc: 0.9914\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 10s 291us/step - loss: 0.1620 - acc: 0.9439 - val_loss: 0.0343 - val_acc: 0.9921\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 10s 291us/step - loss: 0.1469 - acc: 0.9502 - val_loss: 0.0375 - val_acc: 0.9931\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 10s 296us/step - loss: 0.1503 - acc: 0.9504 - val_loss: 0.0356 - val_acc: 0.9920\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1454 - acc: 0.9522 - val_loss: 0.0380 - val_acc: 0.9925\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.1414 - acc: 0.9540 - val_loss: 0.0459 - val_acc: 0.9896\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.1440 - acc: 0.9525 - val_loss: 0.0343 - val_acc: 0.9929\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1330 - acc: 0.9549 - val_loss: 0.0414 - val_acc: 0.9915\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 14s 424us/step - loss: 0.1245 - acc: 0.9576 - val_loss: 0.0354 - val_acc: 0.9927\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1178 - acc: 0.9583 - val_loss: 0.0346 - val_acc: 0.9929\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1218 - acc: 0.9571 - val_loss: 0.0355 - val_acc: 0.9925\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1182 - acc: 0.9581 - val_loss: 0.0346 - val_acc: 0.9932\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.1177 - acc: 0.9575 - val_loss: 0.0336 - val_acc: 0.9933\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1177 - acc: 0.9575 - val_loss: 0.0347 - val_acc: 0.9931\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.1073 - acc: 0.9618 - val_loss: 0.0344 - val_acc: 0.9931\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.1063 - acc: 0.9627 - val_loss: 0.0344 - val_acc: 0.9935\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.1078 - acc: 0.9601 - val_loss: 0.0369 - val_acc: 0.9931\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1089 - acc: 0.9620 - val_loss: 0.0342 - val_acc: 0.9936\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.1082 - acc: 0.9606 - val_loss: 0.0353 - val_acc: 0.9932\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1057 - acc: 0.9613 - val_loss: 0.0355 - val_acc: 0.9935\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1054 - acc: 0.9613 - val_loss: 0.0367 - val_acc: 0.9935\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1053 - acc: 0.9623 - val_loss: 0.0355 - val_acc: 0.9935\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.1066 - acc: 0.9620 - val_loss: 0.0368 - val_acc: 0.9938\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1038 - acc: 0.9625 - val_loss: 0.0376 - val_acc: 0.9937\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.1032 - acc: 0.9635 - val_loss: 0.0374 - val_acc: 0.9940\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1001 - acc: 0.9638 - val_loss: 0.0391 - val_acc: 0.9939\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1056 - acc: 0.9621 - val_loss: 0.0371 - val_acc: 0.9936\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1084 - acc: 0.9614 - val_loss: 0.0365 - val_acc: 0.9938\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.1017 - acc: 0.9640 - val_loss: 0.0366 - val_acc: 0.9943\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.1046 - acc: 0.9634 - val_loss: 0.0382 - val_acc: 0.9939\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.1000 - acc: 0.9634 - val_loss: 0.0399 - val_acc: 0.9935\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.0986 - acc: 0.9642 - val_loss: 0.0387 - val_acc: 0.9942\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 8s 223us/step - loss: 0.1000 - acc: 0.9630 - val_loss: 0.0377 - val_acc: 0.9943\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0979 - acc: 0.9644 - val_loss: 0.0379 - val_acc: 0.9944\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 17s 494us/step - loss: 0.1292 - acc: 0.9581 - val_loss: 0.0371 - val_acc: 0.9918\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.1356 - acc: 0.9564 - val_loss: 0.0385 - val_acc: 0.9936\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 10s 293us/step - loss: 0.1302 - acc: 0.9583 - val_loss: 0.0423 - val_acc: 0.9927\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1261 - acc: 0.9573 - val_loss: 0.0445 - val_acc: 0.9925\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 10s 294us/step - loss: 0.1339 - acc: 0.9565 - val_loss: 0.0456 - val_acc: 0.9921\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1282 - acc: 0.9569 - val_loss: 0.0502 - val_acc: 0.9911\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.1275 - acc: 0.9568 - val_loss: 0.0474 - val_acc: 0.9915\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.1265 - acc: 0.9589 - val_loss: 0.0415 - val_acc: 0.9930\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 10s 293us/step - loss: 0.1246 - acc: 0.9582 - val_loss: 0.0406 - val_acc: 0.9911\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1249 - acc: 0.9590 - val_loss: 0.0466 - val_acc: 0.9905\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 10s 293us/step - loss: 0.1287 - acc: 0.9588 - val_loss: 0.0444 - val_acc: 0.9919\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 10s 290us/step - loss: 0.1179 - acc: 0.9616 - val_loss: 0.0396 - val_acc: 0.9933\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 10s 290us/step - loss: 0.1206 - acc: 0.9605 - val_loss: 0.0381 - val_acc: 0.9929\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 10s 291us/step - loss: 0.1214 - acc: 0.9601 - val_loss: 0.0413 - val_acc: 0.9927\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 10s 294us/step - loss: 0.1140 - acc: 0.9629 - val_loss: 0.0388 - val_acc: 0.9930\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 10s 303us/step - loss: 0.1134 - acc: 0.9624 - val_loss: 0.0465 - val_acc: 0.9930\n",
      "Epoch 17/26\n",
      "33600/33600 [==============================] - 10s 290us/step - loss: 0.1122 - acc: 0.9636 - val_loss: 0.0464 - val_acc: 0.9920\n",
      "Epoch 18/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1120 - acc: 0.9637 - val_loss: 0.0443 - val_acc: 0.9926\n",
      "Epoch 19/26\n",
      "33600/33600 [==============================] - 10s 288us/step - loss: 0.1131 - acc: 0.9629 - val_loss: 0.0365 - val_acc: 0.9937\n",
      "Epoch 20/26\n",
      "33600/33600 [==============================] - 10s 292us/step - loss: 0.1110 - acc: 0.9624 - val_loss: 0.0395 - val_acc: 0.9927\n",
      "Epoch 21/26\n",
      "33600/33600 [==============================] - 10s 293us/step - loss: 0.1164 - acc: 0.9627 - val_loss: 0.0429 - val_acc: 0.9918\n",
      "Epoch 22/26\n",
      "33600/33600 [==============================] - 10s 290us/step - loss: 0.1035 - acc: 0.9652 - val_loss: 0.0396 - val_acc: 0.9917\n",
      "Epoch 23/26\n",
      "33600/33600 [==============================] - 10s 290us/step - loss: 0.1053 - acc: 0.9663 - val_loss: 0.0455 - val_acc: 0.9931\n",
      "Epoch 24/26\n",
      "33600/33600 [==============================] - 10s 289us/step - loss: 0.1038 - acc: 0.9646 - val_loss: 0.0414 - val_acc: 0.9921\n",
      "Epoch 25/26\n",
      "33600/33600 [==============================] - 10s 291us/step - loss: 0.1056 - acc: 0.9642 - val_loss: 0.0474 - val_acc: 0.9923\n",
      "Epoch 26/26\n",
      "33600/33600 [==============================] - 10s 291us/step - loss: 0.1061 - acc: 0.9646 - val_loss: 0.0474 - val_acc: 0.9936\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/26\n",
      "33600/33600 [==============================] - 14s 431us/step - loss: 0.0982 - acc: 0.9673 - val_loss: 0.0418 - val_acc: 0.9942\n",
      "Epoch 2/26\n",
      "33600/33600 [==============================] - 8s 224us/step - loss: 0.0884 - acc: 0.9690 - val_loss: 0.0389 - val_acc: 0.9944\n",
      "Epoch 3/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.0885 - acc: 0.9688 - val_loss: 0.0360 - val_acc: 0.9946\n",
      "Epoch 4/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0920 - acc: 0.9675 - val_loss: 0.0383 - val_acc: 0.9942\n",
      "Epoch 5/26\n",
      "33600/33600 [==============================] - 8s 225us/step - loss: 0.0866 - acc: 0.9698 - val_loss: 0.0380 - val_acc: 0.9945\n",
      "Epoch 6/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.0869 - acc: 0.9687 - val_loss: 0.0392 - val_acc: 0.9942\n",
      "Epoch 7/26\n",
      "33600/33600 [==============================] - 7s 219us/step - loss: 0.0867 - acc: 0.9695 - val_loss: 0.0363 - val_acc: 0.9943\n",
      "Epoch 8/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0818 - acc: 0.9703 - val_loss: 0.0377 - val_acc: 0.9946\n",
      "Epoch 9/26\n",
      "33600/33600 [==============================] - 8s 224us/step - loss: 0.0835 - acc: 0.9709 - val_loss: 0.0362 - val_acc: 0.9951\n",
      "Epoch 10/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0849 - acc: 0.9707 - val_loss: 0.0391 - val_acc: 0.9950\n",
      "Epoch 11/26\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.0841 - acc: 0.9703 - val_loss: 0.0355 - val_acc: 0.9950\n",
      "Epoch 12/26\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.0780 - acc: 0.9721 - val_loss: 0.0406 - val_acc: 0.9949\n",
      "Epoch 13/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0787 - acc: 0.9720 - val_loss: 0.0406 - val_acc: 0.9948\n",
      "Epoch 14/26\n",
      "33600/33600 [==============================] - 7s 220us/step - loss: 0.0808 - acc: 0.9710 - val_loss: 0.0395 - val_acc: 0.9951\n",
      "Epoch 15/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0799 - acc: 0.9722 - val_loss: 0.0421 - val_acc: 0.9948\n",
      "Epoch 16/26\n",
      "33600/33600 [==============================] - 7s 222us/step - loss: 0.0812 - acc: 0.9711 - val_loss: 0.0389 - val_acc: 0.9946\n",
      "Epoch 17/26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 8s 225us/step - loss: 0.0769 - acc: 0.9729 - val_loss: 0.0380 - val_acc: 0.9950\n",
      "Epoch 18/26\n",
      "30336/33600 [==========================>...] - ETA: 0s - loss: 0.0812 - acc: 0.9712"
     ]
    }
   ],
   "source": [
    "drop_outs = [0.45,0.4,0.5,0.55]\n",
    "ensemble_models = []\n",
    "ensemble_histories = []\n",
    "optimizer = 'adam'\n",
    "n_epochs = 40\n",
    "for drop_out in drop_outs:\n",
    "    print(\"starting\")\n",
    "    print(\"\");print(\"\")\n",
    "    model,history = CNN_model_3(optimizer,n_epochs, train_x, train_y, valid_x, valid_y,drop_out = drop_out)\n",
    "    #print(\"Batch Normalization: \", batch)\n",
    "    #print(\"Technique \", technique_dict[tech])\n",
    "    #print(\"drop_out \", drop_out)\n",
    "    ensemble_models.append(model)\n",
    "    ensemble_histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994642857256\n",
      "0.99297619059\n",
      "0.99416666678\n",
      "0.992857142971\n",
      "1 0.962291666667\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-3911dfbb4b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0marg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc_np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_acc_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "val_acc_list = []\n",
    "for history in ensemble_histories:\n",
    "    \n",
    "    print(history.history['val_acc'][-1])\n",
    "    val_acc_list.append(history.history['acc'])\n",
    "val_acc_np = np.array(val_acc_list)\n",
    "arg = np.argmax(val_acc_np,axis = 0)[-1]\n",
    "print (arg,val_acc_np[3,-1])\n",
    "models[3].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission(models[-2],test_x,'sub_adam_cnn_2c_1p_2c_1p_train9965_valid9962_batch_lrannealing.csv',['ImageId','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f07bccbaa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926517857143 0.842172619048\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "hist1 = ensemble_models[0].history['acc']\n",
    "hist2 = ensemble_models[1].history['acc']\n",
    "h = history_appending(ensemble_models[0],ensemble_models[1])\n",
    "hist3 = ensemble_models[2].history['acc']\n",
    "plt.title('linear model')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('training accuracy')\n",
    "plt.plot(hist1,label='rmsprop')\n",
    "plt.plot(hist2,label='SGD')\n",
    "plt.plot(h.history['acc'],label='adam')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(hist1[-1],hist3[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[[1 2]\n",
      " [2 0]\n",
      " [3 9]\n",
      " [4 7]] (28000, 2)\n",
      "   ImageId  Label\n",
      "0        1      2\n",
      "1        2      0\n",
      "2        3      9\n",
      "3        4      7\n",
      "4        5      3\n"
     ]
    }
   ],
   "source": [
    "predictions.shape\n",
    "result = predictions.argmax(axis =1 )\n",
    "result.shape\n",
    "result = result.reshape((28000,1))\n",
    "ids = (np.arange(28000)+1).reshape((28000,1))\n",
    "result = np.hstack((ids,result))\n",
    "print(result[:4,:],result.shape)\n",
    "m = to_kaggle_csv(result,['ImageId','Label'],'submission_adam_model_300ep_no_regularization.csv')\n",
    "print(m.head())\n",
    "#plt.imshow(test_x[3,:].reshape((28,28)))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary for the linear model\n",
    "####  The output of the linear model\n",
    "the submission got the 1653th place out of 1879, this is the top 87.9% with accuracy of 91.6%, which is really bad for mnist dataset, as it's the easiest task you'll ever workon and it's far below our human level accuracy.\n",
    "Note that training accuracy is around 94.4% and the submission accuracy is around 91.6%.\n",
    "We've been training for around 270 epochs\n",
    "<img src=\"1-linearmodel.PNG\" height=\"300\" width = \"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Linear models with regularization\n",
    "\n",
    "now the model is overfitting --> training acc is 94.4 and submission accuracy is 91.6.\n",
    "Next step is to add regularization and reduce overfitting --> thus we need a separate validation set to monitor overfitting.\n",
    "We'll find that a linear model isn't capable of overfitting and any further regularization will only reduce the training error, as the model doesn't have the capacity to overfit\n",
    "\n",
    "That's why it's a best practice in neural networks to overfit the data first to make sure that your model is good, then add regularization to generalize better, but you shouldn't spend anytime trying to prevent overfitting using a model with high bias (simple models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## validation set\n",
    "y = train_data[:,0]\n",
    "X = train_data[:,1:]\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(y.reshape((y.shape[0],1)))\n",
    "train_y = enc.transform(y.reshape((y.shape[0],1))).toarray()\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/150\n",
      "33600/33600 [==============================] - 4s 107us/step - loss: 1.5293 - acc: 0.8036 - val_loss: 1.3471 - val_acc: 0.8408\n",
      "Epoch 2/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3466 - acc: 0.8391 - val_loss: 1.3412 - val_acc: 0.8390\n",
      "Epoch 3/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3423 - acc: 0.8398 - val_loss: 1.3424 - val_acc: 0.8383\n",
      "Epoch 4/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3394 - acc: 0.8385 - val_loss: 1.3337 - val_acc: 0.8396\n",
      "Epoch 5/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3376 - acc: 0.8388 - val_loss: 1.3358 - val_acc: 0.8371\n",
      "Epoch 6/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3364 - acc: 0.8385 - val_loss: 1.3352 - val_acc: 0.8418\n",
      "Epoch 7/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3351 - acc: 0.8392 - val_loss: 1.3383 - val_acc: 0.8363\n",
      "Epoch 8/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3345 - acc: 0.8378 - val_loss: 1.3319 - val_acc: 0.8318\n",
      "Epoch 9/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3338 - acc: 0.8373 - val_loss: 1.3335 - val_acc: 0.8387\n",
      "Epoch 10/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 1.3335 - acc: 0.8389 - val_loss: 1.3334 - val_acc: 0.8426\n",
      "Epoch 11/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 1.3336 - acc: 0.8387 - val_loss: 1.3348 - val_acc: 0.8312\n",
      "Epoch 12/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3332 - acc: 0.8381 - val_loss: 1.3317 - val_acc: 0.8364\n",
      "Epoch 13/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3331 - acc: 0.8378 - val_loss: 1.3301 - val_acc: 0.8401\n",
      "Epoch 14/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3326 - acc: 0.8396 - val_loss: 1.3310 - val_acc: 0.8375\n",
      "Epoch 15/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3332 - acc: 0.8376 - val_loss: 1.3331 - val_acc: 0.8236\n",
      "Epoch 16/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 1.3331 - acc: 0.8377 - val_loss: 1.3336 - val_acc: 0.8405\n",
      "Epoch 17/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3333 - acc: 0.8373 - val_loss: 1.3363 - val_acc: 0.8429\n",
      "Epoch 18/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 1.3326 - acc: 0.8376 - val_loss: 1.3342 - val_acc: 0.8363\n",
      "Epoch 19/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3330 - acc: 0.8371 - val_loss: 1.3329 - val_acc: 0.8380\n",
      "Epoch 20/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3329 - acc: 0.8365 - val_loss: 1.3346 - val_acc: 0.8446\n",
      "Epoch 21/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3335 - acc: 0.8365 - val_loss: 1.3319 - val_acc: 0.8402\n",
      "Epoch 22/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3331 - acc: 0.8393 - val_loss: 1.3332 - val_acc: 0.8368\n",
      "Epoch 23/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3333 - acc: 0.8374 - val_loss: 1.3349 - val_acc: 0.8310\n",
      "Epoch 24/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3334 - acc: 0.8391 - val_loss: 1.3335 - val_acc: 0.8408\n",
      "Epoch 25/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3326 - acc: 0.8375 - val_loss: 1.3306 - val_acc: 0.8288\n",
      "Epoch 26/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 1.3337 - acc: 0.8358 - val_loss: 1.3348 - val_acc: 0.8336\n",
      "Epoch 27/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3331 - acc: 0.8373 - val_loss: 1.3355 - val_acc: 0.8352\n",
      "Epoch 28/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 1.3330 - acc: 0.8374 - val_loss: 1.3310 - val_acc: 0.8300\n",
      "Epoch 29/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 1.3329 - acc: 0.8382 - val_loss: 1.3380 - val_acc: 0.8392\n",
      "Epoch 30/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 1.3330 - acc: 0.8375 - val_loss: 1.3390 - val_acc: 0.8349\n",
      "Epoch 31/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 1.3331 - acc: 0.8378 - val_loss: 1.3340 - val_acc: 0.8410\n",
      "Epoch 32/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 1.3334 - acc: 0.8386 - val_loss: 1.3324 - val_acc: 0.8521\n",
      "Epoch 33/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3331 - acc: 0.8385 - val_loss: 1.3365 - val_acc: 0.8368\n",
      "Epoch 34/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 1.3332 - acc: 0.8379 - val_loss: 1.3316 - val_acc: 0.8351\n",
      "Epoch 35/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3326 - acc: 0.8378 - val_loss: 1.3354 - val_acc: 0.8208\n",
      "Epoch 36/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3330 - acc: 0.8372 - val_loss: 1.3385 - val_acc: 0.8375\n",
      "Epoch 37/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3333 - acc: 0.8387 - val_loss: 1.3318 - val_acc: 0.8375\n",
      "Epoch 38/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3329 - acc: 0.8378 - val_loss: 1.3352 - val_acc: 0.8415\n",
      "Epoch 39/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3333 - acc: 0.8378 - val_loss: 1.3345 - val_acc: 0.8460\n",
      "Epoch 40/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3328 - acc: 0.8378 - val_loss: 1.3321 - val_acc: 0.8433\n",
      "Epoch 41/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 1.3329 - acc: 0.8378 - val_loss: 1.3354 - val_acc: 0.8287\n",
      "Epoch 42/150\n",
      "33600/33600 [==============================] - 3s 94us/step - loss: 1.3327 - acc: 0.8386 - val_loss: 1.3333 - val_acc: 0.8324\n",
      "Epoch 43/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3328 - acc: 0.8379 - val_loss: 1.3348 - val_acc: 0.8408\n",
      "Epoch 44/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3331 - acc: 0.8373 - val_loss: 1.3368 - val_acc: 0.8280\n",
      "Epoch 45/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3329 - acc: 0.8369 - val_loss: 1.3321 - val_acc: 0.8439\n",
      "Epoch 46/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3328 - acc: 0.8376 - val_loss: 1.3330 - val_acc: 0.8381\n",
      "Epoch 47/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 1.3330 - acc: 0.8368 - val_loss: 1.3315 - val_acc: 0.8471\n",
      "Epoch 48/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3328 - acc: 0.8394 - val_loss: 1.3362 - val_acc: 0.8454\n",
      "Epoch 49/150\n",
      "33600/33600 [==============================] - 3s 97us/step - loss: 1.3333 - acc: 0.8376 - val_loss: 1.3350 - val_acc: 0.8275\n",
      "Epoch 50/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 1.3336 - acc: 0.8372 - val_loss: 1.3346 - val_acc: 0.8449\n",
      "Epoch 51/150\n",
      "33600/33600 [==============================] - 3s 93us/step - loss: 1.3333 - acc: 0.8373 - val_loss: 1.3333 - val_acc: 0.8383\n",
      "Epoch 52/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 1.3334 - acc: 0.8382 - val_loss: 1.3330 - val_acc: 0.8375\n",
      "Epoch 53/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3334 - acc: 0.8373 - val_loss: 1.3322 - val_acc: 0.8373\n",
      "Epoch 54/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3334 - acc: 0.8363 - val_loss: 1.3328 - val_acc: 0.8480\n",
      "Epoch 55/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3330 - acc: 0.8378 - val_loss: 1.3291 - val_acc: 0.8446\n",
      "Epoch 56/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3335 - acc: 0.8367 - val_loss: 1.3336 - val_acc: 0.8463\n",
      "Epoch 57/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3329 - acc: 0.8388 - val_loss: 1.3295 - val_acc: 0.8437\n",
      "Epoch 58/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3324 - acc: 0.8370 - val_loss: 1.3321 - val_acc: 0.8392\n",
      "Epoch 59/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3329 - acc: 0.8377 - val_loss: 1.3340 - val_acc: 0.8404\n",
      "Epoch 60/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 1.3330 - acc: 0.8370 - val_loss: 1.3336 - val_acc: 0.8425\n",
      "Epoch 61/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3332 - acc: 0.8375 - val_loss: 1.3311 - val_acc: 0.8394\n",
      "Epoch 62/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3334 - acc: 0.8376 - val_loss: 1.3312 - val_acc: 0.8361\n",
      "Epoch 63/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3331 - acc: 0.8367 - val_loss: 1.3313 - val_acc: 0.8363\n",
      "Epoch 64/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3334 - acc: 0.8379 - val_loss: 1.3392 - val_acc: 0.8340\n",
      "Epoch 65/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3332 - acc: 0.8382 - val_loss: 1.3293 - val_acc: 0.8327\n",
      "Epoch 66/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3335 - acc: 0.8374 - val_loss: 1.3379 - val_acc: 0.8373\n",
      "Epoch 67/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3334 - acc: 0.8373 - val_loss: 1.3341 - val_acc: 0.8354\n",
      "Epoch 68/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3332 - acc: 0.8372 - val_loss: 1.3378 - val_acc: 0.8333\n",
      "Epoch 69/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3333 - acc: 0.8375 - val_loss: 1.3329 - val_acc: 0.8376\n",
      "Epoch 70/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3335 - acc: 0.8383 - val_loss: 1.3346 - val_acc: 0.8383\n",
      "Epoch 71/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3328 - acc: 0.8375 - val_loss: 1.3335 - val_acc: 0.8320\n",
      "Epoch 72/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3330 - acc: 0.8382 - val_loss: 1.3401 - val_acc: 0.8369\n",
      "Epoch 73/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3330 - acc: 0.8375 - val_loss: 1.3377 - val_acc: 0.8338\n",
      "Epoch 74/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 1.3329 - acc: 0.8381 - val_loss: 1.3364 - val_acc: 0.8331\n",
      "Epoch 75/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3327 - acc: 0.8391 - val_loss: 1.3323 - val_acc: 0.8405\n",
      "Epoch 76/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3331 - acc: 0.8378 - val_loss: 1.3297 - val_acc: 0.8371\n",
      "Epoch 77/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3331 - acc: 0.8372 - val_loss: 1.3302 - val_acc: 0.8385\n",
      "Epoch 78/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3329 - acc: 0.8367 - val_loss: 1.3308 - val_acc: 0.8412\n",
      "Epoch 79/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3327 - acc: 0.8383 - val_loss: 1.3356 - val_acc: 0.8313\n",
      "Epoch 80/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3332 - acc: 0.8372 - val_loss: 1.3306 - val_acc: 0.8444\n",
      "Epoch 81/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3333 - acc: 0.8355 - val_loss: 1.3325 - val_acc: 0.8307\n",
      "Epoch 82/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 1.3334 - acc: 0.8385 - val_loss: 1.3303 - val_acc: 0.8368\n",
      "Epoch 83/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 1.3328 - acc: 0.8377 - val_loss: 1.3346 - val_acc: 0.8417\n",
      "Epoch 84/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3332 - acc: 0.8371 - val_loss: 1.3364 - val_acc: 0.8392\n",
      "Epoch 85/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3332 - acc: 0.8383 - val_loss: 1.3324 - val_acc: 0.8346\n",
      "Epoch 86/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3334 - acc: 0.8379 - val_loss: 1.3308 - val_acc: 0.8327\n",
      "Epoch 87/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3334 - acc: 0.8363 - val_loss: 1.3328 - val_acc: 0.8370\n",
      "Epoch 88/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3334 - acc: 0.8386 - val_loss: 1.3313 - val_acc: 0.8375\n",
      "Epoch 89/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3332 - acc: 0.8375 - val_loss: 1.3326 - val_acc: 0.8380\n",
      "Epoch 90/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3324 - acc: 0.8377 - val_loss: 1.3317 - val_acc: 0.8411\n",
      "Epoch 91/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 1.3335 - acc: 0.8383 - val_loss: 1.3329 - val_acc: 0.8462\n",
      "Epoch 92/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3333 - acc: 0.8375 - val_loss: 1.3296 - val_acc: 0.8354\n",
      "Epoch 93/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3331 - acc: 0.8371 - val_loss: 1.3336 - val_acc: 0.8412\n",
      "Epoch 94/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3324 - acc: 0.8384 - val_loss: 1.3310 - val_acc: 0.8412\n",
      "Epoch 95/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3330 - acc: 0.8368 - val_loss: 1.3355 - val_acc: 0.8452\n",
      "Epoch 96/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3333 - acc: 0.8381 - val_loss: 1.3317 - val_acc: 0.8363\n",
      "Epoch 97/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3328 - acc: 0.8372 - val_loss: 1.3364 - val_acc: 0.8308\n",
      "Epoch 98/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3330 - acc: 0.8370 - val_loss: 1.3335 - val_acc: 0.8427\n",
      "Epoch 99/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3333 - acc: 0.8381 - val_loss: 1.3324 - val_acc: 0.8269\n",
      "Epoch 100/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3331 - acc: 0.8382 - val_loss: 1.3310 - val_acc: 0.8355\n",
      "Epoch 101/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3331 - acc: 0.8385 - val_loss: 1.3324 - val_acc: 0.8280\n",
      "Epoch 102/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3330 - acc: 0.8374 - val_loss: 1.3343 - val_acc: 0.8375\n",
      "Epoch 103/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3331 - acc: 0.8380 - val_loss: 1.3314 - val_acc: 0.8270\n",
      "Epoch 104/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 1.3336 - acc: 0.8372 - val_loss: 1.3334 - val_acc: 0.8321\n",
      "Epoch 105/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3333 - acc: 0.8371 - val_loss: 1.3308 - val_acc: 0.8418\n",
      "Epoch 106/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3331 - acc: 0.8370 - val_loss: 1.3311 - val_acc: 0.8319\n",
      "Epoch 107/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3332 - acc: 0.8371 - val_loss: 1.3308 - val_acc: 0.8412\n",
      "Epoch 108/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3328 - acc: 0.8375 - val_loss: 1.3371 - val_acc: 0.8381\n",
      "Epoch 109/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3335 - acc: 0.8354 - val_loss: 1.3302 - val_acc: 0.8368\n",
      "Epoch 110/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3328 - acc: 0.8378 - val_loss: 1.3309 - val_acc: 0.8343\n",
      "Epoch 111/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3330 - acc: 0.8371 - val_loss: 1.3379 - val_acc: 0.8430\n",
      "Epoch 112/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3332 - acc: 0.8376 - val_loss: 1.3344 - val_acc: 0.8486\n",
      "Epoch 113/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3328 - acc: 0.8390 - val_loss: 1.3334 - val_acc: 0.8369\n",
      "Epoch 114/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3336 - acc: 0.8376 - val_loss: 1.3326 - val_acc: 0.8333\n",
      "Epoch 115/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3330 - acc: 0.8376 - val_loss: 1.3364 - val_acc: 0.8420\n",
      "Epoch 116/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3335 - acc: 0.8370 - val_loss: 1.3308 - val_acc: 0.8385\n",
      "Epoch 117/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3330 - acc: 0.8365 - val_loss: 1.3414 - val_acc: 0.8349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3326 - acc: 0.8363 - val_loss: 1.3314 - val_acc: 0.8404\n",
      "Epoch 119/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 1.3337 - acc: 0.8361 - val_loss: 1.3327 - val_acc: 0.8371\n",
      "Epoch 120/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 1.3330 - acc: 0.8361 - val_loss: 1.3306 - val_acc: 0.8437\n",
      "Epoch 121/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 1.3332 - acc: 0.8365 - val_loss: 1.3299 - val_acc: 0.8361\n",
      "Epoch 122/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3330 - acc: 0.8366 - val_loss: 1.3323 - val_acc: 0.8321\n",
      "Epoch 123/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3333 - acc: 0.8381 - val_loss: 1.3316 - val_acc: 0.8438\n",
      "Epoch 124/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3329 - acc: 0.8381 - val_loss: 1.3328 - val_acc: 0.8383\n",
      "Epoch 125/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3331 - acc: 0.8377 - val_loss: 1.3339 - val_acc: 0.8402\n",
      "Epoch 126/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3328 - acc: 0.8378 - val_loss: 1.3426 - val_acc: 0.8210\n",
      "Epoch 127/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3333 - acc: 0.8394 - val_loss: 1.3309 - val_acc: 0.8424\n",
      "Epoch 128/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3334 - acc: 0.8361 - val_loss: 1.3306 - val_acc: 0.8438\n",
      "Epoch 129/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3332 - acc: 0.8372 - val_loss: 1.3318 - val_acc: 0.8376\n",
      "Epoch 130/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3333 - acc: 0.8382 - val_loss: 1.3339 - val_acc: 0.8400\n",
      "Epoch 131/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3334 - acc: 0.8367 - val_loss: 1.3291 - val_acc: 0.8396\n",
      "Epoch 132/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3328 - acc: 0.8371 - val_loss: 1.3315 - val_acc: 0.8375\n",
      "Epoch 133/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3334 - acc: 0.8380 - val_loss: 1.3360 - val_acc: 0.8317\n",
      "Epoch 134/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3331 - acc: 0.8392 - val_loss: 1.3307 - val_acc: 0.8381\n",
      "Epoch 135/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 1.3335 - acc: 0.8381 - val_loss: 1.3289 - val_acc: 0.8432\n",
      "Epoch 136/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3329 - acc: 0.8364 - val_loss: 1.3338 - val_acc: 0.8412\n",
      "Epoch 137/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3333 - acc: 0.8385 - val_loss: 1.3330 - val_acc: 0.8464\n",
      "Epoch 138/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3330 - acc: 0.8355 - val_loss: 1.3359 - val_acc: 0.8377\n",
      "Epoch 139/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3330 - acc: 0.8377 - val_loss: 1.3311 - val_acc: 0.8440\n",
      "Epoch 140/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3328 - acc: 0.8379 - val_loss: 1.3328 - val_acc: 0.8358\n",
      "Epoch 141/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3338 - acc: 0.8382 - val_loss: 1.3309 - val_acc: 0.8439\n",
      "Epoch 142/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3330 - acc: 0.8374 - val_loss: 1.3373 - val_acc: 0.8444\n",
      "Epoch 143/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3330 - acc: 0.8379 - val_loss: 1.3353 - val_acc: 0.8354\n",
      "Epoch 144/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3329 - acc: 0.8378 - val_loss: 1.3351 - val_acc: 0.8388\n",
      "Epoch 145/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 1.3334 - acc: 0.8360 - val_loss: 1.3339 - val_acc: 0.8425\n",
      "Epoch 146/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3331 - acc: 0.8365 - val_loss: 1.3307 - val_acc: 0.8343\n",
      "Epoch 147/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 1.3329 - acc: 0.8373 - val_loss: 1.3330 - val_acc: 0.8258\n",
      "Epoch 148/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3331 - acc: 0.8377 - val_loss: 1.3354 - val_acc: 0.8345\n",
      "Epoch 149/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 1.3330 - acc: 0.8378 - val_loss: 1.3390 - val_acc: 0.8233\n",
      "Epoch 150/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 1.3332 - acc: 0.8377 - val_loss: 1.3332 - val_acc: 0.8370\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/150\n",
      "33600/33600 [==============================] - 3s 103us/step - loss: 0.8762 - acc: 0.8389 - val_loss: 0.6969 - val_acc: 0.8806\n",
      "Epoch 2/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.6845 - acc: 0.8878 - val_loss: 0.6831 - val_acc: 0.8839\n",
      "Epoch 3/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6771 - acc: 0.8889 - val_loss: 0.6813 - val_acc: 0.8886\n",
      "Epoch 4/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6744 - acc: 0.8907 - val_loss: 0.6791 - val_acc: 0.8887\n",
      "Epoch 5/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6731 - acc: 0.8898 - val_loss: 0.6768 - val_acc: 0.8876\n",
      "Epoch 6/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6715 - acc: 0.8906 - val_loss: 0.6783 - val_acc: 0.8860\n",
      "Epoch 7/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6705 - acc: 0.8911 - val_loss: 0.6800 - val_acc: 0.8840\n",
      "Epoch 8/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6698 - acc: 0.8908 - val_loss: 0.6730 - val_acc: 0.8849\n",
      "Epoch 9/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6689 - acc: 0.8914 - val_loss: 0.6715 - val_acc: 0.8867\n",
      "Epoch 10/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6684 - acc: 0.8923 - val_loss: 0.6754 - val_acc: 0.8880\n",
      "Epoch 11/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6680 - acc: 0.8910 - val_loss: 0.6744 - val_acc: 0.8914\n",
      "Epoch 12/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6677 - acc: 0.8909 - val_loss: 0.6730 - val_acc: 0.8896\n",
      "Epoch 13/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6670 - acc: 0.8914 - val_loss: 0.6739 - val_acc: 0.8836\n",
      "Epoch 14/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6672 - acc: 0.8905 - val_loss: 0.6711 - val_acc: 0.8882\n",
      "Epoch 15/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.6669 - acc: 0.8907 - val_loss: 0.6730 - val_acc: 0.8869\n",
      "Epoch 16/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6665 - acc: 0.8909 - val_loss: 0.6715 - val_acc: 0.8894\n",
      "Epoch 17/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6663 - acc: 0.8915 - val_loss: 0.6702 - val_acc: 0.8881\n",
      "Epoch 18/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6660 - acc: 0.8923 - val_loss: 0.6730 - val_acc: 0.8832\n",
      "Epoch 19/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.6663 - acc: 0.8906 - val_loss: 0.6716 - val_acc: 0.8829\n",
      "Epoch 20/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.6662 - acc: 0.8906 - val_loss: 0.6722 - val_acc: 0.8851\n",
      "Epoch 21/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6656 - acc: 0.8916 - val_loss: 0.6712 - val_acc: 0.8876\n",
      "Epoch 22/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6660 - acc: 0.8907 - val_loss: 0.6693 - val_acc: 0.8858\n",
      "Epoch 23/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6660 - acc: 0.8909 - val_loss: 0.6734 - val_acc: 0.8852\n",
      "Epoch 24/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6659 - acc: 0.8910 - val_loss: 0.6702 - val_acc: 0.8860\n",
      "Epoch 25/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6657 - acc: 0.8912 - val_loss: 0.6712 - val_acc: 0.8855\n",
      "Epoch 26/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6659 - acc: 0.8910 - val_loss: 0.6744 - val_acc: 0.8839\n",
      "Epoch 27/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6654 - acc: 0.8910 - val_loss: 0.6684 - val_acc: 0.8868\n",
      "Epoch 28/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6653 - acc: 0.8910 - val_loss: 0.6718 - val_acc: 0.8849\n",
      "Epoch 29/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6659 - acc: 0.8905 - val_loss: 0.6710 - val_acc: 0.8887\n",
      "Epoch 30/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6654 - acc: 0.8899 - val_loss: 0.6731 - val_acc: 0.8858\n",
      "Epoch 31/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6651 - acc: 0.8907 - val_loss: 0.6696 - val_acc: 0.8879\n",
      "Epoch 32/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6653 - acc: 0.8909 - val_loss: 0.6734 - val_acc: 0.8830\n",
      "Epoch 33/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6650 - acc: 0.8904 - val_loss: 0.6741 - val_acc: 0.8814\n",
      "Epoch 34/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6651 - acc: 0.8910 - val_loss: 0.6758 - val_acc: 0.8815\n",
      "Epoch 35/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6654 - acc: 0.8906 - val_loss: 0.6744 - val_acc: 0.8820\n",
      "Epoch 36/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6653 - acc: 0.8909 - val_loss: 0.6704 - val_acc: 0.8844\n",
      "Epoch 37/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6651 - acc: 0.8899 - val_loss: 0.6711 - val_acc: 0.8840\n",
      "Epoch 38/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6656 - acc: 0.8900 - val_loss: 0.6713 - val_acc: 0.8876\n",
      "Epoch 39/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6651 - acc: 0.8899 - val_loss: 0.6739 - val_acc: 0.8858\n",
      "Epoch 40/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6654 - acc: 0.8906 - val_loss: 0.6692 - val_acc: 0.8862\n",
      "Epoch 41/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6660 - acc: 0.8908 - val_loss: 0.6730 - val_acc: 0.8840\n",
      "Epoch 42/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6656 - acc: 0.8903 - val_loss: 0.6734 - val_acc: 0.8863\n",
      "Epoch 43/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6655 - acc: 0.8899 - val_loss: 0.6701 - val_acc: 0.8865\n",
      "Epoch 44/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6659 - acc: 0.8902 - val_loss: 0.6704 - val_acc: 0.8857\n",
      "Epoch 45/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.6655 - acc: 0.8912 - val_loss: 0.6698 - val_acc: 0.8846\n",
      "Epoch 46/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6657 - acc: 0.8899 - val_loss: 0.6688 - val_acc: 0.8870\n",
      "Epoch 47/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6655 - acc: 0.8896 - val_loss: 0.6711 - val_acc: 0.8870\n",
      "Epoch 48/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6652 - acc: 0.8900 - val_loss: 0.6706 - val_acc: 0.8876\n",
      "Epoch 49/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6656 - acc: 0.8901 - val_loss: 0.6686 - val_acc: 0.8848\n",
      "Epoch 50/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6652 - acc: 0.8907 - val_loss: 0.6690 - val_acc: 0.8861\n",
      "Epoch 51/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6656 - acc: 0.8909 - val_loss: 0.6719 - val_acc: 0.8821\n",
      "Epoch 52/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6657 - acc: 0.8905 - val_loss: 0.6726 - val_acc: 0.8824\n",
      "Epoch 53/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.6653 - acc: 0.8898 - val_loss: 0.6688 - val_acc: 0.8856\n",
      "Epoch 54/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6650 - acc: 0.8903 - val_loss: 0.6723 - val_acc: 0.8857\n",
      "Epoch 55/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6650 - acc: 0.8910 - val_loss: 0.6705 - val_acc: 0.8863\n",
      "Epoch 56/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6655 - acc: 0.8903 - val_loss: 0.6711 - val_acc: 0.8861\n",
      "Epoch 57/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6654 - acc: 0.8898 - val_loss: 0.6720 - val_acc: 0.8837\n",
      "Epoch 58/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6655 - acc: 0.8902 - val_loss: 0.6693 - val_acc: 0.8855\n",
      "Epoch 59/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6655 - acc: 0.8898 - val_loss: 0.6722 - val_acc: 0.8833\n",
      "Epoch 60/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6654 - acc: 0.8904 - val_loss: 0.6719 - val_acc: 0.8854\n",
      "Epoch 61/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6657 - acc: 0.8900 - val_loss: 0.6709 - val_acc: 0.8849\n",
      "Epoch 62/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6654 - acc: 0.8901 - val_loss: 0.6718 - val_acc: 0.8840\n",
      "Epoch 63/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6654 - acc: 0.8895 - val_loss: 0.6689 - val_acc: 0.8875\n",
      "Epoch 64/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6649 - acc: 0.8894 - val_loss: 0.6727 - val_acc: 0.8865\n",
      "Epoch 65/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6652 - acc: 0.8899 - val_loss: 0.6735 - val_acc: 0.8855\n",
      "Epoch 66/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6659 - acc: 0.8897 - val_loss: 0.6701 - val_acc: 0.8882\n",
      "Epoch 67/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6656 - acc: 0.8916 - val_loss: 0.6701 - val_acc: 0.8856\n",
      "Epoch 68/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6652 - acc: 0.8896 - val_loss: 0.6743 - val_acc: 0.8792\n",
      "Epoch 69/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6656 - acc: 0.8897 - val_loss: 0.6747 - val_acc: 0.8845\n",
      "Epoch 70/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6654 - acc: 0.8905 - val_loss: 0.6720 - val_acc: 0.8840\n",
      "Epoch 71/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.6653 - acc: 0.8893 - val_loss: 0.6753 - val_acc: 0.8843\n",
      "Epoch 72/150\n",
      "33600/33600 [==============================] - 3s 93us/step - loss: 0.6658 - acc: 0.8908 - val_loss: 0.6722 - val_acc: 0.8873\n",
      "Epoch 73/150\n",
      "33600/33600 [==============================] - 3s 94us/step - loss: 0.6659 - acc: 0.8906 - val_loss: 0.6711 - val_acc: 0.8850\n",
      "Epoch 74/150\n",
      "33600/33600 [==============================] - 3s 96us/step - loss: 0.6657 - acc: 0.8886 - val_loss: 0.6694 - val_acc: 0.8876\n",
      "Epoch 75/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.6656 - acc: 0.8893 - val_loss: 0.6759 - val_acc: 0.8856\n",
      "Epoch 76/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6657 - acc: 0.8892 - val_loss: 0.6713 - val_acc: 0.8863\n",
      "Epoch 77/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6653 - acc: 0.8905 - val_loss: 0.6708 - val_acc: 0.8857\n",
      "Epoch 78/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.6657 - acc: 0.8897 - val_loss: 0.6754 - val_acc: 0.8838\n",
      "Epoch 79/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6655 - acc: 0.8895 - val_loss: 0.6718 - val_acc: 0.8865\n",
      "Epoch 80/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6652 - acc: 0.8904 - val_loss: 0.6700 - val_acc: 0.8860\n",
      "Epoch 81/150\n",
      "33600/33600 [==============================] - 3s 93us/step - loss: 0.6656 - acc: 0.8891 - val_loss: 0.6694 - val_acc: 0.8869\n",
      "Epoch 82/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6653 - acc: 0.8908 - val_loss: 0.6693 - val_acc: 0.8854\n",
      "Epoch 83/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6651 - acc: 0.8915 - val_loss: 0.6709 - val_acc: 0.8882\n",
      "Epoch 84/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6656 - acc: 0.8891 - val_loss: 0.6698 - val_acc: 0.8867\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6653 - acc: 0.8905 - val_loss: 0.6684 - val_acc: 0.8850\n",
      "Epoch 86/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6654 - acc: 0.8897 - val_loss: 0.6715 - val_acc: 0.8860\n",
      "Epoch 87/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6656 - acc: 0.8898 - val_loss: 0.6701 - val_acc: 0.8871\n",
      "Epoch 88/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6652 - acc: 0.8899 - val_loss: 0.6712 - val_acc: 0.8862\n",
      "Epoch 89/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6653 - acc: 0.8900 - val_loss: 0.6729 - val_acc: 0.8867\n",
      "Epoch 90/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6653 - acc: 0.8908 - val_loss: 0.6699 - val_acc: 0.8874\n",
      "Epoch 91/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.6655 - acc: 0.8901 - val_loss: 0.6718 - val_acc: 0.8862\n",
      "Epoch 92/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6654 - acc: 0.8904 - val_loss: 0.6735 - val_acc: 0.8830\n",
      "Epoch 93/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6655 - acc: 0.8899 - val_loss: 0.6687 - val_acc: 0.8865\n",
      "Epoch 94/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6652 - acc: 0.8903 - val_loss: 0.6693 - val_acc: 0.8849\n",
      "Epoch 95/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6652 - acc: 0.8895 - val_loss: 0.6701 - val_acc: 0.8860\n",
      "Epoch 96/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6650 - acc: 0.8902 - val_loss: 0.6707 - val_acc: 0.8849\n",
      "Epoch 97/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6656 - acc: 0.8902 - val_loss: 0.6694 - val_acc: 0.8855\n",
      "Epoch 98/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6657 - acc: 0.8909 - val_loss: 0.6702 - val_acc: 0.8863\n",
      "Epoch 99/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6656 - acc: 0.8897 - val_loss: 0.6726 - val_acc: 0.8836\n",
      "Epoch 100/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6655 - acc: 0.8899 - val_loss: 0.6728 - val_acc: 0.8860\n",
      "Epoch 101/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6650 - acc: 0.8899 - val_loss: 0.6712 - val_acc: 0.8843\n",
      "Epoch 102/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6652 - acc: 0.8898 - val_loss: 0.6723 - val_acc: 0.8855\n",
      "Epoch 103/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6653 - acc: 0.8905 - val_loss: 0.6682 - val_acc: 0.8875\n",
      "Epoch 104/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6653 - acc: 0.8905 - val_loss: 0.6729 - val_acc: 0.8862\n",
      "Epoch 105/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6656 - acc: 0.8901 - val_loss: 0.6726 - val_acc: 0.8858\n",
      "Epoch 106/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6653 - acc: 0.8901 - val_loss: 0.6704 - val_acc: 0.8874\n",
      "Epoch 107/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6655 - acc: 0.8907 - val_loss: 0.6695 - val_acc: 0.8863\n",
      "Epoch 108/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6655 - acc: 0.8898 - val_loss: 0.6708 - val_acc: 0.8850\n",
      "Epoch 109/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6656 - acc: 0.8896 - val_loss: 0.6724 - val_acc: 0.8876\n",
      "Epoch 110/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6654 - acc: 0.8900 - val_loss: 0.6712 - val_acc: 0.8863\n",
      "Epoch 111/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6653 - acc: 0.8901 - val_loss: 0.6786 - val_acc: 0.8814\n",
      "Epoch 112/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6658 - acc: 0.8912 - val_loss: 0.6698 - val_acc: 0.8881\n",
      "Epoch 113/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6656 - acc: 0.8902 - val_loss: 0.6711 - val_acc: 0.8832\n",
      "Epoch 114/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6656 - acc: 0.8905 - val_loss: 0.6699 - val_acc: 0.8875\n",
      "Epoch 115/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.6653 - acc: 0.8901 - val_loss: 0.6698 - val_acc: 0.8840\n",
      "Epoch 116/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6654 - acc: 0.8897 - val_loss: 0.6749 - val_acc: 0.8831\n",
      "Epoch 117/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6654 - acc: 0.8909 - val_loss: 0.6710 - val_acc: 0.8854\n",
      "Epoch 118/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6653 - acc: 0.8901 - val_loss: 0.6703 - val_acc: 0.8823\n",
      "Epoch 119/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6651 - acc: 0.8905 - val_loss: 0.6717 - val_acc: 0.8851\n",
      "Epoch 120/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6653 - acc: 0.8896 - val_loss: 0.6694 - val_acc: 0.8875\n",
      "Epoch 121/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6653 - acc: 0.8911 - val_loss: 0.6705 - val_acc: 0.8875\n",
      "Epoch 122/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6650 - acc: 0.8902 - val_loss: 0.6724 - val_acc: 0.8867\n",
      "Epoch 123/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6652 - acc: 0.8894 - val_loss: 0.6717 - val_acc: 0.8856\n",
      "Epoch 124/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6656 - acc: 0.8892 - val_loss: 0.6713 - val_acc: 0.8873\n",
      "Epoch 125/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.6653 - acc: 0.8902 - val_loss: 0.6714 - val_acc: 0.8849\n",
      "Epoch 126/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6651 - acc: 0.8911 - val_loss: 0.6708 - val_acc: 0.8843\n",
      "Epoch 127/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6656 - acc: 0.8906 - val_loss: 0.6695 - val_acc: 0.8869\n",
      "Epoch 128/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6649 - acc: 0.8905 - val_loss: 0.6713 - val_acc: 0.8845\n",
      "Epoch 129/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6654 - acc: 0.8903 - val_loss: 0.6702 - val_acc: 0.8875\n",
      "Epoch 130/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6655 - acc: 0.8905 - val_loss: 0.6705 - val_acc: 0.8862\n",
      "Epoch 131/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6654 - acc: 0.8904 - val_loss: 0.6712 - val_acc: 0.8857\n",
      "Epoch 132/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6649 - acc: 0.8895 - val_loss: 0.6716 - val_acc: 0.8850\n",
      "Epoch 133/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6655 - acc: 0.8902 - val_loss: 0.6698 - val_acc: 0.8873\n",
      "Epoch 134/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6652 - acc: 0.8904 - val_loss: 0.6732 - val_acc: 0.8855\n",
      "Epoch 135/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.6652 - acc: 0.8904 - val_loss: 0.6693 - val_acc: 0.8844\n",
      "Epoch 136/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.6654 - acc: 0.8901 - val_loss: 0.6757 - val_acc: 0.8836\n",
      "Epoch 137/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.6657 - acc: 0.8897 - val_loss: 0.6710 - val_acc: 0.8869\n",
      "Epoch 138/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.6654 - acc: 0.8899 - val_loss: 0.6709 - val_acc: 0.8862\n",
      "Epoch 139/150\n",
      "33600/33600 [==============================] - 3s 96us/step - loss: 0.6657 - acc: 0.8888 - val_loss: 0.6699 - val_acc: 0.8868\n",
      "Epoch 140/150\n",
      "33600/33600 [==============================] - 3s 93us/step - loss: 0.6654 - acc: 0.8908 - val_loss: 0.6688 - val_acc: 0.8860\n",
      "Epoch 141/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.6657 - acc: 0.8904 - val_loss: 0.6731 - val_acc: 0.8838\n",
      "Epoch 142/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6653 - acc: 0.8902 - val_loss: 0.6697 - val_acc: 0.8870\n",
      "Epoch 143/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6655 - acc: 0.8895 - val_loss: 0.6696 - val_acc: 0.8876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6656 - acc: 0.8895 - val_loss: 0.6725 - val_acc: 0.8817\n",
      "Epoch 145/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.6657 - acc: 0.8900 - val_loss: 0.6707 - val_acc: 0.8831\n",
      "Epoch 146/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.6653 - acc: 0.8900 - val_loss: 0.6692 - val_acc: 0.8885\n",
      "Epoch 147/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.6654 - acc: 0.8901 - val_loss: 0.6709 - val_acc: 0.8880\n",
      "Epoch 148/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.6648 - acc: 0.8896 - val_loss: 0.6720 - val_acc: 0.8875\n",
      "Epoch 149/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.6652 - acc: 0.8896 - val_loss: 0.6730 - val_acc: 0.8875\n",
      "Epoch 150/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.6658 - acc: 0.8893 - val_loss: 0.6698 - val_acc: 0.8875\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/150\n",
      "33600/33600 [==============================] - 4s 121us/step - loss: 0.6631 - acc: 0.8437 - val_loss: 0.4433 - val_acc: 0.8950\n",
      "Epoch 2/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.4195 - acc: 0.9021 - val_loss: 0.4164 - val_acc: 0.9014\n",
      "Epoch 3/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.4026 - acc: 0.9082 - val_loss: 0.4096 - val_acc: 0.9073\n",
      "Epoch 4/150\n",
      "33600/33600 [==============================] - 3s 95us/step - loss: 0.3964 - acc: 0.9114 - val_loss: 0.4088 - val_acc: 0.9062\n",
      "Epoch 5/150\n",
      "33600/33600 [==============================] - 3s 95us/step - loss: 0.3937 - acc: 0.9128 - val_loss: 0.4083 - val_acc: 0.9080\n",
      "Epoch 6/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 0.3914 - acc: 0.9134 - val_loss: 0.4054 - val_acc: 0.9092\n",
      "Epoch 7/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3909 - acc: 0.9147 - val_loss: 0.4062 - val_acc: 0.9090\n",
      "Epoch 8/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3904 - acc: 0.9148 - val_loss: 0.4048 - val_acc: 0.9106\n",
      "Epoch 9/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3896 - acc: 0.9156 - val_loss: 0.4062 - val_acc: 0.9095\n",
      "Epoch 10/150\n",
      "33600/33600 [==============================] - 3s 95us/step - loss: 0.3897 - acc: 0.9162 - val_loss: 0.4052 - val_acc: 0.9092\n",
      "Epoch 11/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3890 - acc: 0.9144 - val_loss: 0.4062 - val_acc: 0.9106\n",
      "Epoch 12/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3891 - acc: 0.9153 - val_loss: 0.4054 - val_acc: 0.9117\n",
      "Epoch 13/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3883 - acc: 0.9158 - val_loss: 0.4065 - val_acc: 0.9105\n",
      "Epoch 14/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3886 - acc: 0.9160 - val_loss: 0.4057 - val_acc: 0.9110\n",
      "Epoch 15/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3888 - acc: 0.9165 - val_loss: 0.4074 - val_acc: 0.9104\n",
      "Epoch 16/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3883 - acc: 0.9164 - val_loss: 0.4042 - val_acc: 0.9102\n",
      "Epoch 17/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3882 - acc: 0.9155 - val_loss: 0.4055 - val_acc: 0.9107\n",
      "Epoch 18/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3879 - acc: 0.9166 - val_loss: 0.4055 - val_acc: 0.9093\n",
      "Epoch 19/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3879 - acc: 0.9164 - val_loss: 0.4042 - val_acc: 0.9113\n",
      "Epoch 20/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3879 - acc: 0.9152 - val_loss: 0.4064 - val_acc: 0.9101\n",
      "Epoch 21/150\n",
      "33600/33600 [==============================] - 3s 99us/step - loss: 0.3877 - acc: 0.9168 - val_loss: 0.4062 - val_acc: 0.9101\n",
      "Epoch 22/150\n",
      "33600/33600 [==============================] - 3s 94us/step - loss: 0.3880 - acc: 0.9161 - val_loss: 0.4051 - val_acc: 0.9108\n",
      "Epoch 23/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3875 - acc: 0.9170 - val_loss: 0.4064 - val_acc: 0.9095\n",
      "Epoch 24/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3876 - acc: 0.9160 - val_loss: 0.4043 - val_acc: 0.9127\n",
      "Epoch 25/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3879 - acc: 0.9166 - val_loss: 0.4059 - val_acc: 0.9107\n",
      "Epoch 26/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3876 - acc: 0.9158 - val_loss: 0.4072 - val_acc: 0.9092\n",
      "Epoch 27/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3875 - acc: 0.9163 - val_loss: 0.4065 - val_acc: 0.9098\n",
      "Epoch 28/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3878 - acc: 0.9162 - val_loss: 0.4077 - val_acc: 0.9101\n",
      "Epoch 29/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3875 - acc: 0.9156 - val_loss: 0.4068 - val_acc: 0.9107\n",
      "Epoch 30/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3871 - acc: 0.9168 - val_loss: 0.4060 - val_acc: 0.9121\n",
      "Epoch 31/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3877 - acc: 0.9159 - val_loss: 0.4046 - val_acc: 0.9102\n",
      "Epoch 32/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3872 - acc: 0.9161 - val_loss: 0.4075 - val_acc: 0.9102\n",
      "Epoch 33/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3876 - acc: 0.9157 - val_loss: 0.4064 - val_acc: 0.9105\n",
      "Epoch 34/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3873 - acc: 0.9159 - val_loss: 0.4060 - val_acc: 0.9114\n",
      "Epoch 35/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3872 - acc: 0.9156 - val_loss: 0.4047 - val_acc: 0.9090\n",
      "Epoch 36/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3876 - acc: 0.9160 - val_loss: 0.4070 - val_acc: 0.9070\n",
      "Epoch 37/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3874 - acc: 0.9162 - val_loss: 0.4057 - val_acc: 0.9106\n",
      "Epoch 38/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3874 - acc: 0.9157 - val_loss: 0.4038 - val_acc: 0.9108\n",
      "Epoch 39/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3870 - acc: 0.9154 - val_loss: 0.4056 - val_acc: 0.9123\n",
      "Epoch 40/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3876 - acc: 0.9163 - val_loss: 0.4067 - val_acc: 0.9102\n",
      "Epoch 41/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3873 - acc: 0.9165 - val_loss: 0.4039 - val_acc: 0.9124\n",
      "Epoch 42/150\n",
      "33600/33600 [==============================] - 3s 99us/step - loss: 0.3873 - acc: 0.9162 - val_loss: 0.4050 - val_acc: 0.9094\n",
      "Epoch 43/150\n",
      "33600/33600 [==============================] - 4s 105us/step - loss: 0.3868 - acc: 0.9163 - val_loss: 0.4043 - val_acc: 0.9081\n",
      "Epoch 44/150\n",
      "33600/33600 [==============================] - 3s 99us/step - loss: 0.3872 - acc: 0.9156 - val_loss: 0.4069 - val_acc: 0.9096\n",
      "Epoch 45/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 0.3871 - acc: 0.9161 - val_loss: 0.4056 - val_acc: 0.9111\n",
      "Epoch 46/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3877 - acc: 0.9159 - val_loss: 0.4061 - val_acc: 0.9104\n",
      "Epoch 47/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3872 - acc: 0.9163 - val_loss: 0.4038 - val_acc: 0.9106\n",
      "Epoch 48/150\n",
      "33600/33600 [==============================] - 3s 99us/step - loss: 0.3873 - acc: 0.9166 - val_loss: 0.4073 - val_acc: 0.9096\n",
      "Epoch 49/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 0.3875 - acc: 0.9156 - val_loss: 0.4055 - val_acc: 0.9087\n",
      "Epoch 50/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3875 - acc: 0.9160 - val_loss: 0.4047 - val_acc: 0.9107\n",
      "Epoch 51/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3876 - acc: 0.9168 - val_loss: 0.4072 - val_acc: 0.9098\n",
      "Epoch 52/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3871 - acc: 0.9169 - val_loss: 0.4068 - val_acc: 0.9102\n",
      "Epoch 53/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3872 - acc: 0.9154 - val_loss: 0.4039 - val_acc: 0.9115\n",
      "Epoch 54/150\n",
      "33600/33600 [==============================] - 3s 95us/step - loss: 0.3872 - acc: 0.9149 - val_loss: 0.4065 - val_acc: 0.9085\n",
      "Epoch 55/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3873 - acc: 0.9161 - val_loss: 0.4070 - val_acc: 0.9083\n",
      "Epoch 56/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3870 - acc: 0.9160 - val_loss: 0.4062 - val_acc: 0.9090\n",
      "Epoch 57/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3872 - acc: 0.9157 - val_loss: 0.4079 - val_acc: 0.9086\n",
      "Epoch 58/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3872 - acc: 0.9166 - val_loss: 0.4051 - val_acc: 0.9107\n",
      "Epoch 59/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3873 - acc: 0.9154 - val_loss: 0.4111 - val_acc: 0.9050\n",
      "Epoch 60/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3873 - acc: 0.9156 - val_loss: 0.4064 - val_acc: 0.9083\n",
      "Epoch 61/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3877 - acc: 0.9161 - val_loss: 0.4065 - val_acc: 0.9064\n",
      "Epoch 62/150\n",
      "33600/33600 [==============================] - 3s 94us/step - loss: 0.3870 - acc: 0.9160 - val_loss: 0.4103 - val_acc: 0.9093\n",
      "Epoch 63/150\n",
      "33600/33600 [==============================] - 3s 93us/step - loss: 0.3868 - acc: 0.9161 - val_loss: 0.4068 - val_acc: 0.9105\n",
      "Epoch 64/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3870 - acc: 0.9154 - val_loss: 0.4089 - val_acc: 0.9065\n",
      "Epoch 65/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3869 - acc: 0.9157 - val_loss: 0.4045 - val_acc: 0.9098\n",
      "Epoch 66/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3870 - acc: 0.9160 - val_loss: 0.4107 - val_acc: 0.9075\n",
      "Epoch 67/150\n",
      "33600/33600 [==============================] - 3s 95us/step - loss: 0.3872 - acc: 0.9158 - val_loss: 0.4031 - val_acc: 0.9110\n",
      "Epoch 68/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 0.3865 - acc: 0.9154 - val_loss: 0.4032 - val_acc: 0.9124\n",
      "Epoch 69/150\n",
      "33600/33600 [==============================] - 3s 94us/step - loss: 0.3868 - acc: 0.9159 - val_loss: 0.4043 - val_acc: 0.9120\n",
      "Epoch 70/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3869 - acc: 0.9156 - val_loss: 0.4031 - val_acc: 0.9117\n",
      "Epoch 71/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3864 - acc: 0.9153 - val_loss: 0.4095 - val_acc: 0.9060\n",
      "Epoch 72/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3869 - acc: 0.9151 - val_loss: 0.4056 - val_acc: 0.9085\n",
      "Epoch 73/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3867 - acc: 0.9166 - val_loss: 0.4053 - val_acc: 0.9090\n",
      "Epoch 74/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3868 - acc: 0.9154 - val_loss: 0.4065 - val_acc: 0.9086\n",
      "Epoch 75/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 0.3870 - acc: 0.9154 - val_loss: 0.4058 - val_acc: 0.9101\n",
      "Epoch 76/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3873 - acc: 0.9158 - val_loss: 0.4049 - val_acc: 0.9100\n",
      "Epoch 77/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3870 - acc: 0.9161 - val_loss: 0.4070 - val_acc: 0.9100\n",
      "Epoch 78/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3871 - acc: 0.9153 - val_loss: 0.4053 - val_acc: 0.9113\n",
      "Epoch 79/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3868 - acc: 0.9160 - val_loss: 0.4050 - val_acc: 0.9108\n",
      "Epoch 80/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3870 - acc: 0.9163 - val_loss: 0.4061 - val_acc: 0.9100\n",
      "Epoch 81/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3866 - acc: 0.9159 - val_loss: 0.4035 - val_acc: 0.9127\n",
      "Epoch 82/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3867 - acc: 0.9156 - val_loss: 0.4069 - val_acc: 0.9071\n",
      "Epoch 83/150\n",
      "33600/33600 [==============================] - 3s 93us/step - loss: 0.3865 - acc: 0.9154 - val_loss: 0.4073 - val_acc: 0.9100\n",
      "Epoch 84/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3860 - acc: 0.9161 - val_loss: 0.4065 - val_acc: 0.9088\n",
      "Epoch 85/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3861 - acc: 0.9159 - val_loss: 0.4049 - val_acc: 0.9094\n",
      "Epoch 86/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3864 - acc: 0.9151 - val_loss: 0.4050 - val_acc: 0.9092\n",
      "Epoch 87/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3863 - acc: 0.9159 - val_loss: 0.4059 - val_acc: 0.9098\n",
      "Epoch 88/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3864 - acc: 0.9152 - val_loss: 0.4062 - val_acc: 0.9102\n",
      "Epoch 89/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 0.3867 - acc: 0.9156 - val_loss: 0.4065 - val_acc: 0.9086\n",
      "Epoch 90/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3865 - acc: 0.9150 - val_loss: 0.4048 - val_acc: 0.9094\n",
      "Epoch 91/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3865 - acc: 0.9151 - val_loss: 0.4068 - val_acc: 0.9104\n",
      "Epoch 92/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3867 - acc: 0.9159 - val_loss: 0.4039 - val_acc: 0.9110\n",
      "Epoch 93/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3868 - acc: 0.9158 - val_loss: 0.4042 - val_acc: 0.9104\n",
      "Epoch 94/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3862 - acc: 0.9162 - val_loss: 0.4076 - val_acc: 0.9080\n",
      "Epoch 95/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3868 - acc: 0.9161 - val_loss: 0.4054 - val_acc: 0.9107\n",
      "Epoch 96/150\n",
      "33600/33600 [==============================] - 3s 95us/step - loss: 0.3865 - acc: 0.9154 - val_loss: 0.4053 - val_acc: 0.9086\n",
      "Epoch 97/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 0.3869 - acc: 0.9161 - val_loss: 0.4044 - val_acc: 0.9100\n",
      "Epoch 98/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 0.3866 - acc: 0.9159 - val_loss: 0.4060 - val_acc: 0.9105\n",
      "Epoch 99/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 0.3871 - acc: 0.9161 - val_loss: 0.4046 - val_acc: 0.9098\n",
      "Epoch 100/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3873 - acc: 0.9153 - val_loss: 0.4029 - val_acc: 0.9113\n",
      "Epoch 101/150\n",
      "33600/33600 [==============================] - 3s 94us/step - loss: 0.3868 - acc: 0.9166 - val_loss: 0.4055 - val_acc: 0.9096\n",
      "Epoch 102/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3869 - acc: 0.9150 - val_loss: 0.4034 - val_acc: 0.9112\n",
      "Epoch 103/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3873 - acc: 0.9154 - val_loss: 0.4044 - val_acc: 0.9115\n",
      "Epoch 104/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3868 - acc: 0.9155 - val_loss: 0.4061 - val_acc: 0.9077\n",
      "Epoch 105/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3868 - acc: 0.9157 - val_loss: 0.4073 - val_acc: 0.9100\n",
      "Epoch 106/150\n",
      "33600/33600 [==============================] - 3s 94us/step - loss: 0.3866 - acc: 0.9162 - val_loss: 0.4080 - val_acc: 0.9099\n",
      "Epoch 107/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3870 - acc: 0.9163 - val_loss: 0.4043 - val_acc: 0.9098\n",
      "Epoch 108/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3866 - acc: 0.9163 - val_loss: 0.4071 - val_acc: 0.9089\n",
      "Epoch 109/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3865 - acc: 0.9160 - val_loss: 0.4082 - val_acc: 0.9100\n",
      "Epoch 110/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3870 - acc: 0.9154 - val_loss: 0.4044 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3869 - acc: 0.9161 - val_loss: 0.4075 - val_acc: 0.9065\n",
      "Epoch 112/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3869 - acc: 0.9160 - val_loss: 0.4049 - val_acc: 0.9107\n",
      "Epoch 113/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3865 - acc: 0.9167 - val_loss: 0.4045 - val_acc: 0.9119\n",
      "Epoch 114/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3868 - acc: 0.9164 - val_loss: 0.4080 - val_acc: 0.9065\n",
      "Epoch 115/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3869 - acc: 0.9153 - val_loss: 0.4049 - val_acc: 0.9101\n",
      "Epoch 116/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3870 - acc: 0.9160 - val_loss: 0.4074 - val_acc: 0.9088\n",
      "Epoch 117/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3869 - acc: 0.9162 - val_loss: 0.4047 - val_acc: 0.9095\n",
      "Epoch 118/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3866 - acc: 0.9158 - val_loss: 0.4037 - val_acc: 0.9119\n",
      "Epoch 119/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3866 - acc: 0.9159 - val_loss: 0.4052 - val_acc: 0.9108\n",
      "Epoch 120/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3867 - acc: 0.9160 - val_loss: 0.4053 - val_acc: 0.9102\n",
      "Epoch 121/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3867 - acc: 0.9164 - val_loss: 0.4051 - val_acc: 0.9095\n",
      "Epoch 122/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3866 - acc: 0.9157 - val_loss: 0.4095 - val_acc: 0.9046\n",
      "Epoch 123/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3867 - acc: 0.9157 - val_loss: 0.4051 - val_acc: 0.9094\n",
      "Epoch 124/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3872 - acc: 0.9156 - val_loss: 0.4063 - val_acc: 0.9104\n",
      "Epoch 125/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3869 - acc: 0.9160 - val_loss: 0.4047 - val_acc: 0.9101\n",
      "Epoch 126/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3863 - acc: 0.9167 - val_loss: 0.4064 - val_acc: 0.9069\n",
      "Epoch 127/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3870 - acc: 0.9151 - val_loss: 0.4040 - val_acc: 0.9108\n",
      "Epoch 128/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3866 - acc: 0.9155 - val_loss: 0.4079 - val_acc: 0.9049\n",
      "Epoch 129/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3869 - acc: 0.9165 - val_loss: 0.4055 - val_acc: 0.9096\n",
      "Epoch 130/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3871 - acc: 0.9168 - val_loss: 0.4059 - val_acc: 0.9087\n",
      "Epoch 131/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3872 - acc: 0.9154 - val_loss: 0.4050 - val_acc: 0.9108\n",
      "Epoch 132/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3870 - acc: 0.9158 - val_loss: 0.4075 - val_acc: 0.9088\n",
      "Epoch 133/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3871 - acc: 0.9161 - val_loss: 0.4048 - val_acc: 0.9098\n",
      "Epoch 134/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3867 - acc: 0.9164 - val_loss: 0.4074 - val_acc: 0.9060\n",
      "Epoch 135/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3866 - acc: 0.9162 - val_loss: 0.4044 - val_acc: 0.9089\n",
      "Epoch 136/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3864 - acc: 0.9152 - val_loss: 0.4055 - val_acc: 0.9094\n",
      "Epoch 137/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3870 - acc: 0.9151 - val_loss: 0.4049 - val_acc: 0.9101\n",
      "Epoch 138/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3868 - acc: 0.9167 - val_loss: 0.4081 - val_acc: 0.9062\n",
      "Epoch 139/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3864 - acc: 0.9157 - val_loss: 0.4101 - val_acc: 0.9062\n",
      "Epoch 140/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3866 - acc: 0.9160 - val_loss: 0.4086 - val_acc: 0.9082\n",
      "Epoch 141/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3863 - acc: 0.9159 - val_loss: 0.4076 - val_acc: 0.9062\n",
      "Epoch 142/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3868 - acc: 0.9153 - val_loss: 0.4068 - val_acc: 0.9099\n",
      "Epoch 143/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3867 - acc: 0.9157 - val_loss: 0.4038 - val_acc: 0.9114\n",
      "Epoch 144/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3868 - acc: 0.9155 - val_loss: 0.4071 - val_acc: 0.9087\n",
      "Epoch 145/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3869 - acc: 0.9155 - val_loss: 0.4046 - val_acc: 0.9092\n",
      "Epoch 146/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3868 - acc: 0.9156 - val_loss: 0.4047 - val_acc: 0.9110\n",
      "Epoch 147/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3867 - acc: 0.9148 - val_loss: 0.4053 - val_acc: 0.9094\n",
      "Epoch 148/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3871 - acc: 0.9163 - val_loss: 0.4066 - val_acc: 0.9083\n",
      "Epoch 149/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3871 - acc: 0.9156 - val_loss: 0.4042 - val_acc: 0.9085\n",
      "Epoch 150/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3867 - acc: 0.9157 - val_loss: 0.4059 - val_acc: 0.9094\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/150\n",
      "33600/33600 [==============================] - 4s 105us/step - loss: 0.6303 - acc: 0.8414 - val_loss: 0.3832 - val_acc: 0.8950\n",
      "Epoch 2/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3493 - acc: 0.9054 - val_loss: 0.3474 - val_acc: 0.9069\n",
      "Epoch 3/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3216 - acc: 0.9141 - val_loss: 0.3312 - val_acc: 0.9114\n",
      "Epoch 4/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3101 - acc: 0.9174 - val_loss: 0.3226 - val_acc: 0.9149\n",
      "Epoch 5/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3040 - acc: 0.9199 - val_loss: 0.3207 - val_acc: 0.9173\n",
      "Epoch 6/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2997 - acc: 0.9216 - val_loss: 0.3215 - val_acc: 0.9163\n",
      "Epoch 7/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2973 - acc: 0.9223 - val_loss: 0.3193 - val_acc: 0.9164\n",
      "Epoch 8/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2947 - acc: 0.9240 - val_loss: 0.3200 - val_acc: 0.9182\n",
      "Epoch 9/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2936 - acc: 0.9249 - val_loss: 0.3180 - val_acc: 0.9189\n",
      "Epoch 10/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2926 - acc: 0.9255 - val_loss: 0.3191 - val_acc: 0.9180\n",
      "Epoch 11/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2920 - acc: 0.9257 - val_loss: 0.3189 - val_acc: 0.9180\n",
      "Epoch 12/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.2914 - acc: 0.9255 - val_loss: 0.3200 - val_acc: 0.9195\n",
      "Epoch 13/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2909 - acc: 0.9262 - val_loss: 0.3191 - val_acc: 0.9188\n",
      "Epoch 14/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2901 - acc: 0.9275 - val_loss: 0.3232 - val_acc: 0.9182\n",
      "Epoch 15/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2905 - acc: 0.9272 - val_loss: 0.3264 - val_acc: 0.9171\n",
      "Epoch 16/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2903 - acc: 0.9278 - val_loss: 0.3214 - val_acc: 0.9193\n",
      "Epoch 17/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2898 - acc: 0.9285 - val_loss: 0.3234 - val_acc: 0.9193\n",
      "Epoch 18/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2900 - acc: 0.9277 - val_loss: 0.3250 - val_acc: 0.9177\n",
      "Epoch 19/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2901 - acc: 0.9285 - val_loss: 0.3284 - val_acc: 0.9175\n",
      "Epoch 20/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.2908 - acc: 0.9284 - val_loss: 0.3231 - val_acc: 0.9193\n",
      "Epoch 21/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2907 - acc: 0.9282 - val_loss: 0.3244 - val_acc: 0.9189\n",
      "Epoch 22/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2903 - acc: 0.9286 - val_loss: 0.3263 - val_acc: 0.9185\n",
      "Epoch 23/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2910 - acc: 0.9288 - val_loss: 0.3294 - val_acc: 0.9168\n",
      "Epoch 24/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2908 - acc: 0.9299 - val_loss: 0.3285 - val_acc: 0.9176\n",
      "Epoch 25/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2913 - acc: 0.9294 - val_loss: 0.3274 - val_acc: 0.9201\n",
      "Epoch 26/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2915 - acc: 0.9296 - val_loss: 0.3282 - val_acc: 0.9180\n",
      "Epoch 27/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.2921 - acc: 0.9294 - val_loss: 0.3286 - val_acc: 0.9188\n",
      "Epoch 28/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2925 - acc: 0.9301 - val_loss: 0.3292 - val_acc: 0.9187\n",
      "Epoch 29/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.2923 - acc: 0.9297 - val_loss: 0.3297 - val_acc: 0.9201\n",
      "Epoch 30/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2933 - acc: 0.9303 - val_loss: 0.3289 - val_acc: 0.9195\n",
      "Epoch 31/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2935 - acc: 0.9297 - val_loss: 0.3302 - val_acc: 0.9207\n",
      "Epoch 32/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2941 - acc: 0.9296 - val_loss: 0.3320 - val_acc: 0.9196\n",
      "Epoch 33/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.2944 - acc: 0.9293 - val_loss: 0.3323 - val_acc: 0.9195\n",
      "Epoch 34/150\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 0.2947 - acc: 0.9307 - val_loss: 0.3341 - val_acc: 0.9188\n",
      "Epoch 35/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.2953 - acc: 0.9302 - val_loss: 0.3318 - val_acc: 0.9193\n",
      "Epoch 36/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2959 - acc: 0.9301 - val_loss: 0.3344 - val_acc: 0.9186\n",
      "Epoch 37/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2964 - acc: 0.9303 - val_loss: 0.3351 - val_acc: 0.9193\n",
      "Epoch 38/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2967 - acc: 0.9302 - val_loss: 0.3357 - val_acc: 0.9200\n",
      "Epoch 39/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2970 - acc: 0.9307 - val_loss: 0.3372 - val_acc: 0.9188\n",
      "Epoch 40/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2973 - acc: 0.9303 - val_loss: 0.3375 - val_acc: 0.9195\n",
      "Epoch 41/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2974 - acc: 0.9302 - val_loss: 0.3384 - val_acc: 0.9183\n",
      "Epoch 42/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2981 - acc: 0.9304 - val_loss: 0.3390 - val_acc: 0.9174\n",
      "Epoch 43/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.2986 - acc: 0.9305 - val_loss: 0.3380 - val_acc: 0.9208\n",
      "Epoch 44/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2992 - acc: 0.9296 - val_loss: 0.3379 - val_acc: 0.9202\n",
      "Epoch 45/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2996 - acc: 0.9297 - val_loss: 0.3411 - val_acc: 0.9181\n",
      "Epoch 46/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2997 - acc: 0.9301 - val_loss: 0.3415 - val_acc: 0.9179\n",
      "Epoch 47/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3004 - acc: 0.9304 - val_loss: 0.3416 - val_acc: 0.9193\n",
      "Epoch 48/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3011 - acc: 0.9292 - val_loss: 0.3417 - val_acc: 0.9185\n",
      "Epoch 49/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3013 - acc: 0.9305 - val_loss: 0.3424 - val_acc: 0.9198\n",
      "Epoch 50/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3015 - acc: 0.9304 - val_loss: 0.3434 - val_acc: 0.9185\n",
      "Epoch 51/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3018 - acc: 0.9312 - val_loss: 0.3425 - val_acc: 0.9187\n",
      "Epoch 52/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3026 - acc: 0.9300 - val_loss: 0.3458 - val_acc: 0.9186\n",
      "Epoch 53/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3028 - acc: 0.9296 - val_loss: 0.3435 - val_acc: 0.9177\n",
      "Epoch 54/150\n",
      "33600/33600 [==============================] - 3s 93us/step - loss: 0.3031 - acc: 0.9299 - val_loss: 0.3438 - val_acc: 0.9204\n",
      "Epoch 55/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3034 - acc: 0.9308 - val_loss: 0.3464 - val_acc: 0.9181\n",
      "Epoch 56/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3035 - acc: 0.9306 - val_loss: 0.3475 - val_acc: 0.9177\n",
      "Epoch 57/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3047 - acc: 0.9304 - val_loss: 0.3473 - val_acc: 0.9190\n",
      "Epoch 58/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3043 - acc: 0.9312 - val_loss: 0.3469 - val_acc: 0.9175\n",
      "Epoch 59/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3051 - acc: 0.9308 - val_loss: 0.3482 - val_acc: 0.9185\n",
      "Epoch 60/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3056 - acc: 0.9306 - val_loss: 0.3481 - val_acc: 0.9180\n",
      "Epoch 61/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3057 - acc: 0.9304 - val_loss: 0.3491 - val_acc: 0.9175\n",
      "Epoch 62/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3061 - acc: 0.9304 - val_loss: 0.3488 - val_acc: 0.9186\n",
      "Epoch 63/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3068 - acc: 0.9301 - val_loss: 0.3491 - val_acc: 0.9192\n",
      "Epoch 64/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3071 - acc: 0.9302 - val_loss: 0.3503 - val_acc: 0.9187\n",
      "Epoch 65/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3081 - acc: 0.9302 - val_loss: 0.3513 - val_acc: 0.9183\n",
      "Epoch 66/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3075 - acc: 0.9300 - val_loss: 0.3509 - val_acc: 0.9189\n",
      "Epoch 67/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3082 - acc: 0.9310 - val_loss: 0.3507 - val_acc: 0.9181\n",
      "Epoch 68/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3082 - acc: 0.9312 - val_loss: 0.3514 - val_acc: 0.9175\n",
      "Epoch 69/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3089 - acc: 0.9307 - val_loss: 0.3508 - val_acc: 0.9176\n",
      "Epoch 70/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3094 - acc: 0.9304 - val_loss: 0.3526 - val_acc: 0.9180\n",
      "Epoch 71/150\n",
      "33600/33600 [==============================] - 3s 96us/step - loss: 0.3098 - acc: 0.9304 - val_loss: 0.3525 - val_acc: 0.9185\n",
      "Epoch 72/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3103 - acc: 0.9303 - val_loss: 0.3538 - val_acc: 0.9179\n",
      "Epoch 73/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3105 - acc: 0.9307 - val_loss: 0.3521 - val_acc: 0.9181\n",
      "Epoch 74/150\n",
      "33600/33600 [==============================] - 3s 95us/step - loss: 0.3112 - acc: 0.9310 - val_loss: 0.3561 - val_acc: 0.9168\n",
      "Epoch 75/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3107 - acc: 0.9301 - val_loss: 0.3555 - val_acc: 0.9167\n",
      "Epoch 76/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3114 - acc: 0.9303 - val_loss: 0.3535 - val_acc: 0.9202\n",
      "Epoch 77/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3118 - acc: 0.9301 - val_loss: 0.3576 - val_acc: 0.9161\n",
      "Epoch 78/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3126 - acc: 0.9303 - val_loss: 0.3558 - val_acc: 0.9175\n",
      "Epoch 79/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3128 - acc: 0.9300 - val_loss: 0.3572 - val_acc: 0.9190\n",
      "Epoch 80/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3128 - acc: 0.9308 - val_loss: 0.3567 - val_acc: 0.9180\n",
      "Epoch 81/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3130 - acc: 0.9305 - val_loss: 0.3562 - val_acc: 0.9185\n",
      "Epoch 82/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3134 - acc: 0.9306 - val_loss: 0.3568 - val_acc: 0.9175\n",
      "Epoch 83/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3140 - acc: 0.9305 - val_loss: 0.3573 - val_acc: 0.9194\n",
      "Epoch 84/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3137 - acc: 0.9310 - val_loss: 0.3602 - val_acc: 0.9173\n",
      "Epoch 85/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3142 - acc: 0.9315 - val_loss: 0.3556 - val_acc: 0.9195\n",
      "Epoch 86/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3146 - acc: 0.9308 - val_loss: 0.3597 - val_acc: 0.9189\n",
      "Epoch 87/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3149 - acc: 0.9307 - val_loss: 0.3595 - val_acc: 0.9183\n",
      "Epoch 88/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3151 - acc: 0.9309 - val_loss: 0.3609 - val_acc: 0.9157\n",
      "Epoch 89/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3155 - acc: 0.9307 - val_loss: 0.3614 - val_acc: 0.9186\n",
      "Epoch 90/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3156 - acc: 0.9310 - val_loss: 0.3606 - val_acc: 0.9173\n",
      "Epoch 91/150\n",
      "33600/33600 [==============================] - 3s 82us/step - loss: 0.3158 - acc: 0.9307 - val_loss: 0.3607 - val_acc: 0.9193\n",
      "Epoch 92/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3165 - acc: 0.9308 - val_loss: 0.3611 - val_acc: 0.9176\n",
      "Epoch 93/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3168 - acc: 0.9301 - val_loss: 0.3605 - val_acc: 0.9187\n",
      "Epoch 94/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3166 - acc: 0.9306 - val_loss: 0.3611 - val_acc: 0.9181\n",
      "Epoch 95/150\n",
      "33600/33600 [==============================] - 3s 82us/step - loss: 0.3174 - acc: 0.9313 - val_loss: 0.3614 - val_acc: 0.9181\n",
      "Epoch 96/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3176 - acc: 0.9310 - val_loss: 0.3610 - val_acc: 0.9188\n",
      "Epoch 97/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3183 - acc: 0.9310 - val_loss: 0.3652 - val_acc: 0.9155\n",
      "Epoch 98/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3184 - acc: 0.9304 - val_loss: 0.3610 - val_acc: 0.9199\n",
      "Epoch 99/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3192 - acc: 0.9310 - val_loss: 0.3626 - val_acc: 0.9181\n",
      "Epoch 100/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3198 - acc: 0.9300 - val_loss: 0.3620 - val_acc: 0.9182\n",
      "Epoch 101/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3197 - acc: 0.9307 - val_loss: 0.3631 - val_acc: 0.9176\n",
      "Epoch 102/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3198 - acc: 0.9305 - val_loss: 0.3655 - val_acc: 0.9164\n",
      "Epoch 103/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3203 - acc: 0.9301 - val_loss: 0.3629 - val_acc: 0.9182\n",
      "Epoch 104/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3204 - acc: 0.9303 - val_loss: 0.3635 - val_acc: 0.9189\n",
      "Epoch 105/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3210 - acc: 0.9304 - val_loss: 0.3636 - val_acc: 0.9177\n",
      "Epoch 106/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3216 - acc: 0.9310 - val_loss: 0.3653 - val_acc: 0.9180\n",
      "Epoch 107/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3216 - acc: 0.9307 - val_loss: 0.3646 - val_acc: 0.9180\n",
      "Epoch 108/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3215 - acc: 0.9302 - val_loss: 0.3662 - val_acc: 0.9180\n",
      "Epoch 109/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3221 - acc: 0.9306 - val_loss: 0.3651 - val_acc: 0.9180\n",
      "Epoch 110/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3222 - acc: 0.9299 - val_loss: 0.3651 - val_acc: 0.9193\n",
      "Epoch 111/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3222 - acc: 0.9301 - val_loss: 0.3674 - val_acc: 0.9175\n",
      "Epoch 112/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3226 - acc: 0.9309 - val_loss: 0.3687 - val_acc: 0.9158\n",
      "Epoch 113/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3230 - acc: 0.9306 - val_loss: 0.3653 - val_acc: 0.9179\n",
      "Epoch 114/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3231 - acc: 0.9307 - val_loss: 0.3659 - val_acc: 0.9192\n",
      "Epoch 115/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3233 - acc: 0.9310 - val_loss: 0.3685 - val_acc: 0.9176\n",
      "Epoch 116/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3237 - acc: 0.9306 - val_loss: 0.3683 - val_acc: 0.9185\n",
      "Epoch 117/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3239 - acc: 0.9309 - val_loss: 0.3678 - val_acc: 0.9181\n",
      "Epoch 118/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3234 - acc: 0.9307 - val_loss: 0.3685 - val_acc: 0.9155\n",
      "Epoch 119/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3240 - acc: 0.9309 - val_loss: 0.3687 - val_acc: 0.9174\n",
      "Epoch 120/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3236 - acc: 0.9305 - val_loss: 0.3666 - val_acc: 0.9185\n",
      "Epoch 121/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3242 - acc: 0.9307 - val_loss: 0.3668 - val_acc: 0.9195\n",
      "Epoch 122/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3240 - acc: 0.9303 - val_loss: 0.3696 - val_acc: 0.9177\n",
      "Epoch 123/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3242 - acc: 0.9307 - val_loss: 0.3682 - val_acc: 0.9183\n",
      "Epoch 124/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3243 - acc: 0.9308 - val_loss: 0.3690 - val_acc: 0.9176\n",
      "Epoch 125/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3247 - acc: 0.9305 - val_loss: 0.3694 - val_acc: 0.9169\n",
      "Epoch 126/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3247 - acc: 0.9306 - val_loss: 0.3682 - val_acc: 0.9173\n",
      "Epoch 127/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3241 - acc: 0.9313 - val_loss: 0.3679 - val_acc: 0.9181\n",
      "Epoch 128/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3250 - acc: 0.9307 - val_loss: 0.3707 - val_acc: 0.9167\n",
      "Epoch 129/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3250 - acc: 0.9312 - val_loss: 0.3688 - val_acc: 0.9185\n",
      "Epoch 130/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3254 - acc: 0.9305 - val_loss: 0.3693 - val_acc: 0.9185\n",
      "Epoch 131/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3251 - acc: 0.9310 - val_loss: 0.3711 - val_acc: 0.9179\n",
      "Epoch 132/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3249 - acc: 0.9316 - val_loss: 0.3720 - val_acc: 0.9155\n",
      "Epoch 133/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.3253 - acc: 0.9306 - val_loss: 0.3705 - val_acc: 0.9175\n",
      "Epoch 134/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3256 - acc: 0.9317 - val_loss: 0.3712 - val_acc: 0.9180\n",
      "Epoch 135/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.3258 - acc: 0.9308 - val_loss: 0.3717 - val_acc: 0.9164\n",
      "Epoch 136/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.3256 - acc: 0.9305 - val_loss: 0.3707 - val_acc: 0.9171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3262 - acc: 0.9307 - val_loss: 0.3689 - val_acc: 0.9186\n",
      "Epoch 138/150\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3258 - acc: 0.9304 - val_loss: 0.3692 - val_acc: 0.9194\n",
      "Epoch 139/150\n",
      "33600/33600 [==============================] - 3s 94us/step - loss: 0.3265 - acc: 0.9310 - val_loss: 0.3690 - val_acc: 0.9182\n",
      "Epoch 140/150\n",
      "33600/33600 [==============================] - 3s 99us/step - loss: 0.3266 - acc: 0.9309 - val_loss: 0.3719 - val_acc: 0.9174\n",
      "Epoch 141/150\n",
      "33600/33600 [==============================] - 3s 99us/step - loss: 0.3269 - acc: 0.9307 - val_loss: 0.3718 - val_acc: 0.9175\n",
      "Epoch 142/150\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.3263 - acc: 0.9318 - val_loss: 0.3712 - val_acc: 0.9175\n",
      "Epoch 143/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3272 - acc: 0.9309 - val_loss: 0.3708 - val_acc: 0.9187\n",
      "Epoch 144/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3273 - acc: 0.9308 - val_loss: 0.3712 - val_acc: 0.9180\n",
      "Epoch 145/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3275 - acc: 0.9306 - val_loss: 0.3716 - val_acc: 0.9186\n",
      "Epoch 146/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3274 - acc: 0.9312 - val_loss: 0.3711 - val_acc: 0.9188\n",
      "Epoch 147/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.3281 - acc: 0.9313 - val_loss: 0.3717 - val_acc: 0.9188\n",
      "Epoch 148/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3285 - acc: 0.9306 - val_loss: 0.3722 - val_acc: 0.9194\n",
      "Epoch 149/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.3283 - acc: 0.9306 - val_loss: 0.3735 - val_acc: 0.9195\n",
      "Epoch 150/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3284 - acc: 0.9309 - val_loss: 0.3733 - val_acc: 0.9195\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/150\n",
      "33600/33600 [==============================] - 4s 106us/step - loss: 0.6256 - acc: 0.8427 - val_loss: 0.3724 - val_acc: 0.8955\n",
      "Epoch 2/150\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.3356 - acc: 0.9063 - val_loss: 0.3251 - val_acc: 0.9068\n",
      "Epoch 3/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.3024 - acc: 0.9150 - val_loss: 0.3076 - val_acc: 0.9139\n",
      "Epoch 4/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2871 - acc: 0.9198 - val_loss: 0.2993 - val_acc: 0.9149\n",
      "Epoch 5/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2772 - acc: 0.9226 - val_loss: 0.2955 - val_acc: 0.9185\n",
      "Epoch 6/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2713 - acc: 0.9251 - val_loss: 0.2896 - val_acc: 0.9201\n",
      "Epoch 7/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2668 - acc: 0.9263 - val_loss: 0.2891 - val_acc: 0.9200\n",
      "Epoch 8/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2627 - acc: 0.9267 - val_loss: 0.2889 - val_acc: 0.9188\n",
      "Epoch 9/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2595 - acc: 0.9285 - val_loss: 0.2861 - val_acc: 0.9205\n",
      "Epoch 10/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2570 - acc: 0.9299 - val_loss: 0.2843 - val_acc: 0.9225\n",
      "Epoch 11/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2546 - acc: 0.9299 - val_loss: 0.2852 - val_acc: 0.9214\n",
      "Epoch 12/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2535 - acc: 0.9304 - val_loss: 0.2842 - val_acc: 0.9225\n",
      "Epoch 13/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2516 - acc: 0.9321 - val_loss: 0.2855 - val_acc: 0.9226\n",
      "Epoch 14/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2496 - acc: 0.9317 - val_loss: 0.2827 - val_acc: 0.9230\n",
      "Epoch 15/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2493 - acc: 0.9332 - val_loss: 0.2843 - val_acc: 0.9221\n",
      "Epoch 16/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2477 - acc: 0.9335 - val_loss: 0.2842 - val_acc: 0.9240\n",
      "Epoch 17/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2471 - acc: 0.9339 - val_loss: 0.2851 - val_acc: 0.9233\n",
      "Epoch 18/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2458 - acc: 0.9343 - val_loss: 0.2892 - val_acc: 0.9215\n",
      "Epoch 19/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2451 - acc: 0.9333 - val_loss: 0.2856 - val_acc: 0.9238\n",
      "Epoch 20/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2439 - acc: 0.9343 - val_loss: 0.2845 - val_acc: 0.9242\n",
      "Epoch 21/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2429 - acc: 0.9345 - val_loss: 0.2852 - val_acc: 0.9226\n",
      "Epoch 22/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2428 - acc: 0.9345 - val_loss: 0.2851 - val_acc: 0.9242\n",
      "Epoch 23/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2423 - acc: 0.9356 - val_loss: 0.2871 - val_acc: 0.9211\n",
      "Epoch 24/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2416 - acc: 0.9348 - val_loss: 0.2866 - val_acc: 0.9248\n",
      "Epoch 25/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2412 - acc: 0.9357 - val_loss: 0.2880 - val_acc: 0.9240\n",
      "Epoch 26/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2405 - acc: 0.9360 - val_loss: 0.2886 - val_acc: 0.9245\n",
      "Epoch 27/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2398 - acc: 0.9363 - val_loss: 0.2870 - val_acc: 0.9242\n",
      "Epoch 28/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2394 - acc: 0.9360 - val_loss: 0.2887 - val_acc: 0.9236\n",
      "Epoch 29/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2390 - acc: 0.9360 - val_loss: 0.2887 - val_acc: 0.9244\n",
      "Epoch 30/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2386 - acc: 0.9367 - val_loss: 0.2897 - val_acc: 0.9243\n",
      "Epoch 31/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2384 - acc: 0.9371 - val_loss: 0.2894 - val_acc: 0.9230\n",
      "Epoch 32/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2377 - acc: 0.9366 - val_loss: 0.2901 - val_acc: 0.9246\n",
      "Epoch 33/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2376 - acc: 0.9371 - val_loss: 0.2904 - val_acc: 0.9252\n",
      "Epoch 34/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2372 - acc: 0.9374 - val_loss: 0.2926 - val_acc: 0.9231\n",
      "Epoch 35/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2370 - acc: 0.9372 - val_loss: 0.2906 - val_acc: 0.9245\n",
      "Epoch 36/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2367 - acc: 0.9373 - val_loss: 0.2916 - val_acc: 0.9262\n",
      "Epoch 37/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2367 - acc: 0.9377 - val_loss: 0.2929 - val_acc: 0.9237\n",
      "Epoch 38/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2366 - acc: 0.9378 - val_loss: 0.2924 - val_acc: 0.9238\n",
      "Epoch 39/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2359 - acc: 0.9384 - val_loss: 0.2944 - val_acc: 0.9246\n",
      "Epoch 40/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2357 - acc: 0.9381 - val_loss: 0.2950 - val_acc: 0.9244\n",
      "Epoch 41/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2355 - acc: 0.9386 - val_loss: 0.2939 - val_acc: 0.9245\n",
      "Epoch 42/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2354 - acc: 0.9383 - val_loss: 0.2959 - val_acc: 0.9224\n",
      "Epoch 43/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2348 - acc: 0.9385 - val_loss: 0.2969 - val_acc: 0.9240\n",
      "Epoch 44/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2347 - acc: 0.9389 - val_loss: 0.3001 - val_acc: 0.9201\n",
      "Epoch 45/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2341 - acc: 0.9385 - val_loss: 0.2969 - val_acc: 0.9239\n",
      "Epoch 46/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2345 - acc: 0.9388 - val_loss: 0.2969 - val_acc: 0.9231\n",
      "Epoch 47/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2340 - acc: 0.9386 - val_loss: 0.2993 - val_acc: 0.9238\n",
      "Epoch 48/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2338 - acc: 0.9387 - val_loss: 0.3004 - val_acc: 0.9225\n",
      "Epoch 49/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2338 - acc: 0.9392 - val_loss: 0.3021 - val_acc: 0.9224\n",
      "Epoch 50/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.2336 - acc: 0.9399 - val_loss: 0.3002 - val_acc: 0.9227\n",
      "Epoch 51/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2334 - acc: 0.9396 - val_loss: 0.2999 - val_acc: 0.9239\n",
      "Epoch 52/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.2337 - acc: 0.9397 - val_loss: 0.3026 - val_acc: 0.9230\n",
      "Epoch 53/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2332 - acc: 0.9400 - val_loss: 0.3008 - val_acc: 0.9237\n",
      "Epoch 54/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2330 - acc: 0.9393 - val_loss: 0.3042 - val_acc: 0.9225\n",
      "Epoch 55/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2325 - acc: 0.9396 - val_loss: 0.3032 - val_acc: 0.9243\n",
      "Epoch 56/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2331 - acc: 0.9400 - val_loss: 0.3012 - val_acc: 0.9235\n",
      "Epoch 57/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2321 - acc: 0.9406 - val_loss: 0.3023 - val_acc: 0.9227\n",
      "Epoch 58/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2324 - acc: 0.9408 - val_loss: 0.3032 - val_acc: 0.9232\n",
      "Epoch 59/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2320 - acc: 0.9399 - val_loss: 0.3047 - val_acc: 0.9221\n",
      "Epoch 60/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2321 - acc: 0.9401 - val_loss: 0.3037 - val_acc: 0.9240\n",
      "Epoch 61/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2315 - acc: 0.9413 - val_loss: 0.3052 - val_acc: 0.9231\n",
      "Epoch 62/150\n",
      "33600/33600 [==============================] - 3s 83us/step - loss: 0.2314 - acc: 0.9403 - val_loss: 0.3055 - val_acc: 0.9232\n",
      "Epoch 63/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2313 - acc: 0.9400 - val_loss: 0.3094 - val_acc: 0.9229\n",
      "Epoch 64/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2312 - acc: 0.9410 - val_loss: 0.3061 - val_acc: 0.9240\n",
      "Epoch 65/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2311 - acc: 0.9401 - val_loss: 0.3091 - val_acc: 0.9208\n",
      "Epoch 66/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2308 - acc: 0.9403 - val_loss: 0.3082 - val_acc: 0.9237\n",
      "Epoch 67/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2308 - acc: 0.9409 - val_loss: 0.3083 - val_acc: 0.9232\n",
      "Epoch 68/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2304 - acc: 0.9413 - val_loss: 0.3089 - val_acc: 0.9231\n",
      "Epoch 69/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2305 - acc: 0.9404 - val_loss: 0.3091 - val_acc: 0.9233\n",
      "Epoch 70/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2301 - acc: 0.9409 - val_loss: 0.3106 - val_acc: 0.9229\n",
      "Epoch 71/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2300 - acc: 0.9413 - val_loss: 0.3107 - val_acc: 0.9230\n",
      "Epoch 72/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2301 - acc: 0.9412 - val_loss: 0.3105 - val_acc: 0.9233\n",
      "Epoch 73/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2302 - acc: 0.9404 - val_loss: 0.3115 - val_acc: 0.9219\n",
      "Epoch 74/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2297 - acc: 0.9412 - val_loss: 0.3125 - val_acc: 0.9219\n",
      "Epoch 75/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2296 - acc: 0.9414 - val_loss: 0.3119 - val_acc: 0.9208\n",
      "Epoch 76/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2293 - acc: 0.9417 - val_loss: 0.3139 - val_acc: 0.9200\n",
      "Epoch 77/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2294 - acc: 0.9417 - val_loss: 0.3145 - val_acc: 0.9230\n",
      "Epoch 78/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2299 - acc: 0.9415 - val_loss: 0.3167 - val_acc: 0.9211\n",
      "Epoch 79/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2293 - acc: 0.9418 - val_loss: 0.3138 - val_acc: 0.9238\n",
      "Epoch 80/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2294 - acc: 0.9416 - val_loss: 0.3146 - val_acc: 0.9218\n",
      "Epoch 81/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2292 - acc: 0.9421 - val_loss: 0.3160 - val_acc: 0.9232\n",
      "Epoch 82/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2294 - acc: 0.9413 - val_loss: 0.3184 - val_acc: 0.9204\n",
      "Epoch 83/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2283 - acc: 0.9424 - val_loss: 0.3164 - val_acc: 0.9210\n",
      "Epoch 84/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2288 - acc: 0.9414 - val_loss: 0.3166 - val_acc: 0.9230\n",
      "Epoch 85/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2287 - acc: 0.9422 - val_loss: 0.3198 - val_acc: 0.9192\n",
      "Epoch 86/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2289 - acc: 0.9418 - val_loss: 0.3192 - val_acc: 0.9225\n",
      "Epoch 87/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2289 - acc: 0.9416 - val_loss: 0.3183 - val_acc: 0.9213\n",
      "Epoch 88/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2289 - acc: 0.9422 - val_loss: 0.3182 - val_acc: 0.9219\n",
      "Epoch 89/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2283 - acc: 0.9419 - val_loss: 0.3202 - val_acc: 0.9207\n",
      "Epoch 90/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2285 - acc: 0.9418 - val_loss: 0.3183 - val_acc: 0.9223\n",
      "Epoch 91/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2284 - acc: 0.9423 - val_loss: 0.3206 - val_acc: 0.9219\n",
      "Epoch 92/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2280 - acc: 0.9424 - val_loss: 0.3214 - val_acc: 0.9204\n",
      "Epoch 93/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2282 - acc: 0.9421 - val_loss: 0.3210 - val_acc: 0.9213\n",
      "Epoch 94/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2280 - acc: 0.9420 - val_loss: 0.3196 - val_acc: 0.9211\n",
      "Epoch 95/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2280 - acc: 0.9431 - val_loss: 0.3226 - val_acc: 0.9218\n",
      "Epoch 96/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2278 - acc: 0.9423 - val_loss: 0.3213 - val_acc: 0.9192\n",
      "Epoch 97/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2278 - acc: 0.9426 - val_loss: 0.3218 - val_acc: 0.9224\n",
      "Epoch 98/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2279 - acc: 0.9425 - val_loss: 0.3220 - val_acc: 0.9214\n",
      "Epoch 99/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2277 - acc: 0.9426 - val_loss: 0.3228 - val_acc: 0.9223\n",
      "Epoch 100/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2275 - acc: 0.9428 - val_loss: 0.3237 - val_acc: 0.9198\n",
      "Epoch 101/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2275 - acc: 0.9434 - val_loss: 0.3242 - val_acc: 0.9207\n",
      "Epoch 102/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2277 - acc: 0.9428 - val_loss: 0.3241 - val_acc: 0.9214\n",
      "Epoch 103/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2272 - acc: 0.9428 - val_loss: 0.3267 - val_acc: 0.9208\n",
      "Epoch 104/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2276 - acc: 0.9424 - val_loss: 0.3251 - val_acc: 0.9198\n",
      "Epoch 105/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2275 - acc: 0.9429 - val_loss: 0.3244 - val_acc: 0.9199\n",
      "Epoch 106/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2270 - acc: 0.9424 - val_loss: 0.3254 - val_acc: 0.9215\n",
      "Epoch 107/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2270 - acc: 0.9422 - val_loss: 0.3254 - val_acc: 0.9199\n",
      "Epoch 108/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2270 - acc: 0.9436 - val_loss: 0.3269 - val_acc: 0.9201\n",
      "Epoch 109/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2272 - acc: 0.9428 - val_loss: 0.3254 - val_acc: 0.9199\n",
      "Epoch 110/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2267 - acc: 0.9432 - val_loss: 0.3272 - val_acc: 0.9212\n",
      "Epoch 111/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2269 - acc: 0.9433 - val_loss: 0.3274 - val_acc: 0.9219\n",
      "Epoch 112/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2269 - acc: 0.9437 - val_loss: 0.3275 - val_acc: 0.9202\n",
      "Epoch 113/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2267 - acc: 0.9434 - val_loss: 0.3303 - val_acc: 0.9199\n",
      "Epoch 114/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2270 - acc: 0.9434 - val_loss: 0.3286 - val_acc: 0.9218\n",
      "Epoch 115/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2267 - acc: 0.9432 - val_loss: 0.3305 - val_acc: 0.9206\n",
      "Epoch 116/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2268 - acc: 0.9429 - val_loss: 0.3293 - val_acc: 0.9204\n",
      "Epoch 117/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2267 - acc: 0.9431 - val_loss: 0.3292 - val_acc: 0.9217\n",
      "Epoch 118/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2268 - acc: 0.9438 - val_loss: 0.3309 - val_acc: 0.9190\n",
      "Epoch 119/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2268 - acc: 0.9432 - val_loss: 0.3286 - val_acc: 0.9224\n",
      "Epoch 120/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2266 - acc: 0.9434 - val_loss: 0.3331 - val_acc: 0.9198\n",
      "Epoch 121/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2270 - acc: 0.9434 - val_loss: 0.3336 - val_acc: 0.9195\n",
      "Epoch 122/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2263 - acc: 0.9436 - val_loss: 0.3377 - val_acc: 0.9188\n",
      "Epoch 123/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2265 - acc: 0.9434 - val_loss: 0.3328 - val_acc: 0.9204\n",
      "Epoch 124/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2263 - acc: 0.9441 - val_loss: 0.3409 - val_acc: 0.9174\n",
      "Epoch 125/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2266 - acc: 0.9431 - val_loss: 0.3337 - val_acc: 0.9199\n",
      "Epoch 126/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2265 - acc: 0.9429 - val_loss: 0.3338 - val_acc: 0.9204\n",
      "Epoch 127/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2265 - acc: 0.9435 - val_loss: 0.3337 - val_acc: 0.9210\n",
      "Epoch 128/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2263 - acc: 0.9439 - val_loss: 0.3391 - val_acc: 0.9185\n",
      "Epoch 129/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2261 - acc: 0.9432 - val_loss: 0.3334 - val_acc: 0.9208\n",
      "Epoch 130/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2262 - acc: 0.9449 - val_loss: 0.3358 - val_acc: 0.9215\n",
      "Epoch 131/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2262 - acc: 0.9444 - val_loss: 0.3399 - val_acc: 0.9173\n",
      "Epoch 132/150\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2260 - acc: 0.9439 - val_loss: 0.3384 - val_acc: 0.9193\n",
      "Epoch 133/150\n",
      "33600/33600 [==============================] - 3s 88us/step - loss: 0.2258 - acc: 0.9440 - val_loss: 0.3361 - val_acc: 0.9200\n",
      "Epoch 134/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2261 - acc: 0.9438 - val_loss: 0.3396 - val_acc: 0.9171\n",
      "Epoch 135/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2261 - acc: 0.9437 - val_loss: 0.3367 - val_acc: 0.9202\n",
      "Epoch 136/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2257 - acc: 0.9444 - val_loss: 0.3400 - val_acc: 0.9180\n",
      "Epoch 137/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2259 - acc: 0.9444 - val_loss: 0.3390 - val_acc: 0.9201\n",
      "Epoch 138/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2259 - acc: 0.9431 - val_loss: 0.3359 - val_acc: 0.9210\n",
      "Epoch 139/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2257 - acc: 0.9443 - val_loss: 0.3376 - val_acc: 0.9208\n",
      "Epoch 140/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2254 - acc: 0.9441 - val_loss: 0.3377 - val_acc: 0.9202\n",
      "Epoch 141/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2255 - acc: 0.9444 - val_loss: 0.3370 - val_acc: 0.9208\n",
      "Epoch 142/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2256 - acc: 0.9435 - val_loss: 0.3401 - val_acc: 0.9199\n",
      "Epoch 143/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2256 - acc: 0.9443 - val_loss: 0.3384 - val_acc: 0.9194\n",
      "Epoch 144/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2253 - acc: 0.9437 - val_loss: 0.3385 - val_acc: 0.9215\n",
      "Epoch 145/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2254 - acc: 0.9438 - val_loss: 0.3385 - val_acc: 0.9206\n",
      "Epoch 146/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2258 - acc: 0.9440 - val_loss: 0.3385 - val_acc: 0.9204\n",
      "Epoch 147/150\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2258 - acc: 0.9446 - val_loss: 0.3402 - val_acc: 0.9202\n",
      "Epoch 148/150\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2252 - acc: 0.9440 - val_loss: 0.3397 - val_acc: 0.9194\n",
      "Epoch 149/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2251 - acc: 0.9446 - val_loss: 0.3404 - val_acc: 0.9196\n",
      "Epoch 150/150\n",
      "33600/33600 [==============================] - 3s 84us/step - loss: 0.2254 - acc: 0.9448 - val_loss: 0.3431 - val_acc: 0.9195\n"
     ]
    }
   ],
   "source": [
    "regs = [0.1,0.01,0.001,0.0001,0]\n",
    "models = []\n",
    "optimizer = 'rmsprop'\n",
    "histories = []\n",
    "for reg in regs:\n",
    "    model,history = linear_model(optimizer,150, train_x,train_y,regularization=reg,valid_x=valid_x,valid_y=valid_y)\n",
    "    models.append(model)\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularizaiton training acc validation acc\n",
      "0.1 0.837738095238 0.837023809524\n",
      "0.01 0.889285714286 0.8875\n",
      "0.001 0.915654761905 0.909404761905\n",
      "0.0001 0.930863095238 0.919523809524\n",
      "0 0.944761904762 0.919523809524\n"
     ]
    }
   ],
   "source": [
    "print('regularizaiton','training acc','validation acc')\n",
    "for idx, histor in enumerate(histories):\n",
    "    tr_acc    = histor.history['acc'][-1]\n",
    "    valid_acc = histor.history['val_acc'][-1]\n",
    "    print(regs[idx],tr_acc,valid_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## the best model is the one with 0.0001 reg\n",
    "model = models[-1]\n",
    "generate_submission(model,test_data,'sub_rmsprop_reg0_150ep_94.4tr_91.9valid.csv',['ImageId','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "| Regularization  | Training acc  | Validation acc  |\n",
    "| -------------   |:-------------:| ---------------:|\n",
    "| 0.1             | 83.9          |    83.24        |\n",
    "| 0.01            | 8899          |    8852         |\n",
    "| 0.001           | 9159          |    9114         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
